{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"False\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n",
    "os.environ[\"JAX_ENABLE_X64\"]=\"True\"\n",
    "import multiprocessing\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(\n",
    "    multiprocessing.cpu_count()\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import gvar as gv\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import jax.example_libraries.optimizers as jax_opt\n",
    "from tqdm import trange\n",
    "import cProfile\n",
    "import pickle\n",
    "\n",
    "\n",
    "# set the default device to the cpu\n",
    "jax.default_device(jax.devices(\"cpu\")[0])\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "\n",
    "#TODO: plot relative error against the number of parameters\n",
    "\n",
    "num_particles = 4\n",
    "N = num_particles\n",
    "# structure = [50,100,200,200,200,200,100,50]\n",
    "structure = [50,50,50]\n",
    "num_nodes = np.sum(structure)\n",
    "m = 1\n",
    "hbar = 1\n",
    "omega = 1\n",
    "harmonic_omega = 1\n",
    "g = 0\n",
    "sigma = -g/2\n",
    "C = 5\n",
    "FILENAME = \"2_boson_energies_-5.csv\"\n",
    "PARAMS_FILE = \"2_bosons_g-5.npy\"\n",
    "\n",
    "INITIAL_SAMPLE = jnp.array(np.random.uniform(-4, 4, N))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.tail_weight = 1\n",
    "        \n",
    "        # Initialize weights and biases for each layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        if hidden_sizes != [0]:\n",
    "            sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        else:\n",
    "            sizes = [input_size, output_size]\n",
    "\n",
    "        for i in range(len(sizes) - 1):\n",
    "            w = np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2/sizes[i])\n",
    "            b = np.random.randn(1, sizes[i+1]) \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def transform(self, coords):\n",
    "       # if running into NaNs, try to increase this\n",
    "        ret = jnp.zeros(num_particles)\n",
    "        for i in range(num_particles):\n",
    "            ret = ret.at[i].set(jnp.sum(jnp.power(coords/C, i + 1)))\n",
    "        return ret \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __call__(self, x, params):\n",
    "        x = self.transform(x)\n",
    "        self.weights, self.biases, self.tail_weight = self.unflatten_params(params)\n",
    "        a = x\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = jnp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = celu(z)\n",
    "        a = jnp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a[0][0]\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def flatten_params(self):\n",
    "        params = jnp.array([])\n",
    "        for i in range(len(self.weights)):\n",
    "            params = jnp.concatenate((params, self.weights[i].flatten()))\n",
    "            params = jnp.concatenate((params, self.biases[i].flatten()))\n",
    "        params = jnp.append(params, self.tail_weight)\n",
    "        return jnp.array(params)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def unflatten_params(self, params):\n",
    "        tail_weight = params[-1]\n",
    "        params = params[:-1]\n",
    "        weights = []\n",
    "        biases = []\n",
    "        start = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            end = start + self.weights[i].size\n",
    "            weights.append(jnp.reshape(jnp.array(params[start:end]), self.weights[i].shape))\n",
    "            start = end\n",
    "            end = start + self.biases[i].size\n",
    "            biases.append(jnp.reshape(jnp.array(params[start:end]), self.biases[i].shape))\n",
    "            start = end\n",
    "        return weights, biases, tail_weight\n",
    "    \n",
    "\n",
    "# initialize the network\n",
    "nn = NeuralNetwork(num_particles, structure, 1)\n",
    "\n",
    "# symmetrization transformation\n",
    "# I1 = x_1/C + x_2/C + ... + x_N/C\n",
    "# I2 = (x_1/C)^2 + (x_2/C)^2 + ... + (x_N/C)^2\n",
    "# ...\n",
    "# IN = (x_1/C)^N + (x_2/C)^N + ... + (x_N/C)^N\n",
    "\n",
    "@jit\n",
    "def A(coords, params):\n",
    "    return nn(coords, params) + omega * jnp.sum(coords**2) * params[-1]\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-A(coords, params)) \n",
    "\n",
    "# sample_body function except it also returns whether or not the move was accepted\n",
    "# @jit\n",
    "# def sample_body_accept(coords_t, params, key, variation_size):\n",
    "#     gen_rand = jax.random.uniform(key, minval=-variation_size, maxval=variation_size)\n",
    "#     new_key, subkey = jax.random.split(key)\n",
    "    \n",
    "#     coords_prime = coords_t + gen_rand\n",
    "#     r = jax.random.uniform(subkey, minval=0, maxval=1)\n",
    "#     condition = r <= psi(coords_prime, params)**2/psi(coords_t, params)**2\n",
    "#     return (jax.lax.cond(condition, lambda x, _: x, lambda _, y : y, coords_prime, coords_t), new_key, condition)\n",
    "\n",
    "\n",
    "# the sample function without any thermalization steps or skipping steps\n",
    "def accept_ratio(params, num_samples=10**3, variation_size=5.0, key=jax.random.PRNGKey(np.random.randint(0,100))):\n",
    "    coords_t = np.random.uniform(-variation_size, variation_size)\n",
    "    num_accepted = 0\n",
    "    for _ in range(num_samples):\n",
    "        coords_t, key, accepted = sample_body_accept(coords_t, params, key, variation_size)\n",
    "        if accepted:\n",
    "            num_accepted += 1\n",
    "\n",
    "    return num_accepted / num_samples\n",
    "\n",
    "\n",
    "#### New sampling function\n",
    "# def sample(params, num_samples=10**3, thermalization_steps=200, skip_count=50, variation_size=1.0):\n",
    "#     outputs = []\n",
    "#     num_accepted = 0\n",
    "#     num_total = num_samples * skip_count + thermalization_steps + 1\n",
    "#     rand_coords = np.random.uniform(-variation_size, variation_size, size=(num_total, num_particles))\n",
    "#     rand_accepts = np.random.uniform(0, 1, size=num_total)\n",
    "\n",
    "#     coords_t = jnp.zeros(num_particles)\n",
    "#     for step in range(num_total):\n",
    "#         coords_t, accepted = sample_body(params, coords_t, rand_coords[step], rand_accepts[step])\n",
    "#         if accepted:\n",
    "#             num_accepted += 1\n",
    "#         if ((step > thermalization_steps) and (step % skip_count == 0)):\n",
    "#             outputs.append(coords_t)\n",
    "#     # create a second output array, where the second coordinate is equal to the first coordinate\n",
    "#     outputs_prime = outputs.copy()\n",
    "#     for i in range(len(outputs)):\n",
    "#         a = np.array(outputs[i])\n",
    "#         a[1] = a[0]\n",
    "#         outputs_prime[i] = jnp.array(a)\n",
    "#     return jnp.array(outputs), jnp.array(outputs_prime), num_accepted/num_total\n",
    "\n",
    "\n",
    "@jit\n",
    "def mcstep_E(xis, limit, positions, params):\n",
    "    \n",
    "    params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "    \n",
    "    newpositions = jnp.array(positions) + xis\n",
    "    \n",
    "    prob = psi(newpositions, params)**2./psi(positions, params)**2.\n",
    "    \n",
    "    def truefunc(p):\n",
    "        return [newpositions, True]\n",
    "\n",
    "    def falsefunc(p):\n",
    "        return [positions, False]\n",
    "    \n",
    "    return jax.lax.cond(prob >= limit, truefunc, falsefunc, prob)\n",
    "\n",
    "def sample(params, Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "    sq = []\n",
    "    sq_prime = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "    randoms = np.random.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = np.random.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "    # generate the primed samples by going through every sample and making sample[N_up] = sample[0]\n",
    "    sq_prime = sq.copy()\n",
    "    for i in range(len(sq)):\n",
    "        a = np.array(sq[i])\n",
    "        a[1] = a[0]\n",
    "        sq_prime[i] = jnp.array(a) \n",
    "\n",
    "    return jnp.array(sq), jnp.array(sq_prime) , counter/num_total\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def sample_body(params, coords_t, rand_coords, rand_accepts):\n",
    "    coords_prime = coords_t + rand_coords\n",
    "    return jax.lax.cond(rand_accepts < psi(coords_prime, params)**2/psi(coords_t, params)**2, lambda x,_: (x,True) , lambda _,y: (y,False), coords_prime, coords_t)\n",
    "\n",
    "\n",
    "# ----- Adapted from fermion code -----\n",
    "@partial(jit, static_argnums=(1,2,3,4))\n",
    "def sample_pmap(params, Nsweeps, Ntherm, keep, stepsize, key, positions_initial=INITIAL_SAMPLE):\n",
    "\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    sq = jnp.empty((Nsweeps+1, N))\n",
    "    sq_prime = jnp.empty((Nsweeps+1, N))\n",
    "    \n",
    "    # How many keys do we need?\n",
    "    subkeys = jax.random.split(key, N)\n",
    "    # key, shape, datatype, then bounds\n",
    "    randoms = vmap(jax.random.uniform, in_axes=(0,None,None, None,None))(subkeys,(num_total,), jnp.float32,-stepsize, stepsize)\n",
    "    randoms = jnp.transpose(randoms)\n",
    "    subkeys = jax.random.split(subkeys[-1], num_total)\n",
    "    limits = vmap(jax.random.uniform, in_axes=(0,None, None, None, None))(subkeys,(), jnp.float32,0.0,1.0)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    \n",
    "    def true_fun(sq, new, i):\n",
    "        return sq.at[(i - Ntherm)//keep].set(new)\n",
    "    def false_fun(sq, new, i):\n",
    "        return sq\n",
    "    \n",
    "    def body_fun(i, val):\n",
    "        # unpack val\n",
    "        sq, positions_prev = val\n",
    "        new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        sq = jax.lax.cond(jnp.logical_and(jnp.mod(i, keep) == 0,i >= Ntherm), true_fun, false_fun, sq, new, i)\n",
    "        positions_prev = new\n",
    "        return sq, positions_prev\n",
    "    \n",
    "    sq, _ = jax.lax.fori_loop(0, num_total, body_fun, (sq, positions_prev))\n",
    "\n",
    "    def set_prime(a):\n",
    "        # Set sample[N_up] = sample[0] for each sample\n",
    "        return a.at[1].set(a[0])\n",
    "\n",
    "    # Apply the `set_prime` function to every sample in `sq` using vmap\n",
    "    sq_prime = jax.vmap(set_prime)(sq)\n",
    "        \n",
    "    return jnp.array(sq), jnp.array(sq_prime)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# first derivative of the neural network with respect to the coordinates\n",
    "# in Andy's notation this is dA/dx\n",
    "dA_dx = jit(grad(A, 0)) # type: ignore\n",
    "\n",
    "# second derivative of the neural network with respect to the coordinates\n",
    "# in Andy's notation this is d^2A/dx^2\n",
    "A_hessian = jax.jacfwd(dA_dx, 0) # type: ignore\n",
    "\n",
    "@jit\n",
    "def d2A_dx2(coords, params):\n",
    "    #return jnp.diagonal(A_hessian(transform(coords), params))\n",
    "    return jnp.diag(A_hessian(coords, params))\n",
    "\n",
    "@jit\n",
    "def Hpsi(coords, coords_prime, params, alpha,g,sigma):\n",
    "    return Hpsi_without_delta(coords, params,sigma) + delta_potential(coords,coords_prime, params, alpha,g)\n",
    "\n",
    "@jit\n",
    "def sigma_term(coords,sigma):\n",
    "    N = num_particles \n",
    "    sigma_term = 0\n",
    "    for i in range(N):\n",
    "        for j in range(i,N):\n",
    "            sigma_term += sigma* jnp.abs(coords[i] - coords[j])  \n",
    "\n",
    "@jit\n",
    "def Hpsi_without_delta(coords, params,sigma):\n",
    "   # sigma term\n",
    "    N = num_particles \n",
    "    sigma_term = 0\n",
    "    for i in range(N):\n",
    "        for j in range(i,N):\n",
    "            sigma_term += sigma * jnp.abs(coords[i] - coords[j]) \n",
    "    # return jnp.sum((m*.5*omega**2*coords**2)) - hbar**2 / (2*m) * jnp.sum(ddpsi(coords, params) ) * 1/psi(coords, params) + sigma_term \n",
    "    return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(dA_dx(coords, params)**2)) + m*.5*harmonic_omega**2* jnp.sum(coords**2) + sigma_term\n",
    "    # return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(dA_dx(coords, params)**2))\n",
    "    # return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(dA_dx(coords, params)**2)) + m*.5*omega**2* jnp.sum(coords**2)\n",
    "\n",
    "@jit\n",
    "def second_term(coords, params, g,sigma):\n",
    "    return dnn_dtheta(coords, params) * Hpsi_without_delta(coords, params,sigma)\n",
    "\n",
    "vsecond_term = jit(vmap(second_term, in_axes=(0, None, None,None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def third_term(coords,coords_prime, params, y_max,g):\n",
    "    return dnn_dtheta(coords_prime, params) * delta_potential(coords, coords_prime, params, y_max, g)\n",
    "\n",
    "vthird_term = jit(vmap(third_term, in_axes=(0,0, None, None,None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def delta_potential(coords, coords_prime, params, alpha, g):\n",
    "    N = num_particles    \n",
    "    # compute e^(-2 NN(params_prime))\n",
    "    # ratio = jnp.exp(-2 * A(coords_prime, params) + 2 * A(coords, params))\n",
    "    ratio = (psi(coords_prime, params)**2)/(psi(coords, params)**2)\n",
    "    delta_dist = (1/(jnp.sqrt(jnp.pi) * alpha)) * jnp.exp(-(coords[1]**2)/(alpha**2))\n",
    "    return g * N*(N-1)/2 * ratio * delta_dist\n",
    "\n",
    "vdelta_potential = jit(vmap(delta_potential, in_axes=(0,0, None, None,None), out_axes=0))\n",
    "venergy = jit(vmap(Hpsi, in_axes=(0,0, None, None,None,None), out_axes=0))\n",
    "vHpsi_without_delta = jit(vmap(Hpsi_without_delta, in_axes=(0, None,None), out_axes=0))\n",
    "\n",
    "\n",
    "# derivative of the neural network with respect to every parameter\n",
    "# in Andy's notation this is dA/dtheta\n",
    "dnn_dtheta = jit(grad(A, 1)) \n",
    "vdnn_dtheta = vmap(dnn_dtheta, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "def gradient(params, num_samples=10**3, thermal=200, skip=50, variation_size=1.0, verbose=False,g=0,sigma=0):\n",
    "    # get the samples\n",
    "    samples, samples_prime  = sample_pmap(params, num_samples, thermal, skip, variation_size, jax.random.key(int(time.time())))\n",
    "#     samples, samples_prime,_ = sample(params, num_samples, thermal, skip, variation_size, INITIAL_SAMPLE)\n",
    "\n",
    "    y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "    alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "\n",
    "    psiHpsi = venergy(samples, samples_prime, params, alpha,g,sigma) \n",
    "    # Hpsi_terms_without_delta = vHpsi_without_delta(samples, params)\n",
    "    # delta_term = vdelta_potential(samples,samples_prime, params, samples)\n",
    "\n",
    "    # delta function additions\n",
    "    dA_dtheta = vdnn_dtheta(samples, params)\n",
    "    # dA_dtheta_repeated = vdnn_dtheta(samples_prime, params)\n",
    "\n",
    "    dA_dtheta_avg = 1/num_samples * jnp.sum(dA_dtheta, 0)\n",
    "\n",
    "    second_term = 1/num_samples * jnp.sum(vsecond_term(samples, params,g,sigma), 0)\n",
    "    third_term = 1/num_samples * jnp.sum(vthird_term(samples, samples_prime, params, alpha,g), 0)\n",
    "    # third_term =1/num_samples * jnp.sum(vboth(dA_dtheta_repeated,delta_term), 0)\n",
    "    uncert = jnp.std(psiHpsi)/jnp.sqrt(num_samples)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(psiHpsi)\n",
    "\n",
    "   \n",
    "    if verbose:\n",
    "        print(energy)\n",
    "\n",
    "    gradient_calc = 2 * energy * dA_dtheta_avg - 2 * second_term - 2*third_term\n",
    "    return gradient_calc, energy, uncert\n",
    "\n",
    "def ugradient(params, num_samples=10**3, thermal=200, skip=50, variation_size=1.0, verbose=False):\n",
    "\n",
    "    samples, samples_prime, _ = sample(params, num_samples, thermal, skip, variation_size)\n",
    "    y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "    alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "    Es = []\n",
    "    dA_dthetas = []\n",
    "    seconds = []\n",
    "    thirds = []\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        coord = samples[i]\n",
    "        coord_prime = samples_prime[i]\n",
    "\n",
    "        Es.append(Hpsi(coord, coord_prime, params, alpha))\n",
    "        dA_dthetas.append(dnn_dtheta(coord, params)) \n",
    "        seconds.append(second_term(coord, params))\n",
    "        thirds.append(third_term(coord, coord_prime, params, alpha))\n",
    "\n",
    "\n",
    "    Es = jnp.array(Es)\n",
    "    dA_dthetas = jnp.array(dA_dthetas)\n",
    "    seconds = jnp.array(seconds)\n",
    "    thirds = jnp.array(thirds)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(Es)\n",
    "    avg_dA_dtheta = 1/num_samples * jnp.sum(dA_dthetas, 0)\n",
    "    second = 1/num_samples * jnp.sum(seconds, 0)\n",
    "    third =  1/num_samples * jnp.sum(thirds, 0)\n",
    "\n",
    "    uncert = jnp.std(Es)/jnp.sqrt(num_samples)\n",
    "    \n",
    "    gradient_calc = 2 * energy * avg_dA_dtheta - 2 * second - 2 * third\n",
    "    return gradient_calc, energy, uncert\n",
    "\n",
    "\n",
    "# define a function that takes in samples, bins them, and returns the average of each bin\n",
    "def bin_samples(energies, bin_size):\n",
    "    # first, bin the samples\n",
    "    binned = np.array_split(energies, bin_size)\n",
    "    # now, calculate the average of each bin\n",
    "    binned_averages = [np.mean(b) for b in binned]\n",
    "    # now, calculate the uncertainty of each bin\n",
    "    bin_uncerts = np.std(binned_averages)/np.sqrt(bin_size)\n",
    "    return bin_uncerts\n",
    "\n",
    "\n",
    "# define a function that gets all samples, and then bins them with different bin sizes\n",
    "def autocorrelation(params):\n",
    "    samples = sample(params, num_samples=10**3, thermalization_steps=200, skip_count=40, variation_size=1)[0]\n",
    "    energies = [Hpsi(s, params) for s in samples]\n",
    "    \n",
    "    bins = np.linspace(1, 100, 100, dtype=int)\n",
    "    # now plot the average energy as a function of the number of bins\n",
    "    us = []\n",
    "    for b_size in bins:\n",
    "        us.append(bin_samples(energies, b_size))\n",
    "    plt.scatter(bins, us)\n",
    "    plt.title(\"Bin size vs. Uncertainty\")\n",
    "    plt.xlabel(\"Bin size\")\n",
    "    plt.ylabel(\"Uncertainty\")\n",
    "    plt.show()\n",
    "\n",
    "def step(params_arg, step_num, N, thermal, skip, variation_size,g, sigma):\n",
    "        gr = gradient(params_arg, N, thermal, skip, variation_size,False,g,sigma)\n",
    "        # print(gr)\n",
    "        # hs.append(gr[1])\n",
    "        # us.append(gr[2])\n",
    "        opt_state = opt_init(params_arg)\n",
    "        new = opt_update(step_num, gr[0], opt_state)\n",
    "        return get_params(new), gr[1], gr[2]\n",
    "\n",
    "def train(params, iterations, N, thermal, skip, variation_size, g, sigma):\n",
    "    hs = []\n",
    "    us = []\n",
    "    ns = np.arange(iterations)\n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:\n",
    "        new_params, energy, uncert = step(\n",
    "            old_params, step_num, N, thermal, skip, variation_size, g, sigma\n",
    "        )\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        # write the energy to the file\n",
    "        with open(FILENAME, mode=\"a\") as f:\n",
    "            f.write(str(energy) + \",\" + str(uncert) + \"\\n\")\n",
    "        with open(PARAMS_FILE, \"wb\") as f:\n",
    "            jnp.save(f, new_params)\n",
    "        old_params = new_params.copy()\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "\n",
    "\n",
    "def train_notqdm(params, iterations, N, thermal, skip, variation_size):\n",
    "    hs = []\n",
    "    us = []\n",
    "    ns = np.arange(iterations)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in range(iterations):\n",
    "        new_params, energy, uncert = step(\n",
    "            old_params, step_num, N, thermal, skip, variation_size\n",
    "        )\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        # write the energy to the file\n",
    "        with open(FILENAME, mode=\"a\") as f:\n",
    "            f.write(str(energy) + \",\" + str(uncert) + \"\\n\")\n",
    "        with open(PARAMS_FILE, \"wb\") as f:\n",
    "            jnp.save(f, new_params)\n",
    "        old_params = new_params.copy()\n",
    "#         pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        print(str(energy))\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_step_size(params, start):\n",
    "    lr = .1\n",
    "    target = 0.5\n",
    "    tolerance = .05\n",
    "    max_it = 1000\n",
    "    step = start\n",
    "    best_step = start\n",
    "    best_acc = 0\n",
    "    it_num = 0\n",
    "    # get the samples \n",
    "    _, _, acc = sample(params, 1000, 100, 10, step)\n",
    "    # while the acceptance rate is not within +/- .5 of the target\n",
    "    while (acc < target - tolerance or acc > target + tolerance) and it_num < max_it:\n",
    "        it_num += 1\n",
    "        # if the acceptance rate is too low, increase the step size\n",
    "        if acc < target - tolerance:\n",
    "            step -= lr\n",
    "        # if the acceptance rate is too high, decrease the step size\n",
    "        elif acc > target + tolerance:\n",
    "            step += lr\n",
    "        # if we cross the target, decrease the learning rate and go back\n",
    "        if (acc < target and best_acc > target) or (acc > target and best_acc < target):\n",
    "            lr /= 2\n",
    "            step = best_step\n",
    "        # keep track of the best step size\n",
    "        if abs(acc - target) < abs(best_acc - target):\n",
    "            best_acc = acc\n",
    "            best_step = step\n",
    "        \n",
    "        # get the samples for the next step size\n",
    "        _, _, acc = sample(params, 1000, 100, 10, step)\n",
    "#     print(\"step size:\",best_step)\n",
    "    return best_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters 5402\n",
      "Log of parameters 3.732554579851432\n"
     ]
    }
   ],
   "source": [
    "print(\"number of parameters\" , len(nn.flatten_params()))\n",
    "print(\"Log of parameters\", np.log10(len(nn.flatten_params())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.78462065, dtype=float64)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi(jnp.array([0,0,0,0]),nn.flatten_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = 2.006045468391615: 100%|██████████| 100/100 [00:49<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "open(FILENAME, 'w').close()\n",
    "# start_params = nn.flatten_params()\n",
    "start_params = nn.flatten_params()\n",
    "# start_params = jnp.load(\"40_params_best.npy\", allow_pickle=True)\n",
    "\n",
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-3))\n",
    "\n",
    "g=0\n",
    "sigma = -g/2\n",
    "\n",
    "resultsa = train(start_params, 100, 5000, 50, 5, find_step_size(start_params, .85),g,sigma)\n",
    "# 0 -> energies\n",
    "# 1 -> uncert\n",
    "# 2 -> steps\n",
    "# 3 -> params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618044571043636\n"
     ]
    }
   ],
   "source": [
    "print(resultsa[3][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = 1.999285382941942: 100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-3))\n",
    "g = .05\n",
    "sigma = -g/2\n",
    "resultsb = train(resultsa[3], 100, 7000, 1000, 5, find_step_size(resultsa[3], 1), g, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = 1.9775781807078192: 100%|██████████| 200/200 [02:05<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "g = .1\n",
    "sigma = -g/2\n",
    "\n",
    "resultsc = train(resultsb[3], 200, 7000, 1000, 5, find_step_size(resultsb[3], 1),g,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = 1.4470001308557463: 100%|██████████| 400/400 [1:21:35<00:00, 12.24s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "g = .5\n",
    "sigma = -g/2\n",
    "resultsd = train(resultsc[3], 400, 100000, 1000, 5, find_step_size(resultsc[3], 1),g,sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-3))\n",
    "g= .8\n",
    "sigma = -g/2\n",
    "resultse = train(resultsd[3], 100, 5000, 1000, 3, find_step_size(resultsd[3], 1),g,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "g = 1.0\n",
    "sigma = -g/2\n",
    "resultsf = train(resultse[3], 200, 10000, 1000, 3, find_step_size(resultse[3], 1),g,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "g = 1.5\n",
    "sigma = -g/2\n",
    "resultsg = train(resultsf[3], 200, 6000, 1000, 3, find_step_size(resultsf[3], 1),g,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "g = 2.0\n",
    "sigma = -g/2\n",
    "resultsh = train(resultsg[3], 200, 6000, 1000, 3, find_step_size(resultsg[3], 1),g,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "g = 2.5\n",
    "sigma = -g/2\n",
    "resultsi = train(resultsh[3], 200, 7000, 1000, 3, find_step_size(resultsh[3], 1),g,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "g = 3.0\n",
    "sigma = -g/2\n",
    "resultsj = train(resultsi[3], 200, 7000, 1000, 3, find_step_size(resultsi[3], 1),g,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9742(21)\n",
      "Minimum value:  1.9742(21)\n",
      "Fractional percent error:  -0.04(10)\n",
      "1.9742483027699456\n",
      "0.002051863232386593\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.special import comb\n",
    "def harmonic_energy(n):\n",
    "    return .5 + 2*n - np.sqrt(2/np.pi) * (n+1/2)/g\n",
    "\n",
    "def astra_energy():\n",
    "    return (N * omega)/2 - m * g**2  * (N*(N**2 - 1))/(24)\n",
    "\n",
    "\n",
    "# true_energy = .5 * hbar * omega * num_particles\n",
    "# true_energy = harmonic_energy(0)\n",
    "true_energy = astra_energy()\n",
    "#g = 1, sigma = -g/2\n",
    "# true_energy = .75\n",
    "# g = .1, sigma = 0:\n",
    "# true_energy = 1.03881\n",
    "# g= .8, sigma= -g\n",
    "# true_energy = .9375\n",
    "# true_energy = 0.3098\n",
    "\n",
    "total_hists =  resultsa[0]  + resultsb[0]  + resultsc[0] \n",
    "# + resultsd[0] \n",
    "# + resultse[0]+ resultsf[0]\n",
    "# + resultsg[0]+ resultsh[0]+ resultsi[0]+ resultsj[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]+ resultsb[1]  + resultsc[1] \n",
    "# + resultsd[1] \n",
    "# + resultse[1]+ resultsf[1]\n",
    "# + resultsg[1]+ resultsh[1]+ resultsi[1]+ resultsj[1]\n",
    "# + resultsd[1]\n",
    "\n",
    "# get index of minimum value\n",
    "min_val = np.min(total_hists)\n",
    "# min_val = total_hists[-1]\n",
    "min_index = total_hists.index(min_val)\n",
    "min_err = total_uncerts[min_index]\n",
    "val = gv.gvar(min_val, min_err)\n",
    "print(val)\n",
    "fractional_error = (val - true_energy)/true_energy\n",
    "print(\"Minimum value: \", val)\n",
    "print(\"Fractional percent error: \", fractional_error * 100)\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists, label=\"Adam: \" + str(val))\n",
    "# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# get the network structure\n",
    "structure = nn.hidden_sizes\n",
    "plt.annotate(\" Network Structure: \" + str(structure), xy=(0.1, 0.95), xycoords='axes fraction')\n",
    "plt.plot(np.arange(0, len(total_hists)), [true_energy for x in np.arange(0, len(total_hists))], label=r\"Analytic: \" + str(round(true_energy,3)))\n",
    "pdiff = (min_val - true_energy)/true_energy*100\n",
    "# plt.annotate(\" Final Percent Diff = \" + str(round(pdiff,3)) + \"%\", xy=(.1, .9), xycoords= 'axes fraction')\n",
    "plt.legend()\n",
    "plt.title(r\"Modified LL model ($g = $\" + str(g) + r\", $\\sigma =\" + str(sigma) + \"$), \" + str(num_particles) + \" particles\")\n",
    "print(min_val)\n",
    "print(min_err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "1.9940112653017317\n",
      "140\n",
      "1.994679012589097\n",
      "142\n",
      "1.9940881874988836\n",
      "172\n",
      "1.9938661014576153\n",
      "200\n",
      "1.9881648704795016\n",
      "201\n",
      "1.9921882648677047\n",
      "202\n",
      "1.9915376442184354\n",
      "203\n",
      "1.9854869847782215\n",
      "204\n",
      "1.9864493589125258\n",
      "205\n",
      "1.9850388796884066\n",
      "206\n",
      "1.98710323466482\n",
      "207\n",
      "1.9841999945484396\n",
      "208\n",
      "1.9831363396408084\n",
      "209\n",
      "1.984919734345515\n",
      "210\n",
      "1.9811879496578748\n",
      "211\n",
      "1.9779329889733808\n",
      "212\n",
      "1.9863405340962283\n",
      "213\n",
      "1.9821777926047364\n",
      "214\n",
      "1.9822776770714259\n",
      "215\n",
      "1.9829772908001204\n",
      "216\n",
      "1.9815857212042451\n",
      "217\n",
      "1.9800979534876324\n",
      "218\n",
      "1.9812028543228075\n",
      "219\n",
      "1.98238440897728\n",
      "220\n",
      "1.981339061209512\n",
      "221\n",
      "1.9762317407376577\n",
      "222\n",
      "1.9831217753253743\n",
      "223\n",
      "1.9806259969598108\n",
      "224\n",
      "1.9772397179981525\n",
      "225\n",
      "1.9816103784036112\n",
      "226\n",
      "1.9796512042801837\n",
      "227\n",
      "1.9813127986321846\n",
      "228\n",
      "1.9816401900323566\n",
      "229\n",
      "1.9823165103534048\n",
      "230\n",
      "1.979485256175491\n",
      "231\n",
      "1.9801511375380023\n",
      "232\n",
      "1.9795879889942138\n",
      "233\n",
      "1.981121830620515\n",
      "234\n",
      "1.9778196746675958\n",
      "235\n",
      "1.97726521931254\n",
      "236\n",
      "1.9815343037339586\n",
      "237\n",
      "1.97995405266787\n",
      "238\n",
      "1.9790057227301847\n",
      "239\n",
      "1.982095569853911\n",
      "240\n",
      "1.9799004083291958\n",
      "241\n",
      "1.982377560996225\n",
      "242\n",
      "1.9824151670784218\n",
      "243\n",
      "1.9819060320686026\n",
      "244\n",
      "1.9803680125136958\n",
      "245\n",
      "1.9817210869411308\n",
      "246\n",
      "1.9781239466435023\n",
      "247\n",
      "1.9770608594898076\n",
      "248\n",
      "1.9791052153463384\n",
      "249\n",
      "1.9792508580944168\n",
      "250\n",
      "1.981783191735927\n",
      "251\n",
      "1.9796580871855072\n",
      "252\n",
      "1.979633503014276\n",
      "253\n",
      "1.9790619386415031\n",
      "254\n",
      "1.9818766169934385\n",
      "255\n",
      "1.9786423336253551\n",
      "256\n",
      "1.9823511468998072\n",
      "257\n",
      "1.9813498981329407\n",
      "258\n",
      "1.980920083095558\n",
      "259\n",
      "1.9813846341617942\n",
      "260\n",
      "1.982074909045963\n",
      "261\n",
      "1.9800797375736199\n",
      "262\n",
      "1.9809599950690817\n",
      "263\n",
      "1.9805516771088345\n",
      "264\n",
      "1.9780003187404938\n",
      "265\n",
      "1.979284076243595\n",
      "266\n",
      "1.9798342518717833\n",
      "267\n",
      "1.9793428687585797\n",
      "268\n",
      "1.9770091222581787\n",
      "269\n",
      "1.9820084484753744\n",
      "270\n",
      "1.9798925503611102\n",
      "271\n",
      "1.9808417192017247\n",
      "272\n",
      "1.9797004668341773\n",
      "273\n",
      "1.9783363054928202\n",
      "274\n",
      "1.9822632409804326\n",
      "275\n",
      "1.983648486038705\n",
      "276\n",
      "1.9793022562369067\n",
      "277\n",
      "1.9772358836238604\n",
      "278\n",
      "1.9787334273431485\n",
      "279\n",
      "1.9791521089024735\n",
      "280\n",
      "1.9770858164346148\n",
      "281\n",
      "1.9783045834577337\n",
      "282\n",
      "1.9804093644938867\n",
      "283\n",
      "1.9764432263799199\n",
      "284\n",
      "1.9772813119000194\n",
      "285\n",
      "1.9788510201422405\n",
      "286\n",
      "1.9833922904640975\n",
      "287\n",
      "1.9807153860545206\n",
      "288\n",
      "1.9757255510454017\n",
      "289\n",
      "1.9821108836435988\n",
      "290\n",
      "1.9809236356093063\n",
      "291\n",
      "1.9808208322280778\n",
      "292\n",
      "1.9798406038721887\n",
      "293\n",
      "1.9796439342286278\n",
      "294\n",
      "1.9777182094639467\n",
      "295\n",
      "1.9807455675312458\n",
      "296\n",
      "1.9786977700054913\n",
      "297\n",
      "1.980636990029471\n",
      "298\n",
      "1.983597748798672\n",
      "299\n",
      "1.9820453643332587\n",
      "300\n",
      "1.9810879431822097\n",
      "301\n",
      "1.9773614360519065\n",
      "302\n",
      "1.981553393759008\n",
      "303\n",
      "1.9792603990929827\n",
      "304\n",
      "1.9825647802120456\n",
      "305\n",
      "1.9790476664284689\n",
      "306\n",
      "1.9747608606993117\n",
      "307\n",
      "1.9789648438632323\n",
      "308\n",
      "1.980868212087689\n",
      "309\n",
      "1.9804111777319757\n",
      "310\n",
      "1.9789586387322573\n",
      "311\n",
      "1.977450731723317\n",
      "312\n",
      "1.9750173742891655\n",
      "313\n",
      "1.9838457749794078\n",
      "314\n",
      "1.984864981545108\n",
      "315\n",
      "1.98289475493127\n",
      "316\n",
      "1.981862101148581\n",
      "317\n",
      "1.984574209129822\n",
      "318\n",
      "1.977529512302565\n",
      "319\n",
      "1.9782681182967172\n",
      "320\n",
      "1.979682755622285\n",
      "321\n",
      "1.9778908673930455\n",
      "322\n",
      "1.9800904828810248\n",
      "323\n",
      "1.981335360062932\n",
      "324\n",
      "1.9806574653925815\n",
      "325\n",
      "1.9782344400142782\n",
      "326\n",
      "1.9742483027699456\n",
      "327\n",
      "1.979424553432781\n",
      "328\n",
      "1.9835592879323194\n",
      "329\n",
      "1.9805927229235907\n",
      "330\n",
      "1.9792478234682034\n",
      "331\n",
      "1.9796824021109025\n",
      "332\n",
      "1.9742895591383476\n",
      "333\n",
      "1.9771926223559884\n",
      "334\n",
      "1.9806403223986824\n",
      "335\n",
      "1.9813090915026934\n",
      "336\n",
      "1.9796957854069859\n",
      "337\n",
      "1.9815846803049297\n",
      "338\n",
      "1.9811871365643074\n",
      "339\n",
      "1.9845810479362518\n",
      "340\n",
      "1.9787179571633269\n",
      "341\n",
      "1.982049923004772\n",
      "342\n",
      "1.9820322032306572\n",
      "343\n",
      "1.9794966321207492\n",
      "344\n",
      "1.978547828742564\n",
      "345\n",
      "1.9761769978345194\n",
      "346\n",
      "1.982080513915886\n",
      "347\n",
      "1.9817898548374673\n",
      "348\n",
      "1.9798213270546385\n",
      "349\n",
      "1.9813798910101792\n",
      "350\n",
      "1.9817748188139035\n",
      "351\n",
      "1.9799657199584186\n",
      "352\n",
      "1.9774204697103712\n",
      "353\n",
      "1.9805138012138888\n",
      "354\n",
      "1.980690381809631\n",
      "355\n",
      "1.9837228820354453\n",
      "356\n",
      "1.9818203859008148\n",
      "357\n",
      "1.9795653610133408\n",
      "358\n",
      "1.9774843784635716\n",
      "359\n",
      "1.9789078983033912\n",
      "360\n",
      "1.9771607769661057\n",
      "361\n",
      "1.9800228099407038\n",
      "362\n",
      "1.9795726444507962\n",
      "363\n",
      "1.9815245995325417\n",
      "364\n",
      "1.9790950155776872\n",
      "365\n",
      "1.983707044166238\n",
      "366\n",
      "1.9826427423104165\n",
      "367\n",
      "1.9780051480697145\n",
      "368\n",
      "1.9787878705827826\n",
      "369\n",
      "1.977944726468879\n",
      "370\n",
      "1.9805515691589106\n",
      "371\n",
      "1.98381012576072\n",
      "372\n",
      "1.9783739392540527\n",
      "373\n",
      "1.978739296635904\n",
      "374\n",
      "1.98348418994917\n",
      "375\n",
      "1.9770421160942422\n",
      "376\n",
      "1.977286649119302\n",
      "377\n",
      "1.9766008321919124\n",
      "378\n",
      "1.9825363797123634\n",
      "379\n",
      "1.98007160600527\n",
      "380\n",
      "1.977600540968871\n",
      "381\n",
      "1.9796850177705538\n",
      "382\n",
      "1.9746224320724992\n",
      "383\n",
      "1.9806372548460198\n",
      "384\n",
      "1.9791000002498436\n",
      "385\n",
      "1.9827079191924277\n",
      "386\n",
      "1.97927805667263\n",
      "387\n",
      "1.9763735639316216\n",
      "388\n",
      "1.9825236930063521\n",
      "389\n",
      "1.9800614617832966\n",
      "390\n",
      "1.9827134128245345\n",
      "391\n",
      "1.9787635588046972\n",
      "392\n",
      "1.9775745744476882\n",
      "393\n",
      "1.9775197757157252\n",
      "394\n",
      "1.9793694096893566\n",
      "395\n",
      "1.9785327495934435\n",
      "396\n",
      "1.9823772248303695\n",
      "397\n",
      "1.984179718299806\n",
      "398\n",
      "1.9782879432096083\n",
      "399\n",
      "1.9775781807078192\n"
     ]
    }
   ],
   "source": [
    "#finding the first step that has below 1% error\n",
    "for stepnum in range(len(total_hists)):\n",
    "    energy = total_hists[stepnum]\n",
    "    pdiff = (energy - true_energy)/true_energy * 100\n",
    "    if np.abs(pdiff) <= 1:\n",
    "        print(stepnum)\n",
    "        print(total_hists[stepnum])\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_samples(energies, bin_size):\n",
    "    # first, bin the samples\n",
    "    binned = np.array_split(energies, bin_size)\n",
    "    # now, calculate the average of each bin\n",
    "    binned_averages = [np.mean(b) for b in binned]\n",
    "    # now, calculate the uncertainty of each bin\n",
    "    bin_uncerts = np.std(binned_averages)/np.sqrt(bin_size)\n",
    "    return bin_uncerts\n",
    "\n",
    "\n",
    "samples, samples_prime, _ = sample(params, 4*10**4, 100, 10, 1)\n",
    "y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "energies = venergy(samples,samples_prime, params, alpha)\n",
    "mean_energy = jnp.mean(energies)\n",
    "print(mean_energy)\n",
    "\n",
    "# bins = np.linspace(1, 100, 100, dtype=int)\n",
    "bins = np.array([1,2,5,10,20,50,100,150,200,250,300,360,450,500,550,600,660,750,900,990,1100])\n",
    "# now plot the average energy as a function of the number of bins\n",
    "us = []\n",
    "for b_size in bins:\n",
    "    us.append(bin_samples(energies, b_size))\n",
    "plt.scatter(bins, us)\n",
    "plt.title(\"Bin size vs. Uncertainty\")\n",
    "plt.xlabel(\"Bin size\")\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "print(max(us))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = gv.gvar(mean_energy, max(us))\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, .05)\n",
    "ys = np.arange(-5, 5, .05)\n",
    "wavs = []\n",
    "for i in range(len(xs)):\n",
    "    for j in range(len(ys)):\n",
    "        wavs.append(psi(np.array([xs[i], ys[j]]), params)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(xs,ys)\n",
    "Z = np.array(wavs).reshape(len(xs), len(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.contourf(X, Y, Z, 100)\n",
    "\n",
    "plt.plot(xs,-xs)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def astra_wf(coords):\n",
    "    ret = 1\n",
    "    a_s = -2/(m*g) \n",
    "    a_ho = jnp.sqrt(1/(m * harmonic_omega))\n",
    "    for i in range(N):\n",
    "        for j in range(0,i):\n",
    "            ret *= jnp.exp(-jnp.abs(coords[i] - coords[j])/a_s)\n",
    "        ret *= jnp.exp(-coords[i]**2/(2*a_ho**2))\n",
    "    return ret\n",
    "\n",
    "@jit\n",
    "def mcstep_E_exact(xis, limit, positions):\n",
    "    newpositions = jnp.array(positions) + xis\n",
    "    \n",
    "    prob = astra_wf(newpositions)**2 / astra_wf(positions)**2\n",
    "    \n",
    "    def truefunc(p):\n",
    "        return [newpositions, True]\n",
    "\n",
    "    def falsefunc(p):\n",
    "        return [positions, False]\n",
    "    \n",
    "    return jax.lax.cond(prob >= limit, truefunc, falsefunc, prob)\n",
    "\n",
    "def sample_exact(Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "    sq = []\n",
    "    sq_prime = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "\n",
    "    randoms = np.random.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = np.random.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E_exact(randoms[i], limits[i], positions_prev)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E_exact(randoms[i], limits[i], positions_prev)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "    # generate the primed samples by going through every sample and making sample[N_up] = sample[0]\n",
    "    sq_prime = sq.copy()\n",
    "    for i in range(len(sq)):\n",
    "        a = np.array(sq[i])\n",
    "        a[1] = a[0]\n",
    "        sq_prime[i] = jnp.array(a) \n",
    "\n",
    "    return jnp.array(sq), jnp.array(sq_prime), counter/num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(params, 3 * 10**4, 100, 10, .85)\n",
    "samples_exact = sample_exact(3 * 10**4, 100, 10, .85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_density(samples):\n",
    "    x_bins = np.linspace(-9, 9, 72)\n",
    "    dx = x_bins[1] - x_bins[0]  # Bin width\n",
    "    n_x = np.zeros_like(x_bins)\n",
    "\n",
    "    # bin the x_1s\n",
    "    for x in samples[0][:,0]:\n",
    "        n_x[np.digitize(x, x_bins)] += 1\n",
    "\n",
    "    # Normalize\n",
    "    n_x /= (dx * np.sum(n_x))\n",
    "\n",
    "    return x_bins, n_x\n",
    "\n",
    "x_bins, n_x = local_density(samples)\n",
    "x_bins_exact, n_x_exact = local_density(samples_exact)\n",
    "plt.plot(x_bins, n_x,'-o', markersize=2, color=\"red\")\n",
    "plt.plot(x_bins_exact, n_x_exact,'-o', markersize=2, color=\"black\")\n",
    "plt.title(r\"$N = 2$ Local Density Profile\")\n",
    "plt.xlabel(\"$x$\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.ylabel(\"$n(x)/\\int dx n(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
