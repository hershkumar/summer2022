{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import gvar as gv\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import jax.example_libraries.optimizers as jax_opt\n",
    "from tqdm import trange\n",
    "import cProfile\n",
    "import pickle\n",
    "\n",
    "num_particles = 10\n",
    "N = num_particles\n",
    "# The neural network structure (ignoring the number of inputs and the number of outputs)\n",
    "structure = [50,50,50,50,50,50]\n",
    "num_nodes = np.sum(structure)\n",
    "# mass, hbar, and our frequencies are all set to 1\n",
    "m = 1\n",
    "hbar = 1\n",
    "omega = 1\n",
    "harmonic_omega = 1\n",
    "# interaction strength of the delta function\n",
    "g = -.5\n",
    "# interaction strength of the long-range potential\n",
    "sigma = -g/2\n",
    "# C is the symmetrization constant, as we increase the number of particles, we increase this \n",
    "# (otherwise when we symmetrize the coordinates, x_i^N will become too large) \n",
    "C = 6\n",
    "\n",
    "FILENAME = \"2_boson_energies_-5.csv\"\n",
    "PARAMS_FILE = \"2_bosons_g-5.npy\"\n",
    "\n",
    "# when we start our chain of MCMC samples, this is the range of values that we draw the initial sample from\n",
    "INITIAL_SAMPLE = jnp.array(np.random.uniform(-1, 1, N))\n",
    "\n",
    "# class that defines a homebrew neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases for each layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        if hidden_sizes != [0]:\n",
    "            sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        else:\n",
    "            sizes = [input_size, output_size]\n",
    "\n",
    "        for i in range(len(sizes) - 1):\n",
    "            # generate the weights and biases from a random normal distribution\n",
    "            w = np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2/sizes[i])\n",
    "            b = np.random.randn(1, sizes[i+1]) \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    # this is the symmetrization function\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def transform(self, coords):\n",
    "        ret = jnp.zeros(num_particles)\n",
    "        for i in range(num_particles):\n",
    "            # we use the symmetrization constant C to prevent the sum from getting too large\n",
    "            ret = ret.at[i].set(jnp.sum(jnp.power(coords/C, i + 1)))\n",
    "        return ret \n",
    "\n",
    "    # querying the neural network, passing it coordinates and the flattened parameters (weights and biases)\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __call__(self, x, params):\n",
    "        # this is the symmetrization step\n",
    "        x = self.transform(x)\n",
    "        # unflatten the parameters and set them as the weights and biases\n",
    "        self.weights, self.biases = self.unflatten_params(params)\n",
    "        a = x\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = jnp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = celu(z)\n",
    "        a = jnp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a[0][0]\n",
    "    \n",
    "    # This function flattens the weights and biases into a single array (which is the set of variational parameters)\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def flatten_params(self):\n",
    "        params = jnp.array([])\n",
    "        for i in range(len(self.weights)):\n",
    "            params = jnp.concatenate((params, self.weights[i].flatten()))\n",
    "            params = jnp.concatenate((params, self.biases[i].flatten()))\n",
    "        return jnp.array(params)\n",
    "    # this takes the flattened parameters and unflattens them into weights and biases, which are used in the neural network\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def unflatten_params(self, params):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        start = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            end = start + self.weights[i].size\n",
    "            weights.append(jnp.reshape(jnp.array(params[start:end]), self.weights[i].shape))\n",
    "            start = end\n",
    "            end = start + self.biases[i].size\n",
    "            biases.append(jnp.reshape(jnp.array(params[start:end]), self.biases[i].shape))\n",
    "            start = end\n",
    "        return weights, biases\n",
    "    \n",
    "\n",
    "# initialize the network\n",
    "nn = NeuralNetwork(num_particles, structure, 1)\n",
    "\n",
    "#=========================== ANSATZ ===========================\n",
    "# function that we exponentiate to get the wavefunction\n",
    "# the other term is there to force the wavefunction to 0 at infinity, allows us to train the network\n",
    "@jit\n",
    "def A(coords, params):\n",
    "    return nn(coords, params) + omega * jnp.sum(coords**2)\n",
    "\n",
    "# define the wavefunction\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-A(coords, params)) \n",
    "\n",
    "#=========================== MC SAMPLING ===========================\n",
    "# does the accept/reject step of the Metropolis algorithm\n",
    "@jit\n",
    "def mcstep_E(xis, limit, positions, params):\n",
    "    \n",
    "    params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "    \n",
    "    newpositions = jnp.array(positions) + xis\n",
    "    \n",
    "    prob = psi(newpositions, params)**2./psi(positions, params)**2.\n",
    "    \n",
    "    def truefunc(p):\n",
    "        return [newpositions, True]\n",
    "\n",
    "    def falsefunc(p):\n",
    "        return [positions, False]\n",
    "    \n",
    "    return jax.lax.cond(prob >= limit, truefunc, falsefunc, prob)\n",
    "\n",
    "# gets a set of Monte Carlo samples from the wavefunction\n",
    "# params is the flattened set of weights and biases\n",
    "# Nsweeps is the number of samples that we want\n",
    "# Ntherm is the number of thermalization steps (we discard the first Ntherm samples)\n",
    "# keep is the number of steps we take before we record a sample (only keep every keep-th sample)\n",
    "# stepsize is the size of the step we take in the Metropolis-Hastings algorithm (chosen so that the rate of acceptance is around 50%)\n",
    "def sample(params, Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "    sq = []\n",
    "    sq_prime = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "    randoms = np.random.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = np.random.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in trange(0, num_total):\n",
    "            \n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "    # generate the primed samples by going through every sample and making sample[N_up] = sample[0]\n",
    "    sq_prime = sq.copy()\n",
    "    for i in range(len(sq)):\n",
    "        a = np.array(sq[i])\n",
    "        a[1] = a[0]\n",
    "        sq_prime[i] = jnp.array(a) \n",
    "\n",
    "    return jnp.array(sq), jnp.array(sq_prime), counter/num_total\n",
    "\n",
    "\n",
    "\n",
    "# first derivative of the neural network with respect to the coordinates\n",
    "# in Andy's notation this is dA/dx\n",
    "dA_dx = jit(grad(A, 0))\n",
    "\n",
    "# hessian is the matrix of second derivatives\n",
    "A_hessian = jax.jacfwd(dA_dx, 0) \n",
    "\n",
    "# get the second derivative from the hessian by taking the diagonal\n",
    "@jit\n",
    "def d2A_dx2(coords, params):\n",
    "    return jnp.diag(A_hessian(coords, params))\n",
    "\n",
    "\n",
    "# Now we compute all of the different needed parts of the energy and gradient\n",
    "@jit\n",
    "def Hpsi(coords, coords_prime, params, alpha):\n",
    "    return Hpsi_without_delta(coords, params) + delta_potential(coords,coords_prime, params, alpha)\n",
    "\n",
    "# sigma (long range) interaction\n",
    "@jit\n",
    "def sigma_term(coords):\n",
    "    N = num_particles \n",
    "    sigma_term = 0\n",
    "    for i in range(N):\n",
    "        for j in range(i,N):\n",
    "            sigma_term += sigma* jnp.abs(coords[i] - coords[j])  \n",
    "    return sigma_term\n",
    "\n",
    "\n",
    "@jit\n",
    "def Hpsi_without_delta(coords, params):\n",
    "    return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(dA_dx(coords, params)**2)) + m*.5*harmonic_omega**2* jnp.sum(coords**2) + sigma_term(coords)\n",
    "\n",
    "@jit\n",
    "def second_term(coords, params):\n",
    "    return dnn_dtheta(coords, params) * Hpsi_without_delta(coords, params)\n",
    "\n",
    "vsecond_term = jit(vmap(second_term, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def third_term(coords,coords_prime, params, y_max):\n",
    "    return dnn_dtheta(coords_prime, params) * delta_potential(coords, coords_prime, params, y_max)\n",
    "\n",
    "vthird_term = jit(vmap(third_term, in_axes=(0,0, None, None), out_axes=0))\n",
    "\n",
    "# delta function potential \n",
    "@jit\n",
    "def delta_potential(coords, coords_prime, params, alpha):\n",
    "    N = num_particles    \n",
    "    # compute e^(-2 NN(params_prime))\n",
    "    # ratio = jnp.exp(-2 * A(coords_prime, params) + 2 * A(coords, params))\n",
    "    ratio = (psi(coords_prime, params)**2)/(psi(coords, params)**2)\n",
    "    delta_dist = (1/(jnp.sqrt(jnp.pi) * alpha)) * jnp.exp(-(coords[1]**2)/(alpha**2))\n",
    "    return g * N*(N-1)/2 * ratio * delta_dist\n",
    "\n",
    "vdelta_potential = jit(vmap(delta_potential, in_axes=(0,0, None, None), out_axes=0))\n",
    "venergy = jit(vmap(Hpsi, in_axes=(0,0, None, None), out_axes=0))\n",
    "vHpsi_without_delta = jit(vmap(Hpsi_without_delta, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "\n",
    "# derivative of the neural network with respect to every parameter\n",
    "# dA/d theta\n",
    "dnn_dtheta = jit(grad(A, 1)) \n",
    "vdnn_dtheta = vmap(dnn_dtheta, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "#=========================== GRADIENT DESCENT ALGORITHM ===========================\n",
    "def gradient(params, num_samples=10**3, thermal=200, skip=50, variation_size=1.0, verbose=False):\n",
    "    # get the samples\n",
    "    samples, samples_prime, _  = sample(params, num_samples, thermal, skip, variation_size)\n",
    "\n",
    "    y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "    alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "\n",
    "    psiHpsi = venergy(samples, samples_prime, params, alpha) \n",
    "    # Hpsi_terms_without_delta = vHpsi_without_delta(samples, params)\n",
    "    # delta_term = vdelta_potential(samples,samples_prime, params, samples)\n",
    "\n",
    "    # delta function additions\n",
    "    dA_dtheta = vdnn_dtheta(samples, params)\n",
    "    # dA_dtheta_repeated = vdnn_dtheta(samples_prime, params)\n",
    "\n",
    "    dA_dtheta_avg = 1/num_samples * jnp.sum(dA_dtheta, 0)\n",
    "\n",
    "    second_term = 1/num_samples * jnp.sum(vsecond_term(samples, params), 0)\n",
    "    third_term = 1/num_samples * jnp.sum(vthird_term(samples, samples_prime, params, alpha), 0)\n",
    "    # third_term =1/num_samples * jnp.sum(vboth(dA_dtheta_repeated,delta_term), 0)\n",
    "    uncert = jnp.std(psiHpsi)/jnp.sqrt(num_samples)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(psiHpsi)\n",
    "\n",
    "   \n",
    "    if verbose:\n",
    "        print(energy)\n",
    "\n",
    "    gradient_calc = 2 * energy * dA_dtheta_avg - 2 * second_term - 2*third_term\n",
    "    return gradient_calc, energy, uncert\n",
    "\n",
    "\n",
    "# define a function that takes in samples, bins them, and returns the average of each bin\n",
    "def bin_samples(energies, bin_size):\n",
    "    # first, bin the samples\n",
    "    binned = np.array_split(energies, bin_size)\n",
    "    # now, calculate the average of each bin\n",
    "    binned_averages = [np.mean(b) for b in binned]\n",
    "    # now, calculate the uncertainty of each bin\n",
    "    bin_uncerts = np.std(binned_averages)/np.sqrt(bin_size)\n",
    "    return bin_uncerts\n",
    "\n",
    "\n",
    "# define a function that gets all samples, and then bins them with different bin sizes\n",
    "def autocorrelation(params):\n",
    "    samples = sample(params, num_samples=10**3, thermalization_steps=200, skip_count=40, variation_size=1)[0]\n",
    "    energies = [Hpsi(s, params) for s in samples]\n",
    "    \n",
    "    bins = np.linspace(1, 100, 100, dtype=int)\n",
    "    # now plot the average energy as a function of the number of bins\n",
    "    us = []\n",
    "    for b_size in bins:\n",
    "        us.append(bin_samples(energies, b_size))\n",
    "    plt.scatter(bins, us)\n",
    "    plt.title(\"Bin size vs. Uncertainty\")\n",
    "    plt.xlabel(\"Bin size\")\n",
    "    plt.ylabel(\"Uncertainty\")\n",
    "    plt.show()\n",
    "\n",
    "def step(params_arg, step_num, N, thermal, skip, variation_size):\n",
    "        gr = gradient(params_arg, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        # hs.append(gr[1])\n",
    "        # us.append(gr[2])\n",
    "        opt_state = opt_init(params_arg)\n",
    "        new = opt_update(step_num, gr[0], opt_state)\n",
    "        return get_params(new), gr[1], gr[2]\n",
    "\n",
    "def train(params, iterations, N, thermal, skip, variation_size):\n",
    "    hs = []\n",
    "    us = []\n",
    "    ns = np.arange(iterations)\n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:\n",
    "        new_params, energy, uncert = step(\n",
    "            old_params, step_num, N, thermal, skip, variation_size\n",
    "        )\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        # write the energy to the file\n",
    "        with open(FILENAME, mode=\"a\") as f:\n",
    "            f.write(str(energy) + \",\" + str(uncert) + \"\\n\")\n",
    "        with open(PARAMS_FILE, \"wb\") as f:\n",
    "            jnp.save(f, new_params)\n",
    "        old_params = new_params.copy()\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_step_size(params, start):\n",
    "    lr = .1\n",
    "    target = 0.5\n",
    "    tolerance = .05\n",
    "    max_it = 1000\n",
    "    step = start\n",
    "    best_step = start\n",
    "    best_acc = 0\n",
    "    it_num = 0\n",
    "    # get the samples \n",
    "    _, _, acc = sample(params, 1000, 100, 10, step)\n",
    "    # while the acceptance rate is not within +/- .5 of the target\n",
    "    while (acc < target - tolerance or acc > target + tolerance) and it_num < max_it:\n",
    "        it_num += 1\n",
    "        # if the acceptance rate is too low, increase the step size\n",
    "        if acc < target - tolerance:\n",
    "            step -= lr\n",
    "        # if the acceptance rate is too high, decrease the step size\n",
    "        elif acc > target + tolerance:\n",
    "            step += lr\n",
    "        # if we cross the target, decrease the learning rate and go back\n",
    "        if (acc < target and best_acc > target) or (acc > target and best_acc < target):\n",
    "            lr /= 2\n",
    "            step = best_step\n",
    "        # keep track of the best step size\n",
    "        if abs(acc - target) < abs(best_acc - target):\n",
    "            best_acc = acc\n",
    "            best_step = step\n",
    "        \n",
    "        # get the samples for the next step size\n",
    "        _, _, acc = sample(params, 1000, 100, 10, step)\n",
    "    print(\"step size:\",best_step)\n",
    "    return best_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters 13351\n",
      "Log of parameters 4.125513795904115\n"
     ]
    }
   ],
   "source": [
    "print(\"number of parameters\" , len(nn.flatten_params()))\n",
    "print(\"Log of parameters\", np.log10(len(nn.flatten_params())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100101/100101 [00:04<00:00, 23181.10it/s]\n"
     ]
    }
   ],
   "source": [
    "samples = sample(nn.flatten_params(), 10000, 100, 10, 1, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step size: 0.3500000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = -2.242813:  39%|███▉      | 39/100 [00:18<00:28,  2.16it/s]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m start_params \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mflatten_params()\n\u001b[1;32m      5\u001b[0m opt_init, opt_update, get_params \u001b[38;5;241m=\u001b[39m jax_opt\u001b[38;5;241m.\u001b[39madam(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m resultsa \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfind_step_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.85\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 0 -> energies\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1 -> uncert\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2 -> steps\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 3 -> params\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [8], line 342\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, iterations, N, thermal, skip, variation_size)\u001b[0m\n\u001b[1;32m    340\u001b[0m old_params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_num \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m--> 342\u001b[0m     new_params, energy, uncert \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mold_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthermal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariation_size\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(energy)\n\u001b[1;32m    346\u001b[0m     us\u001b[38;5;241m.\u001b[39mappend(uncert)\n",
      "Cell \u001b[0;32mIn [8], line 325\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(params_arg, step_num, N, thermal, skip, variation_size)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(params_arg, step_num, N, thermal, skip, variation_size):\n\u001b[0;32m--> 325\u001b[0m         gr \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthermal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariation_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;66;03m# print(gr)\u001b[39;00m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;66;03m# hs.append(gr[1])\u001b[39;00m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;66;03m# us.append(gr[2])\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         opt_state \u001b[38;5;241m=\u001b[39m opt_init(params_arg)\n",
      "Cell \u001b[0;32mIn [8], line 267\u001b[0m, in \u001b[0;36mgradient\u001b[0;34m(params, num_samples, thermal, skip, variation_size, verbose)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(params, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m, thermal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, variation_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# get the samples\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     samples, samples_prime, _  \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthermal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariation_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     y_max \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmax(jnp\u001b[38;5;241m.\u001b[39mabs(jnp\u001b[38;5;241m.\u001b[39marray(samples[:,\u001b[38;5;241m1\u001b[39m])))\n\u001b[1;32m    270\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m y_max\u001b[38;5;241m/\u001b[39m(jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39mjnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39msqrt(jnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m))))\n",
      "Cell \u001b[0;32mIn [8], line 191\u001b[0m, in \u001b[0;36msample\u001b[0;34m(params, Nsweeps, Ntherm, keep, stepsize, positions_initial, progress)\u001b[0m\n\u001b[1;32m    189\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sq[i])\n\u001b[1;32m    190\u001b[0m     a[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m a[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 191\u001b[0m     sq_prime[i] \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39marray(sq), jnp\u001b[38;5;241m.\u001b[39marray(sq_prime), counter\u001b[38;5;241m/\u001b[39mnum_total\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2046\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mobject\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__jax_array__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   2045\u001b[0m   \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m__jax_array__()\n\u001b[0;32m-> 2046\u001b[0m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__jax_array__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__jax_array__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2048\u001b[0m leaves \u001b[38;5;241m=\u001b[39m tree_leaves(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m   \u001b[38;5;66;03m# Use lattice_result_type rather than result_type to avoid canonicalization.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m   \u001b[38;5;66;03m# Otherwise, weakly-typed inputs would have their dtypes canonicalized.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/_src/tree_util.py:242\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_map\u001b[39m(f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[1;32m    207\u001b[0m              tree: Any,\n\u001b[1;32m    208\u001b[0m              \u001b[38;5;241m*\u001b[39mrest: Any,\n\u001b[1;32m    209\u001b[0m              is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    210\u001b[0m   \u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    [[5, 7, 9], [6, 1, 2]]\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m   leaves, treedef \u001b[38;5;241m=\u001b[39m \u001b[43mtree_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m   all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m    244\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(f(\u001b[38;5;241m*\u001b[39mxs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/_src/tree_util.py:83\u001b[0m, in \u001b[0;36mtree_flatten\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_flatten\u001b[39m(tree: Any,\n\u001b[1;32m     64\u001b[0m                  is_leaf: Callable[[Any], \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     65\u001b[0m                  ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Leaf], PyTreeDef]:\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;124;03m\"\"\"Flattens a pytree.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m  The flattening order (i.e. the order of elements in the output list)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    element is a treedef representing the structure of the flattened tree.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "open(FILENAME, 'w').close()\n",
    "\n",
    "start_params = nn.flatten_params()\n",
    "\n",
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-3))\n",
    "\n",
    "resultsa = train(start_params, 100, 500, 100, 10, find_step_size(start_params, .85))\n",
    "# 0 -> energies\n",
    "# 1 -> uncert\n",
    "# 2 -> steps\n",
    "# 3 -> params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "resultsb = train(resultsa[3], 1000, 10000, 500, 10, find_step_size(resultsa[3], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "resultsc = train(resultsb[3], 1000, 50000, 500, 3, find_step_size(resultsb[3], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-5))\n",
    "# resultsd = train(resultsc[3], 25, 80000, 500, 5, find_step_size(resultsc[3], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultse = train(resultsd[3], 25, 100000, 500, 2, find_step_size(resultsd[3], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = resultsc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "def harmonic_energy(n):\n",
    "    return .5 + 2*n - np.sqrt(2/np.pi) * (n+1/2)/g\n",
    "\n",
    "def astra_energy():\n",
    "    return (N * omega)/2 - m * g**2  * (N*(N**2 - 1))/(24)\n",
    "\n",
    "\n",
    "true_energy = .5 * hbar * omega * num_particles\n",
    "true_energy = harmonic_energy(0)\n",
    "true_energy = astra_energy()\n",
    "#g = 1, sigma = -g/2\n",
    "# true_energy = .75\n",
    "# g = .1, sigma = 0:\n",
    "# true_energy = 1.03881\n",
    "# g= .8, sigma= -g\n",
    "# true_energy = .9375\n",
    "# true_energy = 0.3098\n",
    "\n",
    "total_hists =  resultsa[0] + resultsb[0] + resultsc[0] \n",
    "#  + resultsd[0] + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1] + resultsb[1] + resultsc[1] \n",
    "# + resultsd[1] + resultse[1]\n",
    "# + resultsd[1]\n",
    "\n",
    "# get index of minimum value\n",
    "min_val = np.min(total_hists)\n",
    "min_val = total_hists[-1]\n",
    "min_index = total_hists.index(min_val)\n",
    "min_err = total_uncerts[min_index]\n",
    "val = gv.gvar(min_val, min_err)\n",
    "fractional_error = (val - true_energy)/true_energy\n",
    "print(\"Minimum value: \", val)\n",
    "print(\"Fractional error: \", fractional_error)\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists, label=\"Adam: \" + str(val))\n",
    "# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# get the network structure\n",
    "structure = nn.hidden_sizes\n",
    "plt.annotate(\" Network Structure: \" + str(structure), xy=(0.1, 0.95), xycoords='axes fraction')\n",
    "plt.plot(np.arange(0, len(total_hists)), [true_energy for x in np.arange(0, len(total_hists))], label=r\"Analytic: \" + str(round(true_energy,3)))\n",
    "pdiff = (min_val - true_energy)/true_energy*100\n",
    "# plt.annotate(\" Final Percent Diff = \" + str(round(pdiff,3)) + \"%\", xy=(.1, .9), xycoords= 'axes fraction')\n",
    "plt.legend()\n",
    "plt.title(r\"Modified LL model ($g = $\" + str(g) + r\", $\\sigma =\" + str(sigma) + \"$), \" + str(num_particles) + \" particles\")\n",
    "print(min_val)\n",
    "print(min_err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, samples_prime, _ = sample(params, 4*10**4, 100, 10, 1)\n",
    "y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "energies = venergy(samples,samples_prime, params, alpha)\n",
    "mean_energy = jnp.mean(energies)\n",
    "print(mean_energy)\n",
    "\n",
    "# bins = np.linspace(1, 100, 100, dtype=int)\n",
    "bins = np.array([1,2,5,10,20,50,100,150,200,250,300,360,450,500,550,600,660,750,900,990,1100])\n",
    "# now plot the average energy as a function of the number of bins\n",
    "us = []\n",
    "for b_size in bins:\n",
    "    us.append(bin_samples(energies, b_size))\n",
    "plt.scatter(bins, us)\n",
    "plt.title(\"Bin size vs. Uncertainty\")\n",
    "plt.xlabel(\"Bin size\")\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "print(max(us))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = gv.gvar(mean_energy, max(us))\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, .05)\n",
    "ys = np.arange(-5, 5, .05)\n",
    "wavs = []\n",
    "for i in range(len(xs)):\n",
    "    for j in range(len(ys)):\n",
    "        wavs.append(psi(np.array([xs[i], ys[j]]), params)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(xs,ys)\n",
    "Z = np.array(wavs).reshape(len(xs), len(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.contourf(X, Y, Z, 100)\n",
    "\n",
    "plt.plot(xs,-xs)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the exact wavefunction for the case where sigma = -g/2\n",
    "@jit\n",
    "def astra_wf(coords):\n",
    "    ret = 1\n",
    "    a_s = -2/(m*g) \n",
    "    a_ho = jnp.sqrt(1/(m * harmonic_omega))\n",
    "    for i in range(N):\n",
    "        for j in range(0,i):\n",
    "            ret *= jnp.exp(-jnp.abs(coords[i] - coords[j])/a_s)\n",
    "        ret *= jnp.exp(-coords[i]**2/(2*a_ho**2))\n",
    "    return ret\n",
    "\n",
    "# sampling from the exact wavefunction\n",
    "\n",
    "@jit\n",
    "def mcstep_E_exact(xis, limit, positions):\n",
    "    newpositions = jnp.array(positions) + xis\n",
    "    \n",
    "    prob = astra_wf(newpositions)**2 / astra_wf(positions)**2\n",
    "    \n",
    "    def truefunc(p):\n",
    "        return [newpositions, True]\n",
    "\n",
    "    def falsefunc(p):\n",
    "        return [positions, False]\n",
    "    \n",
    "    return jax.lax.cond(prob >= limit, truefunc, falsefunc, prob)\n",
    "\n",
    "def sample_exact(Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "    sq = []\n",
    "    sq_prime = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "\n",
    "    randoms = np.random.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = np.random.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E_exact(randoms[i], limits[i], positions_prev)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E_exact(randoms[i], limits[i], positions_prev)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "    # generate the primed samples by going through every sample and making sample[N_up] = sample[0]\n",
    "    sq_prime = sq.copy()\n",
    "    for i in range(len(sq)):\n",
    "        a = np.array(sq[i])\n",
    "        a[1] = a[0]\n",
    "        sq_prime[i] = jnp.array(a) \n",
    "\n",
    "    return jnp.array(sq), jnp.array(sq_prime), counter/num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(params, 3 * 10**4, 100, 10, .85)\n",
    "samples_exact = sample_exact(3 * 10**4, 100, 10, .85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_density(samples):\n",
    "    x_bins = np.linspace(-9, 9, 72)\n",
    "    dx = x_bins[1] - x_bins[0]  # Bin width\n",
    "    n_x = np.zeros_like(x_bins)\n",
    "\n",
    "    # bin the x_1s\n",
    "    for x in samples[0][:,0]:\n",
    "        n_x[np.digitize(x, x_bins)] += 1\n",
    "\n",
    "    # Normalize\n",
    "    n_x /= (dx * np.sum(n_x))\n",
    "\n",
    "    return x_bins, n_x\n",
    "\n",
    "x_bins, n_x = local_density(samples)\n",
    "x_bins_exact, n_x_exact = local_density(samples_exact)\n",
    "plt.plot(x_bins, n_x,'-o', markersize=2, color=\"red\")\n",
    "plt.plot(x_bins_exact, n_x_exact,'-o', markersize=2, color=\"black\")\n",
    "plt.title(r\"$N = 2$ Local Density Profile\")\n",
    "plt.xlabel(\"$x$\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.ylabel(\"$n(x)/\\int dx n(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
