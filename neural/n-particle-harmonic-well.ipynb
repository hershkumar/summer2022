{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import optax\n",
    "import multiprocess\n",
    "\n",
    "\n",
    "\n",
    "num_particles = 2\n",
    "hbar = 1\n",
    "m = 1\n",
    "omegas = jnp.array([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the neural network class\n",
    "\n",
    "class Network:\n",
    "    #[1, 5, 10, 10, 10, 10, 5, 1]\n",
    "    def __init__(self, node_counts=[num_particles, 50,50,50, 1]):\n",
    "        # defining the structure of the neural network\n",
    "        self.num_layers = len(node_counts)\n",
    "        # the number of nodes for each layer\n",
    "        self.node_counts = node_counts\n",
    "        # the total number of weights\n",
    "        self.params_length = 0\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.params_length += node_counts[i] * node_counts[i + 1]\n",
    "            i+=1\n",
    "        # the list that stores the weight matrices (index 0 is the connections from the input to the first hidden layer)\n",
    "        self.weights = []\n",
    "        # storage for all the biases\n",
    "        self.biases = []\n",
    "        \n",
    "        # generate weight matrices with the correct sizes, and random elements\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights.append(np.random.randn(node_counts[i + 1], node_counts[i]) * np.sqrt(1. / (node_counts[i + 1])))\n",
    "        self.weights = np.array(self.weights, dtype=object)\n",
    "        \n",
    "        # generate the bias arrays\n",
    "        for i in range(self.num_layers - 1):\n",
    "            temp = np.random.randn(node_counts[i + 1]) * np.sqrt(1. / node_counts[i + 1])\n",
    "            self.biases.append(temp[:, None])\n",
    "        self.biases = np.array(self.biases, dtype=object)\n",
    "        # get the shape for reshaping a 1d array to this later\n",
    "        self.dimensions = []\n",
    "        for m in self.weights:\n",
    "            self.dimensions.append(m.shape)\n",
    "\n",
    "    # define the activation function that we use for the layers\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def l_act(self, x):\n",
    "        return celu(x)\n",
    "        #return 1.0 / (1.0 + jnp.exp(-x))    \n",
    "    # define the activation function for the output\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def o_act(self, x):\n",
    "        return celu(x)\n",
    "        #return x\n",
    "        #return 1.0 / (1.0 + jnp.exp(-x))\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def conv1d(self):\n",
    "        flattened = []\n",
    "        # loop through each matrix\n",
    "        for m in range(self.num_layers - 1):\n",
    "            mat = self.weights[m]\n",
    "            for i in range(mat.shape[0]):\n",
    "                for j in range(mat.shape[1]):\n",
    "                    flattened.append(mat[i][j])\n",
    "        for m in range(self.num_layers - 1):\n",
    "            # add the biases onto the end\n",
    "            bias = self.biases[m]\n",
    "            for element in bias:\n",
    "                flattened.append(element)\n",
    "        return jnp.array(flattened)\n",
    "        \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def convnd(self, params):\n",
    "        ret = []\n",
    "        ret_bias = []\n",
    "        # the last index of the matrix\n",
    "        max_ind = 0\n",
    "        prev_ind = 0\n",
    "        for dim in self.dimensions:\n",
    "            max_ind += dim[0]*dim[1]\n",
    "            temp = params[prev_ind : max_ind]\n",
    "            prev_ind = max_ind\n",
    "            ret.append(jnp.array(temp).reshape(dim))\n",
    "        biases_flattened = params[max_ind :]\n",
    "        # now reconstruct the bias matrices\n",
    "        # print(biases_flattened)\n",
    "        ind = 0\n",
    "        for i in range(self.num_layers - 1):\n",
    "            ret_bias.append((biases_flattened[ind: ind + self.node_counts[i + 1]])[:, None])\n",
    "            ind += self.node_counts[i + 1]\n",
    "        return ret, ret_bias\n",
    "\n",
    "\n",
    "    # passing inputs into the neural network and getting an output\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def output(self, coords, params):\n",
    "        # format the parameters as weights\n",
    "        c = self.convnd(params)\n",
    "        self.weights = c[0]\n",
    "        self.biases = c[1]\n",
    "        # compute the output of the neural network\n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = jnp.array(self.weights[i])\n",
    "            b = jnp.array(self.biases[i])\n",
    "            # print(w)\n",
    "            # print(b)\n",
    "            # if its the first layer, dot it against the input and use the activation function\n",
    "            if i == 0:\n",
    "                temp = self.l_act(jnp.dot(w, coords))\n",
    "            elif (i < self.num_layers):\n",
    "                \n",
    "                temp = self.l_act(jnp.dot(w, temp) + b)\n",
    "            else:\n",
    "                # on the output layer we use the output activation function\n",
    "                temp = self.o_act(jnp.dot(w, temp) + b)\n",
    "        # print(\"output:\" + str(time.time() -  start))\n",
    "        return temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network object\n",
    "nn = Network()\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-nn.output(coords, params)[0]) * np.prod(jnp.exp(-coords**2))\n",
    "\n",
    "@jit\n",
    "def sample_body(coords_t, params, key):    \n",
    "    gen_rand = jax.random.uniform(key, minval=-5, maxval=5, shape=(num_particles,))\n",
    "    new_key, subkey = jax.random.split(key)\n",
    "    del key\n",
    "    \n",
    "    coords_prime = coords_t + gen_rand\n",
    "    temp_rand = jax.random.uniform(subkey, minval=0, maxval=1)\n",
    "    return (jax.lax.cond(temp_rand < psi(coords_prime, params)**2/psi(coords_t, params)**2, lambda x, y: x, lambda x, y : y, coords_prime, coords_t), new_key)\n",
    "\n",
    "def sample(params, num_samples=10**3, thermalization_steps=200, skip_count=10, key=jax.random.PRNGKey(np.random.randint(0,100))):\n",
    "    outputs = []\n",
    "    # e_plots = []\n",
    "    coords_t = np.random.uniform(-5, 5, num_particles)\n",
    "    for step in range(num_samples*skip_count + thermalization_steps + 1):\n",
    "        coords_t, key = sample_body(coords_t, params, key)\n",
    "        if ((step > thermalization_steps) & (step % skip_count == 0)):\n",
    "            outputs.append(coords_t)\n",
    "    return jnp.array(outputs)\n",
    "\n",
    "h = hessian(psi)\n",
    "\n",
    "@jit\n",
    "def ddpsi(coords, params):\n",
    "    return jnp.diagonal(h(coords, params))\n",
    "\n",
    "\n",
    "#TODO: figure out what the issue here is\n",
    "@jit\n",
    "def Hpsi(coords, params):\n",
    "    return  jnp.sum(m*.5*jnp.multiply(omegas**2,coords**2)) - hbar**2 / (2*m) * jnp.sum(ddpsi(coords, params)) * 1/psi(coords, params)\n",
    "\n",
    "venergy = vmap(Hpsi, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "@jit\n",
    "def logpsi(coords, params):\n",
    "    return jnp.log(psi(coords, params))\n",
    "\n",
    "# define the derivative with respect to every parameter of the log of psi:\n",
    "dlogpsi_dtheta_stored = jit(grad(logpsi, 1))\n",
    "\n",
    "vlog_term = jit(vmap(dlogpsi_dtheta_stored, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "def gradient(params, num_samples=10**3, verbose=True):\n",
    "    # get the samples\n",
    "    samples = sample(params, num_samples)\n",
    "    psiHpsi = venergy(samples, params)\n",
    "    logs = vlog_term(samples, params)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(psiHpsi)\n",
    "    if verbose:\n",
    "        print(energy)\n",
    "    log_term = 1/num_samples * jnp.sum(logs,0)\n",
    "\n",
    "    both = 1/num_samples * jnp.sum(vboth(psiHpsi, logs),0)\n",
    "\n",
    "    gradient_calc = (2 * both - 2*energy * log_term)\n",
    "    return gradient_calc, energy\n",
    "\n",
    "def avg_energy(params, num_samples = 10**3):\n",
    "    samples = sample(params, num_samples)\n",
    "    psiHpsi = venergy(samples, params)\n",
    "    return 1/num_samples * jnp.sum(psiHpsi)\n",
    "\n",
    "\n",
    "weight1 = []\n",
    "last_bias = []\n",
    "energies = []\n",
    "def vgrad_opt(start_params, num_samples=10**3, learning_rate=.1, max_iterations=10000, tolerance=.000001, verbose=False):\n",
    "    params = start_params\n",
    "    hist = [start_params]\n",
    "    start = time.time()\n",
    "    for it in range(max_iterations):\n",
    "        clear_output(wait=True)\n",
    "        if verbose:\n",
    "            print(\"iteration \" + str(it))\n",
    "        gr = gradient(params, num_samples, verbose=verbose)\n",
    "        energies.append(gr[1])\n",
    "        diff = jnp.asarray((learning_rate * gr[0]))\n",
    "        #print(diff)\n",
    "        # make a step in the direction opposite the gradient\n",
    "        params = params - diff\n",
    "        #weight1.append(params[0])\n",
    "        #last_bias.append(params[-1])\n",
    "        \n",
    "        # print(params)\n",
    "        hist.append(params)\n",
    "        elapsed = time.time() - start\n",
    "        if verbose:\n",
    "            print(\"Iterations per second: \" + str(elapsed/max_iterations))\n",
    "    return hist\n",
    "\n",
    "# adam optimizer seems to be solidly faster than my own one.\n",
    "def adam_opt(params, optimizer, num_samples=10**3, learning_rate=.1, max_iterations=10000, tolerance=.00001, verbose=False):\n",
    "    hist = []\n",
    "    opt_state = optimizer.init(params)\n",
    "    start = time.time()\n",
    "    def step(params, opt_state):\n",
    "        clear_output(wait=True)\n",
    "        gr = gradient(params, num_samples, verbose=verbose)\n",
    "        hist.append(gr[1])\n",
    "        updates, opt_state = optimizer.update(gr[0], opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, gr[1]\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        params, opt_state, _ = step(params, opt_state)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    if verbose:\n",
    "        print(\"Iterations per second: \" + str(elapsed/max_iterations))\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = sample(nn.conv1d())\n",
    "sx = [s[0] for s in samples]\n",
    "sy = [s[1] for s in samples]\n",
    "plt.hist(sx)\n",
    "plt.hist(sy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the homemade gradient descent\n",
    "optd = vgrad_opt(nn.conv1d())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_optd = adam_opt(nn.conv1d(), optax.adam(learning_rate=.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(r\"Two Particles in Harmonic Potential ($\\omega_1 = 1, \\omega_2 = 10$)\")\n",
    "plt.xlabel(\"Iteration of Gradient Descent\")\n",
    "plt.ylabel(\"Average Energy\")\n",
    "plt.plot(np.arange(0,len(optd[1:])), energies, label=\"Naive Gradient Descent\")\n",
    "\n",
    "# get the network structure\n",
    "structure = nn.node_counts\n",
    "plt.annotate(\" Network Structure: \" + str(structure), xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "\n",
    "\n",
    "\n",
    "# now plot the adam optimizer run\n",
    "plt.plot(np.arange(0, len(a_optd)), a_optd, label=\"Adam\")\n",
    "# plot the line for the true ground state energy\n",
    "true_energy = np.sum(hbar/2 * omegas)\n",
    "plt.plot(np.arange(0, len(optd[1:])), [true_energy for x in np.arange(0, len(optd[1:]))], label=r\"True Energy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
