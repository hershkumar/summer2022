{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import optax\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "num_particles = 2\n",
    "num_nodes = 50\n",
    "m = 1\n",
    "hbar = 1\n",
    "omega = 1\n",
    "\n",
    "g = 0.2\n",
    "sigma = -g/2.0\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases for each layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        if hidden_sizes != [0]:\n",
    "            sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        else:\n",
    "            sizes = [input_size, output_size]\n",
    "\n",
    "        for i in range(len(sizes) - 1):\n",
    "            w = np.random.randn(sizes[i], sizes[i+1]) \n",
    "            b = np.random.randn(1, sizes[i+1]) \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def transform(self, coords):\n",
    "       # if running into NaNs, try to increase this\n",
    "        C = 2\n",
    "        ret = jnp.zeros(num_particles)\n",
    "        for i in range(num_particles):\n",
    "            ret = ret.at[i].set(jnp.sum(jnp.power(coords/C, i + 1)))\n",
    "        return ret \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __call__(self, x, params):\n",
    "        x = self.transform(x)\n",
    "        self.weights, self.biases = self.unflatten_params(params)\n",
    "        a = x\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = jnp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = celu(z)\n",
    "        a = jnp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a[0][0]\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def flatten_params(self):\n",
    "        params = jnp.array([])\n",
    "        for i in range(len(self.weights)):\n",
    "            params = jnp.concatenate((params, self.weights[i].flatten()))\n",
    "            params = jnp.concatenate((params, self.biases[i].flatten()))\n",
    "        return jnp.array(params)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def unflatten_params(self, params):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        start = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            end = start + self.weights[i].size\n",
    "            weights.append(jnp.reshape(jnp.array(params[start:end]), self.weights[i].shape))\n",
    "            start = end\n",
    "            end = start + self.biases[i].size\n",
    "            biases.append(jnp.reshape(jnp.array(params[start:end]), self.biases[i].shape))\n",
    "            start = end\n",
    "        return weights, biases\n",
    "    \n",
    "\n",
    "# initialize the network\n",
    "nn = NeuralNetwork(num_particles, [num_nodes], 1)\n",
    "\n",
    "\n",
    "# symmetrization transformation\n",
    "# I1 = x_1/C + x_2/C + ... + x_N/C\n",
    "# I2 = (x_1/C)^2 + (x_2/C)^2 + ... + (x_N/C)^2\n",
    "# ...\n",
    "# IN = (x_1/C)^N + (x_2/C)^N + ... + (x_N/C)^N\n",
    "\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "     return jnp.exp(-nn(coords, params)) * jnp.exp(jnp.sum(-coords**2))\n",
    "\n",
    "@jit\n",
    "def sample_body(coords_t, params, key, variation_size):    \n",
    "    gen_rand = jax.random.uniform(key,shape=(num_particles,), minval=-variation_size, maxval=variation_size)\n",
    "    new_key, subkey = jax.random.split(key)\n",
    "    coords_prime = coords_t + gen_rand\n",
    "    temp_rand = jax.random.uniform(subkey, minval=0, maxval=1)\n",
    "    return (jax.lax.cond(temp_rand < psi(coords_prime, params)**2/psi(coords_t, params)**2, lambda x, _: x, lambda _, y : y, coords_prime, coords_t), new_key)\n",
    "\n",
    "def sample(params, num_samples=10**3, thermalization_steps=200, skip_count=50, variation_size=1.0, key=jax.random.PRNGKey(np.random.randint(0,10000)), skipping = True):\n",
    "    outputs = []\n",
    "    all_data = []\n",
    "\n",
    "    #coords_t = np.random.uniform(-variation_size, variation_size, num_particles)\n",
    "    coords_t = np.zeros(num_particles)\n",
    "    for step in range(num_samples*skip_count + thermalization_steps + 1):\n",
    "        coords_t, key = sample_body(coords_t, params, key, variation_size)\n",
    "        if ((step > thermalization_steps) & (step % skip_count == 0) & (skipping == True)):\n",
    "            outputs.append(coords_t)\n",
    "        elif skipping == False:\n",
    "            all_data.append(coords_t)\n",
    "    if skipping == False:\n",
    "        return all_data\n",
    "\n",
    "    # create a second output array, where the second coordinate is equal to the first coordinate\n",
    "    outputs_prime = outputs\n",
    "    for i in range(len(outputs)):\n",
    "        a = outputs[i]\n",
    "        a.at[1].set(a[0])\n",
    "        outputs_prime[i] = a\n",
    "    return jnp.array(outputs), jnp.array(outputs_prime)\n",
    "\n",
    "\n",
    "# sample_body function except it also returns whether or not the move was accepted\n",
    "@jit\n",
    "def sample_body_accept(coords_t, params, key,variation_size):\n",
    "    gen_rand = jax.random.uniform(key, minval=-variation_size, maxval=variation_size)\n",
    "    new_key, subkey = jax.random.split(key)\n",
    "    \n",
    "    coords_prime = coords_t + gen_rand\n",
    "    condition = jax.random.uniform(subkey, minval=0, maxval=1) < psi(coords_prime, params)**2/psi(coords_t, params)**2\n",
    "    return (jax.lax.cond(condition, lambda x, _: x, lambda _, y : y, coords_prime, coords_t), new_key, condition)\n",
    "\n",
    "\n",
    "# the sample function without any thermalization steps or skipping steps\n",
    "def accept_ratio(params, num_samples=10**3, variation_size=5.0, key=jax.random.PRNGKey(np.random.randint(0,100))):\n",
    "    coords_t = np.random.uniform(-variation_size, variation_size)\n",
    "    num_accepted = 0\n",
    "    for _ in range(num_samples):\n",
    "        coords_t, key, accepted = sample_body_accept(coords_t, params, key, variation_size)\n",
    "        if accepted:\n",
    "            num_accepted += 1\n",
    "\n",
    "    return num_accepted / num_samples\n",
    "\n",
    "\n",
    "\n",
    "psi_hessian = hessian(psi)\n",
    "ddpsi_single = jit(grad(jit(grad(psi, 0, allow_int = True)), 0, allow_int = True))\n",
    "\n",
    "@jit\n",
    "def ddpsi(coords, params):\n",
    "    if num_particles != 1:\n",
    "        return jnp.diagonal(psi_hessian(coords, params))\n",
    "    else:\n",
    "        return ddpsi_single(coords, params)\n",
    "\n",
    "# first derivative of the neural network with respect to the coordinates\n",
    "# in Andy's notation this is dA/dx\n",
    "dA_dx = jit(grad(nn, 0, allow_int=True)) # type: ignore\n",
    "\n",
    "# second derivative of the neural network with respect to the coordinates\n",
    "# in Andy's notation this is d^2A/dx^2\n",
    "A_hessian = hessian(nn) # type: ignore\n",
    "\n",
    "@jit\n",
    "def d2A_dx2(coords, params):\n",
    "    #return jnp.diagonal(A_hessian(transform(coords), params))\n",
    "    return jnp.diagonal(A_hessian(coords, params))\n",
    "\n",
    "\n",
    "@jit\n",
    "def Hpsi(coords, params, samples):\n",
    "    return Hpsi_without_delta(coords, params) + delta_potential(coords, params, samples)\n",
    "\n",
    "@jit\n",
    "def Hpsi_without_delta(coords, params):\n",
    "   # sigma term\n",
    "    N = len(coords)\n",
    "    sigma_term = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if (i<j):\n",
    "                sigma_term += sigma * jnp.abs(coords[i] - coords[j])\n",
    "    return jnp.sum((m*.5*omega**2*coords**2)) - hbar**2 / (2*m) * jnp.sum(ddpsi(coords, params) ) * 1/psi(coords, params) + sigma_term \n",
    "    #return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(jnp.square(dA_dx(coords, params)))) + jnp.sum((m*.5*omega**2*coords**2)) + sigma_term\n",
    "\n",
    "@jit\n",
    "def delta_potential(coords, params, samples):\n",
    "    N = len(coords)\n",
    "    # make a copy of the parameters and set the second one to be equal to the first one\n",
    "    coords_prime = coords.at[1].set(coords[0])\n",
    "    # compute e^(-2 NN(params_prime))\n",
    "    ratio = jnp.exp(-2 * nn(coords_prime, params) + 2 * nn(coords, params))\n",
    "    # compute y_max, largest value of the second coordinate in the samples\n",
    "    # first get all the second coordinates, and find the absolute max of them\n",
    "    y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "    # compute alpha\n",
    "    alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-5))))\n",
    "    delta_dist = 1/(jnp.sqrt(2*jnp.pi) * alpha) * jnp.exp(-coords[1]**2/(alpha**2))\n",
    "    return g * N*(N-1)/2 * ratio * delta_dist\n",
    "\n",
    "vdelta_potential = jit(vmap(delta_potential, in_axes=(0, None, None), out_axes=0))\n",
    "venergy = jit(vmap(Hpsi, in_axes=(0, None, None), out_axes=0))\n",
    "vHpsi_without_delta = jit(vmap(Hpsi_without_delta, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "\n",
    "# derivative of the neural network with respect to every parameter\n",
    "# in Andy's notation this is dA/dtheta\n",
    "dnn_dtheta = jit(grad(nn, 1)) # type: ignore\n",
    "vdnn_dtheta = vmap(dnn_dtheta, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "def gradient(params, num_samples=10**3, thermal=200, skip=50, variation_size=1.0, verbose=False):\n",
    "    # get the samples\n",
    "    samples, samples_prime = sample(params, num_samples, thermal, skip, variation_size)\n",
    "\n",
    "    psiHpsi = venergy(samples, params, samples)\n",
    "    Hpsi_terms_without_delta = vHpsi_without_delta(samples, params)\n",
    "    delta_term = vdelta_potential(samples, params, samples)\n",
    "\n",
    "    # delta function additions\n",
    "    dA_dtheta = vdnn_dtheta(samples, params)\n",
    "    dA_dtheta_repeated = vdnn_dtheta(samples_prime, params)\n",
    "\n",
    "    dA_dtheta_avg = 1/num_samples * jnp.sum(dA_dtheta, 0)\n",
    "\n",
    "    second_term = 1/num_samples * jnp.sum(vboth(dA_dtheta, Hpsi_terms_without_delta), 0) \n",
    "\n",
    "    third_term =1/num_samples * jnp.sum(vboth(dA_dtheta_repeated,delta_term), 0)\n",
    "    uncert = jnp.std(psiHpsi)/jnp.sqrt(num_samples)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(psiHpsi, 0)\n",
    "    if verbose:\n",
    "        print(energy)\n",
    "\n",
    "    N = len(samples[0])\n",
    "    gradient_calc = 2 * energy * dA_dtheta_avg - 2 * second_term - g * N*(N-1) * third_term\n",
    "    return gradient_calc, energy, uncert\n",
    "\n",
    "# define a function that takes in samples, bins them, and returns the average of each bin\n",
    "def bin_samples(energies, bin_size):\n",
    "    # first, bin the samples\n",
    "    binned = np.array_split(energies, bin_size)\n",
    "    # now, calculate the average of each bin\n",
    "    binned_averages = [np.mean(b) for b in binned]\n",
    "    # now, calculate the uncertainty of each bin\n",
    "    bin_uncerts = np.std(binned_averages)/np.sqrt(bin_size)\n",
    "    return bin_uncerts\n",
    "\n",
    "\n",
    "# define a function that gets all samples, and then bins them with different bin sizes\n",
    "def autocorrelation(params):\n",
    "    samples = sample(params, num_samples=10**3, thermalization_steps=200, skip_count=40, variation_size=1, key=jax.random.PRNGKey(np.random.randint(0,100)), skipping = False)[0]\n",
    "    energies = [Hpsi(s, params) for s in samples]\n",
    "    \n",
    "    bins = np.linspace(1, 100, 100, dtype=int)\n",
    "    # now plot the average energy as a function of the number of bins\n",
    "    us = []\n",
    "    for b_size in bins:\n",
    "        us.append(bin_samples(energies, b_size))\n",
    "    plt.scatter(bins, us)\n",
    "    plt.title(\"Bin size vs. Uncertainty\")\n",
    "    plt.xlabel(\"Bin size\")\n",
    "    plt.ylabel(\"Uncertainty\")\n",
    "    plt.show()\n",
    "\n",
    "def train(params, iterations, N, thermal, skip, variation_size, optimizer):\n",
    "    hs = []\n",
    "    us = []\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    def step(params, opt_state, N, thermal, skip, variation_size):\n",
    "        gr = gradient(params, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        hs.append(gr[1])\n",
    "        us.append(gr[2])\n",
    "        updates, opt_state = optimizer.update(gr[0], opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, gr[1]\n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "    for step_num in pbar:   \n",
    "        params, opt_state, energy = step(params, opt_state, N, thermal, skip, variation_size)\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept/Reject ratio 0.51\n"
     ]
    }
   ],
   "source": [
    "variation = 1.5\n",
    "print(\"Accept/Reject ratio\",accept_ratio(nn.flatten_params(), num_samples=10**3, variation_size=variation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = 2.8595645e+16:  74%|███████▍  | 37/50 [00:40<00:14,  1.09s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb Cell 3\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m total_uncerts \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m start_params \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mflatten_params()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m hists, uncerts, params \u001b[39m=\u001b[39m train(start_params, \u001b[39m50\u001b[39;49m, \u001b[39m10\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m, \u001b[39m200\u001b[39;49m, \u001b[39m40\u001b[39;49m, variation, optax\u001b[39m.\u001b[39;49madam(\u001b[39m0.01\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m total_hists \u001b[39m=\u001b[39m total_hists \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(hists)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m total_uncerts \u001b[39m=\u001b[39m total_uncerts \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(uncerts) \n",
      "\u001b[1;32m/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb Cell 3\u001b[0m in \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=291'>292</a>\u001b[0m pbar \u001b[39m=\u001b[39m trange(iterations, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=292'>293</a>\u001b[0m \u001b[39mfor\u001b[39;00m step_num \u001b[39min\u001b[39;00m pbar:   \n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=293'>294</a>\u001b[0m     params, opt_state, energy \u001b[39m=\u001b[39m step(params, opt_state, N, thermal, skip, variation_size)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=294'>295</a>\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39m\"\u001b[39m\u001b[39mEnergy = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(energy), refresh\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=295'>296</a>\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misnan(energy):\n",
      "\u001b[1;32m/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb Cell 3\u001b[0m in \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=282'>283</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(params, opt_state, N, thermal, skip, variation_size):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=283'>284</a>\u001b[0m     gr \u001b[39m=\u001b[39m gradient(params, N, thermal, skip, variation_size)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=284'>285</a>\u001b[0m     \u001b[39m# print(gr)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=285'>286</a>\u001b[0m     hs\u001b[39m.\u001b[39mappend(gr[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32m/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb Cell 3\u001b[0m in \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(params, num_samples\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m, thermal\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, skip\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, variation_size\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m     \u001b[39m# get the samples\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=225'>226</a>\u001b[0m     samples, samples_prime \u001b[39m=\u001b[39m sample(params, num_samples, thermal, skip, variation_size)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=227'>228</a>\u001b[0m     psiHpsi \u001b[39m=\u001b[39m venergy(samples, params, samples)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m     Hpsi_terms_without_delta \u001b[39m=\u001b[39m vHpsi_without_delta(samples, params)\n",
      "\u001b[1;32m/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m coords_t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(num_particles)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_samples\u001b[39m*\u001b[39mskip_count \u001b[39m+\u001b[39m thermalization_steps \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     coords_t, key \u001b[39m=\u001b[39m sample_body(coords_t, params, key, variation_size)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     \u001b[39mif\u001b[39;00m ((step \u001b[39m>\u001b[39m thermalization_steps) \u001b[39m&\u001b[39m (step \u001b[39m%\u001b[39m skip_count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (skipping \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m)):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W2sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m         outputs\u001b[39m.\u001b[39mappend(coords_t)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_hists = []\n",
    "total_uncerts = []\n",
    "start_params = nn.flatten_params()\n",
    "\n",
    "hists, uncerts, params = train(start_params, 50, 10**3, 200, 40, variation, optax.adam(0.01))\n",
    "total_hists = total_hists + list(hists)\n",
    "total_uncerts = total_uncerts + list(uncerts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = -15.492145: 100%|██████████| 50/50 [01:00<00:00,  1.20s/it] \n"
     ]
    }
   ],
   "source": [
    "hists_b, uncerts_b, params_b = train(params, 50, 10**3, 200, 40, variation, optax.adam(0.01))\n",
    "total_hists = total_hists + list(hists_b)\n",
    "total_uncerts = total_uncerts + list(uncerts_b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = nan:   2%|▏         | 1/50 [00:02<02:03,  2.52s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN encountered, stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hists_c, uncerts_c, params_c = train(params_b, 50, 10**3, 200, 40, variation, optax.adam(0.01))\n",
    "total_hists = total_hists + list(hists_c)\n",
    "total_uncerts = total_uncerts + list(uncerts_c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_d, uncerts_d, params_d = train(params_c, 50, 10**3, 200, 40, variation, optax.adam(0.01))\n",
    "total_hists = total_hists + list(hists_d)\n",
    "total_uncerts = total_uncerts + list(uncerts_d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_e, uncerts_e, params_e = train(params_d, 50, 10**3, 200, 40, variation, optax.adam(0.01))\n",
    "total_hists = total_hists + list(hists_e)\n",
    "total_uncerts = total_uncerts + list(uncerts_e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb Cell 8\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m true_energy \u001b[39m=\u001b[39m \u001b[39m.5\u001b[39m \u001b[39m*\u001b[39m hbar \u001b[39m*\u001b[39m omega \u001b[39m*\u001b[39m num_particles\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(total_hists)), total_hists, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAdam, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(total_hists[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\u001b[39m3\u001b[39m)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m +/- \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(total_uncerts[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m2\u001b[39m)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# plot the uncertainties\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hersh/Documents/School/summer2022/neural/nn_delta.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m a_hists \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(total_hists)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "true_energy = .5 * hbar * omega * num_particles\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists, label=\"Adam, \" + str(round(total_hists[-1],3)) + \" +/- \" + str(round(total_uncerts[-1], 2)))\n",
    "# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# get the network structure\n",
    "structure = nn.hidden_sizes\n",
    "plt.annotate(\" Network Structure: \" + str(structure), xy=(0.1, 0.95), xycoords='axes fraction')\n",
    "plt.plot(np.arange(0, len(total_hists)), [true_energy for x in np.arange(0, len(total_hists))], label=r\"True Energy, \" + str(round(true_energy,3)))\n",
    "pdiff = (total_hists[-1] - true_energy)/true_energy*100\n",
    "plt.annotate(\" Final Percent Diff = \" + str(round(pdiff,3)) + \"%\", xy=(.1, .9), xycoords= 'axes fraction')\n",
    "plt.legend()\n",
    "plt.title(\"Average Energy vs. Iteration, \" + str(num_particles) + \" particles\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
