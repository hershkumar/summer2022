{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import time\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import optax\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "num_particles = 2\n",
    "num_nodes = 15\n",
    "m = 1\n",
    "hbar = 1\n",
    "omega = 1\n",
    "\n",
    "g = 0\n",
    "sigma = -g \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases for each layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        if hidden_sizes != [0]:\n",
    "            sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        else:\n",
    "            sizes = [input_size, output_size]\n",
    "\n",
    "        for i in range(len(sizes) - 1):\n",
    "            w = np.random.randn(sizes[i], sizes[i+1])\n",
    "            b = np.random.randn(1, sizes[i+1])\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __call__(self, x, params):\n",
    "        self.weights, self.biases = self.unflatten_params(params)\n",
    "        a = x\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = jnp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = celu(z)\n",
    "        a = jnp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a[0][0]\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def flatten_params(self):\n",
    "        params = jnp.array([])\n",
    "        for i in range(len(self.weights)):\n",
    "            params = jnp.concatenate((params, self.weights[i].flatten()))\n",
    "            params = jnp.concatenate((params, self.biases[i].flatten()))\n",
    "        return jnp.array(params)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def unflatten_params(self, params):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        start = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            end = start + self.weights[i].size\n",
    "            weights.append(jnp.reshape(jnp.array(params[start:end]), self.weights[i].shape))\n",
    "            start = end\n",
    "            end = start + self.biases[i].size\n",
    "            biases.append(jnp.reshape(jnp.array(params[start:end]), self.biases[i].shape))\n",
    "            start = end\n",
    "        return weights, biases\n",
    "    \n",
    "\n",
    "# initialize the network\n",
    "nn = NeuralNetwork(num_particles, [num_nodes], 1)\n",
    "\n",
    "\n",
    "# symmetrization transformation\n",
    "# I1 = x_1/C + x_2/C + ... + x_N/C\n",
    "# I2 = (x_1/C)^2 + (x_2/C)^2 + ... + (x_N/C)^2\n",
    "# ...\n",
    "# IN = (x_1/C)^N + (x_2/C)^N + ... + (x_N/C)^N\n",
    "@jit\n",
    "def transform(coords):\n",
    "    C = 2\n",
    "    ret = jnp.zeros(num_particles)\n",
    "    for i in range(num_particles):\n",
    "        ret = ret.at[i].set(jnp.sum(jnp.power(coords/C, i + 1)))\n",
    "    return ret\n",
    "\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-nn(transform(coords), params)) * jnp.exp(jnp.sum(-coords**2))\n",
    "\n",
    "@jit\n",
    "def delta_psi(coords, params):\n",
    "    return jnp.exp(-2 * nn(transform(coords), params)) * jnp.exp(jnp.sum(-coords**2))\n",
    "\n",
    "@jit\n",
    "def sample_body(coords_t, params, key, variation_size):    \n",
    "    gen_rand = jax.random.uniform(key,shape=(num_particles,), minval=-variation_size, maxval=variation_size)\n",
    "    new_key, subkey = jax.random.split(key)\n",
    "    coords_prime = coords_t + gen_rand\n",
    "    temp_rand = jax.random.uniform(subkey, minval=0, maxval=1)\n",
    "    return (jax.lax.cond(temp_rand < psi(coords_prime, params)**2/psi(coords_t, params)**2, lambda x, _: x, lambda _, y : y, coords_prime, coords_t), new_key)\n",
    "\n",
    "\n",
    "\n",
    "def sample(params, num_samples=10**3, thermalization_steps=200, skip_count=50, variation_size=1.0, key=jax.random.PRNGKey(np.random.randint(0,10000)), skipping = True):\n",
    "    outputs = []\n",
    "    all_data = []\n",
    "\n",
    "    coords_t = np.random.uniform(-variation_size, variation_size, num_particles)\n",
    "    for step in range(num_samples*skip_count + thermalization_steps + 1):\n",
    "        coords_t, key = sample_body(coords_t, params, key, variation_size)\n",
    "        if ((step > thermalization_steps) & (step % skip_count == 0) & (skipping == True)):\n",
    "            outputs.append(coords_t)\n",
    "        elif skipping == False:\n",
    "            all_data.append(coords_t)\n",
    "    if skipping == False:\n",
    "        return all_data\n",
    "    return jnp.array(outputs)\n",
    "\n",
    "\n",
    "# sample_body function except it also returns whether or not the move was accepted\n",
    "@jit\n",
    "def sample_body_accept(coords_t, params, key,variation_size):\n",
    "    gen_rand = jax.random.uniform(key, minval=-variation_size, maxval=variation_size)\n",
    "    new_key, subkey = jax.random.split(key)\n",
    "    \n",
    "    coords_prime = coords_t + gen_rand\n",
    "    condition = jax.random.uniform(subkey, minval=0, maxval=1) < psi(coords_prime, params)**2/psi(coords_t, params)**2\n",
    "    return (jax.lax.cond(condition, lambda x, _: x, lambda _, y : y, coords_prime, coords_t), new_key, condition)\n",
    "\n",
    "\n",
    "# the sample function without any thermalization steps or skipping steps\n",
    "def accept_ratio(params, num_samples=10**3, variation_size=5.0, key=jax.random.PRNGKey(np.random.randint(0,100))):\n",
    "    coords_t = np.random.uniform(-variation_size, variation_size)\n",
    "    num_accepted = 0\n",
    "    for _ in range(num_samples):\n",
    "        coords_t, key, accepted = sample_body_accept(coords_t, params, key, variation_size)\n",
    "        if accepted:\n",
    "            num_accepted += 1\n",
    "\n",
    "    return num_accepted / num_samples\n",
    "\n",
    "\n",
    "\n",
    "hessian = hessian(psi)\n",
    "ddpsi_single = jit(grad(jit(grad(psi, 0, allow_int = True)), 0, allow_int = True))\n",
    "\n",
    "@jit\n",
    "def ddpsi(coords, params):\n",
    "    if num_particles != 1:\n",
    "        return jnp.diagonal(hessian(coords, params))\n",
    "    else:\n",
    "        return ddpsi_single(coords, params)\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def Hpsi(coords, params):\n",
    "    # create a mask to ensure that j is less than i\n",
    "    mask = jnp.tril_indices(n=num_particles, k=-1)\n",
    "    # compute the absolute difference between particle coordinates\n",
    "    diff = jnp.abs(coords[mask[0]] - coords[mask[1]])\n",
    "    # sum the pairwise differences and multiply by sigma\n",
    "    dsum = sigma * jnp.sum(diff, axis=0)\n",
    "    return jnp.sum((m*.5*omega**2*coords**2)) - hbar**2 / (2*m) * jnp.sum(ddpsi(coords, params)) * 1/psi(coords, params) + dsum\n",
    "\n",
    "venergy = jit(vmap(Hpsi, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def logpsi(coords, params):\n",
    "    return jnp.log(psi(coords, params))\n",
    "\n",
    "# define the derivative with respect to every parameter of the log of psi:\n",
    "dlogpsi_dtheta_stored = jit(grad(logpsi, 1))\n",
    "\n",
    "vlog_term = jit(vmap(dlogpsi_dtheta_stored, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "@jit\n",
    "def delta_dist(coord, samples):\n",
    "    # get all of the x_2 coordinates \n",
    "    y_star = jnp.sort(samples[:,1])[int(len(samples) * .99)]\n",
    "    epsilon = 10**(-10)\n",
    "    alpha = jnp.sqrt(-jnp.log(epsilon)*y_star**2)\n",
    "    return 1/(alpha * jnp.sqrt(jnp.pi))*jnp.exp(-coord**2/alpha**2)\n",
    "\n",
    "\n",
    "@jit\n",
    "def delta_integral(coords, param, samples):\n",
    "    prefactor = .5 * g * num_particles * (num_particles - 1) \n",
    "    # replace the second element of coords with the first element\n",
    "    modified_coords = coords\n",
    "    modified_coords = modified_coords.at[1].set(modified_coords[0])\n",
    "    return prefactor * delta_dist(coords[1], samples) * delta_psi(modified_coords, param) / delta_psi(coords, param) \n",
    "\n",
    "v_delta_integral = jit(vmap(delta_integral, in_axes=(0, None, None), out_axes=0))\n",
    "\n",
    "# derivative of the neural network with respect to the parameters\n",
    "df_dtheta = jit(grad(psi, 1))\n",
    "# vectorized version of the derivative of the neural network with respect to the parameters\n",
    "vdf_dtheta = jit(vmap(df_dtheta, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def gradient_delta_integral(coords, params, samples):\n",
    "    prefactor = .5 * g * num_particles * (num_particles - 1)\n",
    "    # first compute the derivative of the neural network with respect to the parameters, but for the modified coordinates\n",
    "    mod_coords = coords\n",
    "    mod_coords = mod_coords.at[1].set(mod_coords[0])\n",
    "    df_dthetas = df_dtheta(mod_coords, params)\n",
    "    # multiply by the ratio of the two wavefunctions\n",
    "    ratio = delta_psi(mod_coords, params) / delta_psi(coords, params)\n",
    "    # then multiply by the delta distribution\n",
    "    delta_val = delta_dist(coords[1], samples)\n",
    "\n",
    "    return prefactor * delta_val * ratio * df_dthetas\n",
    "\n",
    "# vectorized version of the gradient_delta_integral function\n",
    "vgradient_delta_integral = jit(vmap(gradient_delta_integral, in_axes=(0, None, None), out_axes=0))\n",
    "\n",
    "def gradient(params, num_samples=10**3, thermal=200, skip=50, variation_size=1.0, verbose=False):\n",
    "    # get the samples\n",
    "    samples = sample(params, num_samples, thermal, skip, variation_size)\n",
    "\n",
    "    psiHpsi = venergy(samples, params)\n",
    "    delta_I = v_delta_integral(samples, params, samples) \n",
    "    df_dthetas = vdf_dtheta(samples, params)\n",
    "    I_e_samples = vgradient_delta_integral(samples, params, samples)    \n",
    "    \n",
    "\n",
    "    avg_df_dtheta = 1/num_samples * jnp.sum(df_dthetas, 0)\n",
    "    energy_without_delta = 1/num_samples * (jnp.sum(psiHpsi))\n",
    "    I_e = 1/num_samples * jnp.sum(I_e_samples, 0)\n",
    "\n",
    "    energy = energy_without_delta + jnp.sum(delta_I)\n",
    "    if verbose:\n",
    "        print(energy)\n",
    "    uncert = jnp.std(psiHpsi + delta_I)/jnp.sqrt(num_samples)\n",
    "    gradient_calc = 2 * energy  * avg_df_dtheta  - 2 * avg_df_dtheta * energy_without_delta - 2 * I_e \n",
    "    return gradient_calc, energy, uncert\n",
    "\n",
    "# define a function that takes in samples, bins them, and returns the average of each bin\n",
    "def bin_samples(energies, bin_size):\n",
    "    # first, bin the samples\n",
    "    binned = np.array_split(energies, bin_size)\n",
    "    # now, calculate the average of each bin\n",
    "    binned_averages = [np.mean(b) for b in binned]\n",
    "    # now, calculate the uncertainty of each bin\n",
    "    bin_uncerts = np.std(binned_averages)/np.sqrt(bin_size)\n",
    "    return bin_uncerts\n",
    "\n",
    "\n",
    "# define a function that gets all samples, and then bins them with different bin sizes\n",
    "def autocorrelation(params):\n",
    "    samples = sample(params, num_samples=10**3, thermalization_steps=200, skip_count=40, variation_size=1, key=jax.random.PRNGKey(np.random.randint(0,100)), skipping = False)\n",
    "    energies = [Hpsi(s, params) for s in samples]\n",
    "    \n",
    "    bins = np.linspace(1, 100, 100, dtype=int)\n",
    "    # now plot the average energy as a function of the number of bins\n",
    "    us = []\n",
    "    for b_size in bins:\n",
    "        us.append(bin_samples(energies, b_size))\n",
    "    plt.scatter(bins, us)\n",
    "    plt.title(\"Bin size vs. Uncertainty\")\n",
    "    plt.xlabel(\"Bin size\")\n",
    "    plt.ylabel(\"Uncertainty\")\n",
    "    plt.show()\n",
    "\n",
    "def train(params, iterations, N, thermal, skip, variation_size, optimizer):\n",
    "    hs = []\n",
    "    us = []\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    def step(params, opt_state, N, thermal, skip, variation_size):\n",
    "        gr = gradient(params, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        hs.append(gr[1])\n",
    "        us.append(gr[2])\n",
    "        updates, opt_state = optimizer.update(gr[0], opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, gr[1]\n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "    for step_num in pbar:   \n",
    "        params, opt_state, energy = step(params, opt_state, N, thermal, skip, variation_size)\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocorrelation(nn.flatten_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept/Reject ratio 0.44\n"
     ]
    }
   ],
   "source": [
    "variation = 1.\n",
    "print(\"Accept/Reject ratio\",accept_ratio(nn.flatten_params(), num_samples=10**3, variation_size=variation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Energy = 3.3104029: 100%|██████████| 50/50 [00:18<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "total_hists = []\n",
    "total_uncerts = []\n",
    "start_params = nn.flatten_params()\n",
    "\n",
    "hists, uncerts, params = train(start_params, 50, 10**3, 200, 40, variation, optax.adam(0.01))\n",
    "total_hists = total_hists + list(hists)\n",
    "total_uncerts = total_uncerts + list(uncerts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists, uncerts, params = train(params, 50, 10**3, 200, 40, variation, optax.adam(0.01))\n",
    "total_hists = total_hists + list(hists)\n",
    "total_uncerts = total_uncerts + list(uncerts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_energy = .5 * hbar * omega * num_particles\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists, label=\"Adam, \" + str(round(total_hists[-1],3)) + \" +/- \" + str(round(total_uncerts[-1], 2)))\n",
    "# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# get the network structure\n",
    "structure = nn.hidden_sizes\n",
    "plt.annotate(\" Network Structure: \" + str(structure), xy=(0.1, 0.95), xycoords='axes fraction')\n",
    "plt.plot(np.arange(0, len(total_hists)), [true_energy for x in np.arange(0, len(total_hists))], label=r\"True Energy, \" + str(round(true_energy,3)))\n",
    "pdiff = (total_hists[-1] - true_energy)/true_energy*100\n",
    "plt.annotate(\" Final Percent Diff = \" + str(round(pdiff,3)) + \"%\", xy=(.1, .9), xycoords= 'axes fraction')\n",
    "plt.legend()\n",
    "plt.title(\"Average Energy vs. Iteration, \" + str(num_particles) + \" particles\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
