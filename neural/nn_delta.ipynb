{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import gvar as gv\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import jax.example_libraries.optimizers as jax_opt\n",
    "from tqdm import trange\n",
    "import cProfile\n",
    "\n",
    "\n",
    "#TODO: plot relative error against the number of parameters\n",
    "\n",
    "num_particles = 40\n",
    "N = num_particles\n",
    "structure = [50,100,200,300,300,200,100,50]\n",
    "# structure = [35,35]\n",
    "num_nodes = np.sum(structure)\n",
    "m = 1\n",
    "hbar = 1\n",
    "omega = 1\n",
    "harmonic_omega = 1\n",
    "g = -1\n",
    "sigma = -g/2\n",
    "C = 6\n",
    "FILENAME = \"energies.csv\"\n",
    "\n",
    "INITIAL_SAMPLE = jnp.array(np.random.uniform(-5, 5, N))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases for each layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        if hidden_sizes != [0]:\n",
    "            sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        else:\n",
    "            sizes = [input_size, output_size]\n",
    "\n",
    "        for i in range(len(sizes) - 1):\n",
    "            w = np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2/sizes[i])\n",
    "            b = np.random.randn(1, sizes[i+1]) \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def transform(self, coords):\n",
    "       # if running into NaNs, try to increase this\n",
    "        ret = jnp.zeros(num_particles)\n",
    "        for i in range(num_particles):\n",
    "            ret = ret.at[i].set(jnp.sum(jnp.power(coords/C, i + 1)))\n",
    "        return ret \n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __call__(self, x, params):\n",
    "        x = self.transform(x)\n",
    "        self.weights, self.biases = self.unflatten_params(params)\n",
    "        a = x\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = jnp.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = celu(z)\n",
    "        a = jnp.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a[0][0]\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def flatten_params(self):\n",
    "        params = jnp.array([])\n",
    "        for i in range(len(self.weights)):\n",
    "            params = jnp.concatenate((params, self.weights[i].flatten()))\n",
    "            params = jnp.concatenate((params, self.biases[i].flatten()))\n",
    "        return jnp.array(params)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def unflatten_params(self, params):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        start = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            end = start + self.weights[i].size\n",
    "            weights.append(jnp.reshape(jnp.array(params[start:end]), self.weights[i].shape))\n",
    "            start = end\n",
    "            end = start + self.biases[i].size\n",
    "            biases.append(jnp.reshape(jnp.array(params[start:end]), self.biases[i].shape))\n",
    "            start = end\n",
    "        return weights, biases\n",
    "    \n",
    "\n",
    "# initialize the network\n",
    "nn = NeuralNetwork(num_particles, structure, 1)\n",
    "\n",
    "# symmetrization transformation\n",
    "# I1 = x_1/C + x_2/C + ... + x_N/C\n",
    "# I2 = (x_1/C)^2 + (x_2/C)^2 + ... + (x_N/C)^2\n",
    "# ...\n",
    "# IN = (x_1/C)^N + (x_2/C)^N + ... + (x_N/C)^N\n",
    "\n",
    "@jit\n",
    "def A(coords, params):\n",
    "    return nn(coords, params) + omega * jnp.sum(coords**2)\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-A(coords, params)) \n",
    "\n",
    "# sample_body function except it also returns whether or not the move was accepted\n",
    "# @jit\n",
    "# def sample_body_accept(coords_t, params, key, variation_size):\n",
    "#     gen_rand = jax.random.uniform(key, minval=-variation_size, maxval=variation_size)\n",
    "#     new_key, subkey = jax.random.split(key)\n",
    "    \n",
    "#     coords_prime = coords_t + gen_rand\n",
    "#     r = jax.random.uniform(subkey, minval=0, maxval=1)\n",
    "#     condition = r <= psi(coords_prime, params)**2/psi(coords_t, params)**2\n",
    "#     return (jax.lax.cond(condition, lambda x, _: x, lambda _, y : y, coords_prime, coords_t), new_key, condition)\n",
    "\n",
    "\n",
    "# the sample function without any thermalization steps or skipping steps\n",
    "def accept_ratio(params, num_samples=10**3, variation_size=5.0, key=jax.random.PRNGKey(np.random.randint(0,100))):\n",
    "    coords_t = np.random.uniform(-variation_size, variation_size)\n",
    "    num_accepted = 0\n",
    "    for _ in range(num_samples):\n",
    "        coords_t, key, accepted = sample_body_accept(coords_t, params, key, variation_size)\n",
    "        if accepted:\n",
    "            num_accepted += 1\n",
    "\n",
    "    return num_accepted / num_samples\n",
    "\n",
    "\n",
    "#### New sampling function\n",
    "# def sample(params, num_samples=10**3, thermalization_steps=200, skip_count=50, variation_size=1.0):\n",
    "#     outputs = []\n",
    "#     num_accepted = 0\n",
    "#     num_total = num_samples * skip_count + thermalization_steps + 1\n",
    "#     rand_coords = np.random.uniform(-variation_size, variation_size, size=(num_total, num_particles))\n",
    "#     rand_accepts = np.random.uniform(0, 1, size=num_total)\n",
    "\n",
    "#     coords_t = jnp.zeros(num_particles)\n",
    "#     for step in range(num_total):\n",
    "#         coords_t, accepted = sample_body(params, coords_t, rand_coords[step], rand_accepts[step])\n",
    "#         if accepted:\n",
    "#             num_accepted += 1\n",
    "#         if ((step > thermalization_steps) and (step % skip_count == 0)):\n",
    "#             outputs.append(coords_t)\n",
    "#     # create a second output array, where the second coordinate is equal to the first coordinate\n",
    "#     outputs_prime = outputs.copy()\n",
    "#     for i in range(len(outputs)):\n",
    "#         a = np.array(outputs[i])\n",
    "#         a[1] = a[0]\n",
    "#         outputs_prime[i] = jnp.array(a)\n",
    "#     return jnp.array(outputs), jnp.array(outputs_prime), num_accepted/num_total\n",
    "\n",
    "\n",
    "@jit\n",
    "def mcstep_E(xis, limit, positions, params):\n",
    "    \n",
    "    params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "    \n",
    "    newpositions = jnp.array(positions) + xis\n",
    "    \n",
    "    prob = psi(newpositions, params)**2./psi(positions, params)**2.\n",
    "    \n",
    "    def truefunc(p):\n",
    "        return [newpositions, True]\n",
    "\n",
    "    def falsefunc(p):\n",
    "        return [positions, False]\n",
    "    \n",
    "    return jax.lax.cond(prob >= limit, truefunc, falsefunc, prob)\n",
    "\n",
    "def sample(params, Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "    sq = []\n",
    "    sq_prime = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "    randoms = np.random.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = np.random.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "    # generate the primed samples by going through every sample and making sample[N_up] = sample[0]\n",
    "    sq_prime = sq.copy()\n",
    "    for i in range(len(sq)):\n",
    "        a = np.array(sq[i])\n",
    "        a[1] = a[0]\n",
    "        sq_prime[i] = jnp.array(a) \n",
    "\n",
    "    return jnp.array(sq), jnp.array(sq_prime), counter/num_total\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def sample_body(params, coords_t, rand_coords, rand_accepts):\n",
    "    coords_prime = coords_t + rand_coords\n",
    "    return jax.lax.cond(rand_accepts < psi(coords_prime, params)**2/psi(coords_t, params)**2, lambda x,_: (x,True) , lambda _,y: (y,False), coords_prime, coords_t)\n",
    "\n",
    "# first derivative of the neural network with respect to the coordinates\n",
    "# in Andy's notation this is dA/dx\n",
    "dA_dx = jit(grad(A, 0)) # type: ignore\n",
    "\n",
    "# second derivative of the neural network with respect to the coordinates\n",
    "# in Andy's notation this is d^2A/dx^2\n",
    "A_hessian = jax.jacfwd(dA_dx, 0) # type: ignore\n",
    "\n",
    "@jit\n",
    "def d2A_dx2(coords, params):\n",
    "    #return jnp.diagonal(A_hessian(transform(coords), params))\n",
    "    return jnp.diag(A_hessian(coords, params))\n",
    "\n",
    "@jit\n",
    "def Hpsi(coords, coords_prime, params, alpha):\n",
    "    return Hpsi_without_delta(coords, params) + delta_potential(coords,coords_prime, params, alpha)\n",
    "\n",
    "@jit\n",
    "def sigma_term(coords):\n",
    "    N = num_particles \n",
    "    sigma_term = 0\n",
    "    for i in range(N):\n",
    "        for j in range(i,N):\n",
    "            sigma_term += sigma* jnp.abs(coords[i] - coords[j])  \n",
    "\n",
    "@jit\n",
    "def Hpsi_without_delta(coords, params):\n",
    "   # sigma term\n",
    "    N = num_particles \n",
    "    sigma_term = 0\n",
    "    for i in range(N):\n",
    "        for j in range(i,N):\n",
    "            sigma_term += sigma* jnp.abs(coords[i] - coords[j]) \n",
    "    # return jnp.sum((m*.5*omega**2*coords**2)) - hbar**2 / (2*m) * jnp.sum(ddpsi(coords, params) ) * 1/psi(coords, params) + sigma_term \n",
    "    return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(dA_dx(coords, params)**2)) + m*.5*harmonic_omega**2* jnp.sum(coords**2) + sigma_term\n",
    "    # return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(dA_dx(coords, params)**2))\n",
    "    # return 1/(2*m) * (jnp.sum(d2A_dx2(coords, params)) - jnp.sum(dA_dx(coords, params)**2)) + m*.5*omega**2* jnp.sum(coords**2)\n",
    "\n",
    "@jit\n",
    "def second_term(coords, params):\n",
    "    return dnn_dtheta(coords, params) * Hpsi_without_delta(coords, params)\n",
    "\n",
    "vsecond_term = jit(vmap(second_term, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def third_term(coords,coords_prime, params, y_max):\n",
    "    return dnn_dtheta(coords_prime, params) * delta_potential(coords, coords_prime, params, y_max)\n",
    "\n",
    "vthird_term = jit(vmap(third_term, in_axes=(0,0, None, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def delta_potential(coords, coords_prime, params, alpha):\n",
    "    N = num_particles    \n",
    "    # compute e^(-2 NN(params_prime))\n",
    "    # ratio = jnp.exp(-2 * A(coords_prime, params) + 2 * A(coords, params))\n",
    "    ratio = (psi(coords_prime, params)**2)/(psi(coords, params)**2)\n",
    "    delta_dist = (1/(jnp.sqrt(jnp.pi) * alpha)) * jnp.exp(-(coords[1]**2)/(alpha**2))\n",
    "    return g * N*(N-1)/2 * ratio * delta_dist\n",
    "\n",
    "vdelta_potential = jit(vmap(delta_potential, in_axes=(0,0, None, None), out_axes=0))\n",
    "venergy = jit(vmap(Hpsi, in_axes=(0,0, None, None), out_axes=0))\n",
    "vHpsi_without_delta = jit(vmap(Hpsi_without_delta, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "\n",
    "# derivative of the neural network with respect to every parameter\n",
    "# in Andy's notation this is dA/dtheta\n",
    "dnn_dtheta = jit(grad(A, 1)) \n",
    "vdnn_dtheta = vmap(dnn_dtheta, in_axes=(0, None), out_axes=0)\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "def gradient(params, num_samples=10**3, thermal=200, skip=50, variation_size=1.0, verbose=False):\n",
    "    # get the samples\n",
    "    samples, samples_prime, _  = sample(params, num_samples, thermal, skip, variation_size)\n",
    "\n",
    "    y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "    alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "\n",
    "    psiHpsi = venergy(samples, samples_prime, params, alpha) \n",
    "    # Hpsi_terms_without_delta = vHpsi_without_delta(samples, params)\n",
    "    # delta_term = vdelta_potential(samples,samples_prime, params, samples)\n",
    "\n",
    "    # delta function additions\n",
    "    dA_dtheta = vdnn_dtheta(samples, params)\n",
    "    # dA_dtheta_repeated = vdnn_dtheta(samples_prime, params)\n",
    "\n",
    "    dA_dtheta_avg = 1/num_samples * jnp.sum(dA_dtheta, 0)\n",
    "\n",
    "    second_term = 1/num_samples * jnp.sum(vsecond_term(samples, params), 0)\n",
    "    third_term = 1/num_samples * jnp.sum(vthird_term(samples, samples_prime, params, alpha), 0)\n",
    "    # third_term =1/num_samples * jnp.sum(vboth(dA_dtheta_repeated,delta_term), 0)\n",
    "    uncert = jnp.std(psiHpsi)/jnp.sqrt(num_samples)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(psiHpsi)\n",
    "\n",
    "   \n",
    "    if verbose:\n",
    "        print(energy)\n",
    "\n",
    "    gradient_calc = 2 * energy * dA_dtheta_avg - 2 * second_term - 2*third_term\n",
    "    return gradient_calc, energy, uncert\n",
    "\n",
    "def ugradient(params, num_samples=10**3, thermal=200, skip=50, variation_size=1.0, verbose=False):\n",
    "\n",
    "    samples, samples_prime, _ = sample(params, num_samples, thermal, skip, variation_size)\n",
    "    y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "    alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "    Es = []\n",
    "    dA_dthetas = []\n",
    "    seconds = []\n",
    "    thirds = []\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        coord = samples[i]\n",
    "        coord_prime = samples_prime[i]\n",
    "\n",
    "        Es.append(Hpsi(coord, coord_prime, params, alpha))\n",
    "        dA_dthetas.append(dnn_dtheta(coord, params)) \n",
    "        seconds.append(second_term(coord, params))\n",
    "        thirds.append(third_term(coord, coord_prime, params, alpha))\n",
    "\n",
    "\n",
    "    Es = jnp.array(Es)\n",
    "    dA_dthetas = jnp.array(dA_dthetas)\n",
    "    seconds = jnp.array(seconds)\n",
    "    thirds = jnp.array(thirds)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(Es)\n",
    "    avg_dA_dtheta = 1/num_samples * jnp.sum(dA_dthetas, 0)\n",
    "    second = 1/num_samples * jnp.sum(seconds, 0)\n",
    "    third =  1/num_samples * jnp.sum(thirds, 0)\n",
    "\n",
    "    uncert = jnp.std(Es)/jnp.sqrt(num_samples)\n",
    "    \n",
    "    gradient_calc = 2 * energy * avg_dA_dtheta - 2 * second - 2 * third\n",
    "    return gradient_calc, energy, uncert\n",
    "\n",
    "\n",
    "# define a function that takes in samples, bins them, and returns the average of each bin\n",
    "def bin_samples(energies, bin_size):\n",
    "    # first, bin the samples\n",
    "    binned = np.array_split(energies, bin_size)\n",
    "    # now, calculate the average of each bin\n",
    "    binned_averages = [np.mean(b) for b in binned]\n",
    "    # now, calculate the uncertainty of each bin\n",
    "    bin_uncerts = np.std(binned_averages)/np.sqrt(bin_size)\n",
    "    return bin_uncerts\n",
    "\n",
    "\n",
    "# define a function that gets all samples, and then bins them with different bin sizes\n",
    "def autocorrelation(params):\n",
    "    samples = sample(params, num_samples=10**3, thermalization_steps=200, skip_count=40, variation_size=1)[0]\n",
    "    energies = [Hpsi(s, params) for s in samples]\n",
    "    \n",
    "    bins = np.linspace(1, 100, 100, dtype=int)\n",
    "    # now plot the average energy as a function of the number of bins\n",
    "    us = []\n",
    "    for b_size in bins:\n",
    "        us.append(bin_samples(energies, b_size))\n",
    "    plt.scatter(bins, us)\n",
    "    plt.title(\"Bin size vs. Uncertainty\")\n",
    "    plt.xlabel(\"Bin size\")\n",
    "    plt.ylabel(\"Uncertainty\")\n",
    "    plt.show()\n",
    "\n",
    "def step(params_arg, step_num, N, thermal, skip, variation_size):\n",
    "        gr = gradient(params_arg, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        # hs.append(gr[1])\n",
    "        # us.append(gr[2])\n",
    "        opt_state = opt_init(params_arg)\n",
    "        new = opt_update(step_num, gr[0], opt_state)\n",
    "        return get_params(new), gr[1], gr[2]\n",
    "\n",
    "def train(params, iterations, N, thermal, skip, variation_size):\n",
    "    hs = []\n",
    "    us = [] \n",
    "    ns = np.arange(iterations) \n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:   \n",
    "        new_params, energy, uncert = step(old_params, step_num, N, thermal, skip, variation_size)\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        # write the energy to the file\n",
    "        with open(FILENAME, mode='a') as f:\n",
    "           f.write(str(energy) + \",\" + str(uncert) + \"\\n\") \n",
    "        old_params = new_params.copy()\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_step_size(params, start):\n",
    "    lr = .1\n",
    "    target = 0.5\n",
    "    tolerance = .05\n",
    "    max_it = 1000\n",
    "    step = start\n",
    "    best_step = start\n",
    "    best_acc = 0\n",
    "    it_num = 0\n",
    "    # get the samples \n",
    "    _, _, acc = sample(params, 1000, 100, 10, step)\n",
    "    # while the acceptance rate is not within +/- .5 of the target\n",
    "    while (acc < target - tolerance or acc > target + tolerance) and it_num < max_it:\n",
    "        it_num += 1\n",
    "        # if the acceptance rate is too low, increase the step size\n",
    "        if acc < target - tolerance:\n",
    "            step -= lr\n",
    "        # if the acceptance rate is too high, decrease the step size\n",
    "        elif acc > target + tolerance:\n",
    "            step += lr\n",
    "        # if we cross the target, decrease the learning rate and go back\n",
    "        if (acc < target and best_acc > target) or (acc > target and best_acc < target):\n",
    "            lr /= 2\n",
    "            step = best_step\n",
    "        # keep track of the best step size\n",
    "        if abs(acc - target) < abs(best_acc - target):\n",
    "            best_acc = acc\n",
    "            best_step = step\n",
    "        \n",
    "        # get the samples for the next step size\n",
    "        _, _, acc = sample(params, 1000, 100, 10, step)\n",
    "    print(\"step size:\",best_step)\n",
    "    return best_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters 173051\n",
      "Log of parameters 5.238174113270292\n"
     ]
    }
   ],
   "source": [
    "print(\"number of parameters\" , len(nn.flatten_params()))\n",
    "print(\"Log of parameters\", np.log10(len(nn.flatten_params())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(FILENAME, 'w').close()\n",
    "# start_params = nn.flatten_params()\n",
    "start_params = nn.flatten_params()\n",
    "\n",
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-3))\n",
    "\n",
    "resultsa = train(start_params, 30, 1500, 100, 10, find_step_size(start_params, 1))\n",
    "# 0 -> energies\n",
    "# 1 -> uncert\n",
    "# 2 -> steps\n",
    "# 3 -> params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_init, opt_update, get_params = jax_opt.adam(10**(-3))\n",
    "resultsb = train(resultsa[3], 200, 2000, 500, 10, find_step_size(resultsa[3], .9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10**(-4))\n",
    "resultsc = train(resultsb[3], 300, 4000, 500, 10, find_step_size(resultsb[3], .3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultsd = train(resultsc[3], 300, 1000, 1000, 10, find_step_size(resultsc[3], .3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "def harmonic_energy(n):\n",
    "    return .5 + 2*n - np.sqrt(2/np.pi) * (n+1/2)/g\n",
    "\n",
    "def astra_energy():\n",
    "    return (N * omega)/2 - m * g**2  * (N*(N**2 - 1))/(24)\n",
    "\n",
    "\n",
    "true_energy = .5 * hbar * omega * num_particles\n",
    "true_energy = harmonic_energy(0)\n",
    "true_energy = astra_energy()\n",
    "#g = 1, sigma = -g/2\n",
    "# true_energy = .75\n",
    "# g = .1, sigma = 0:\n",
    "# true_energy = 1.03881\n",
    "# g= .8, sigma= -g\n",
    "# true_energy = .9375\n",
    "# true_energy = 0.3098\n",
    "\n",
    "total_hists =  resultsa[0] + resultsb[0] + resultsc[0]  \n",
    "# + resultsd[0] \n",
    "#+ resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1] + resultsb[1] + resultsc[1] \n",
    "# + resultsd[1] \n",
    "#+ resultse[1]\n",
    "# + resultsd[1]\n",
    "\n",
    "# get index of minimum value\n",
    "min_val = np.min(total_hists)\n",
    "min_val = total_hists[-1]\n",
    "min_index = total_hists.index(min_val)\n",
    "min_err = total_uncerts[min_index]\n",
    "val = gv.gvar(min_val, min_err)\n",
    "fractional_error = (val - true_energy)/true_energy\n",
    "print(\"Minimum value: \", val)\n",
    "print(\"Fractional error: \", fractional_error)\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists, label=\"Adam: \" + str(val))\n",
    "# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# get the network structure\n",
    "structure = nn.hidden_sizes\n",
    "plt.annotate(\" Network Structure: \" + str(structure), xy=(0.1, 0.95), xycoords='axes fraction')\n",
    "plt.plot(np.arange(0, len(total_hists)), [true_energy for x in np.arange(0, len(total_hists))], label=r\"Analytic: \" + str(round(true_energy,3)))\n",
    "pdiff = (min_val - true_energy)/true_energy*100\n",
    "# plt.annotate(\" Final Percent Diff = \" + str(round(pdiff,3)) + \"%\", xy=(.1, .9), xycoords= 'axes fraction')\n",
    "plt.legend()\n",
    "plt.title(r\"Modified LL model ($g = $\" + str(g) + r\", $\\sigma =\" + str(sigma) + \"$), \" + str(num_particles) + \" particles\")\n",
    "print(min_val)\n",
    "print(min_err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_samples(energies, bin_size):\n",
    "    # first, bin the samples\n",
    "    binned = np.array_split(energies, bin_size)\n",
    "    # now, calculate the average of each bin\n",
    "    binned_averages = [np.mean(b) for b in binned]\n",
    "    # now, calculate the uncertainty of each bin\n",
    "    bin_uncerts = np.std(binned_averages)/np.sqrt(bin_size)\n",
    "    return bin_uncerts\n",
    "\n",
    "\n",
    "samples, samples_prime, _ = sample(resultsc[3], 20000, 100, 10, 1)\n",
    "y_max = jnp.max(jnp.abs(jnp.array(samples[:,1])))\n",
    "alpha = y_max/(jnp.sqrt(-jnp.log(jnp.sqrt(jnp.pi) * 10**(-10))))\n",
    "energies = venergy(samples,samples_prime, resultsc[3], alpha)\n",
    "mean_energy = jnp.mean(energies)\n",
    "print(mean_energy)\n",
    "\n",
    "# bins = np.linspace(1, 100, 100, dtype=int)\n",
    "bins = np.array([1,2,5,10,20,50,100,150,200,250,300,360,450,500,550,600,660,750,900,990,1100])\n",
    "# now plot the average energy as a function of the number of bins\n",
    "us = []\n",
    "for b_size in bins:\n",
    "    us.append(bin_samples(energies, b_size))\n",
    "plt.scatter(bins, us)\n",
    "plt.title(\"Bin size vs. Uncertainty\")\n",
    "plt.xlabel(\"Bin size\")\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "print(max(us))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = gv.gvar(mean_energy, max(us))\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, .05)\n",
    "ys = np.arange(-5, 5, .05)\n",
    "wavs = []\n",
    "for i in range(len(xs)):\n",
    "    for j in range(len(ys)):\n",
    "        wavs.append(psi(np.array([xs[i], ys[j]]), resultsc[3])**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(xs,ys)\n",
    "Z = np.array(wavs).reshape(len(xs), len(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.contourf(X, Y, Z, 100)\n",
    "\n",
    "plt.plot(xs,-xs)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(resultsc[3], 10**4, 100, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_density(samples):\n",
    "    x_bins = np.linspace(-9, 9, 72)\n",
    "    dx = x_bins[1] - x_bins[0]  # Bin width\n",
    "    n_x = np.zeros_like(x_bins)\n",
    "\n",
    "    # bin the x_1s\n",
    "    for x in samples[0][:,0]:\n",
    "        n_x[np.digitize(x, x_bins)] += 1\n",
    "\n",
    "    # Normalize\n",
    "    n_x /= (dx * np.sum(n_x))\n",
    "\n",
    "    return x_bins, n_x\n",
    "\n",
    "x_bins, n_x = local_density(samples)\n",
    "plt.plot(x_bins, n_x,'-o', markersize=2, color=\"red\")\n",
    "plt.title(r\"$N = 3$ Local Density Profile\")\n",
    "plt.xlabel(\"$x$\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.ylabel(\"$n(x)/\\int dx n(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
