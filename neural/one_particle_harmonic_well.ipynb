{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import time\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import optax\n",
    "from tqdm import trange\n",
    "\n",
    "hbar = 1\n",
    "m = 1\n",
    "omega = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    #[1, 5, 10, 10, 10, 10, 5, 1]\n",
    "    def __init__(self, node_counts=[1, 50, 1]):\n",
    "        # defining the structure of the neural network\n",
    "        self.num_layers = len(node_counts)\n",
    "        # the number of nodes for each layer\n",
    "        self.node_counts = node_counts\n",
    "        # the total number of weights\n",
    "        self.params_length = 0\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.params_length += node_counts[i] * node_counts[i + 1]\n",
    "            i+=1\n",
    "        # the list that stores the weight matrices (index 0 is the connections from the input to the first hidden layer)\n",
    "        self.weights = []\n",
    "        # storage for all the biases\n",
    "        self.biases = []\n",
    "        \n",
    "        # generate weight matrices with the correct sizes, and random elements\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights.append(np.random.randn(node_counts[i + 1], node_counts[i]) * np.sqrt(1. / (node_counts[i + 1])))\n",
    "        self.weights = np.array(self.weights, dtype=object)\n",
    "        \n",
    "        # generate the bias arrays\n",
    "        for i in range(self.num_layers - 1):\n",
    "            temp = np.random.randn(node_counts[i + 1]) * np.sqrt(1. / node_counts[i + 1])\n",
    "            self.biases.append(temp[:, None])\n",
    "        self.biases = np.array(self.biases, dtype=object)\n",
    "        # get the shape for reshaping a 1d array to this later\n",
    "        self.dimensions = []\n",
    "        for m in self.weights:\n",
    "            self.dimensions.append(m.shape)\n",
    "\n",
    "    # define the activation function that we use for the layers\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def l_act(self, x):\n",
    "        return celu(x)\n",
    "        #return 1.0 / (1.0 + jnp.exp(-x))    \n",
    "    # define the activation function for the output\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def o_act(self, x):\n",
    "        return celu(x)\n",
    "        #return x\n",
    "        #return 1.0 / (1.0 + jnp.exp(-x))\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def conv1d(self):\n",
    "        flattened = []\n",
    "        # loop through each matrix\n",
    "        for m in range(self.num_layers - 1):\n",
    "            mat = self.weights[m]\n",
    "            for i in range(mat.shape[0]):\n",
    "                for j in range(mat.shape[1]):\n",
    "                    flattened.append(mat[i][j])\n",
    "        for m in range(self.num_layers - 1):\n",
    "            # add the biases onto the end\n",
    "            bias = self.biases[m]\n",
    "            for element in bias:\n",
    "                flattened.append(element)\n",
    "        return jnp.array(flattened)\n",
    "        \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def convnd(self, params):\n",
    "        ret = []\n",
    "        ret_bias = []\n",
    "        # the last index of the matrix\n",
    "        max_ind = 0\n",
    "        prev_ind = 0\n",
    "        for dim in self.dimensions:\n",
    "            max_ind += dim[0]*dim[1]\n",
    "            temp = params[prev_ind : max_ind]\n",
    "            prev_ind = max_ind\n",
    "            ret.append(jnp.array(temp).reshape(dim))\n",
    "        biases_flattened = params[max_ind :]\n",
    "        # now reconstruct the bias matrices\n",
    "        # print(biases_flattened)\n",
    "        ind = 0\n",
    "        for i in range(self.num_layers - 1):\n",
    "            ret_bias.append((biases_flattened[ind: ind + self.node_counts[i + 1]])[:, None])\n",
    "            ind += self.node_counts[i + 1]\n",
    "        return ret, ret_bias\n",
    "\n",
    "\n",
    "    # passing inputs into the neural network and getting an output\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def output(self, coords, params):\n",
    "        # format the parameters as weights\n",
    "        c = self.convnd(params)\n",
    "        self.weights = c[0]\n",
    "        self.biases = c[1]\n",
    "        # compute the output of the neural network\n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = jnp.array(self.weights[i])\n",
    "            b = jnp.array(self.biases[i])\n",
    "            # print(w)\n",
    "            # print(b)\n",
    "            # if its the first layer, dot it against the input and use the activation function\n",
    "            if i == 0:\n",
    "                temp = self.l_act(jnp.dot(w, coords))\n",
    "            elif (i < self.num_layers):\n",
    "                \n",
    "                temp = self.l_act(jnp.dot(w, temp) + b)\n",
    "            else:\n",
    "                # on the output layer we use the output activation function\n",
    "                temp = self.o_act(jnp.dot(w, temp) + b)\n",
    "        # print(\"output:\" + str(time.time() -  start))\n",
    "        return temp[0][0]\n",
    "\n",
    "# create the network object\n",
    "nn = Network()\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-nn.output(coords, params)) * jnp.exp(-coords**2)\n",
    "\n",
    "@jit\n",
    "def sample_body(coords_t, params, key):    \n",
    "    gen_rand = jax.random.uniform(key, minval=-5, maxval=5)\n",
    "    new_key, subkey = jax.random.split(key)\n",
    "    \n",
    "    coords_prime = coords_t + gen_rand\n",
    "    temp_rand = jax.random.uniform(subkey, minval=0, maxval=1)\n",
    "    return (jax.lax.cond(temp_rand < psi(coords_prime, params)**2/psi(coords_t, params)**2, lambda x, _: x, lambda _, y : y, coords_prime, coords_t), new_key)\n",
    "\n",
    "\n",
    "def sample(params, num_samples=10**3, thermalization_steps=200, skip_count=50, key=jax.random.PRNGKey(np.random.randint(0,100))):\n",
    "    outputs = []\n",
    "    # e_plots = []\n",
    "    coords_t = np.random.uniform(-1, 1)\n",
    "    for step in range(num_samples*skip_count + thermalization_steps + 1):\n",
    "        coords_t, key = sample_body(coords_t, params, key)\n",
    "        if ((step > thermalization_steps) & (step % skip_count == 0)):\n",
    "            outputs.append(coords_t)\n",
    "    return jnp.array(outputs)\n",
    "# second derivative of the wavefunction with respect to the coordinate\n",
    "ddpsi = jit(grad(jit(grad(psi, 0, allow_int = True)), 0, allow_int = True))\n",
    "\n",
    "@jit\n",
    "def Hpsi(coords, params, omega):\n",
    "    return (m*.5*omega**2*coords**2) - hbar**2 / (2*m) * jnp.sum(ddpsi(coords, params)) * 1/psi(coords, params)\n",
    "\n",
    "venergy = jit(vmap(Hpsi, in_axes=(0, None, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def logpsi(coords, params):\n",
    "    return jnp.log(psi(coords, params))\n",
    "\n",
    "# define the derivative with respect to every parameter of the log of psi:\n",
    "dlogpsi_dtheta_stored = jit(grad(logpsi, 1))\n",
    "\n",
    "vlog_term = jit(vmap(dlogpsi_dtheta_stored, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "def gradient(params, omega, num_samples=10**3, thermal=200, skip=50, verbose=False):\n",
    "    # get the samples\n",
    "    samples = sample(params, num_samples, thermal, skip)\n",
    "    psiHpsi = venergy(samples, params, omega)\n",
    "    logs = vlog_term(samples, params)\n",
    "    \n",
    "    uncert = jnp.std(psiHpsi)/jnp.sqrt(num_samples)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(psiHpsi)\n",
    "    if verbose:\n",
    "        print(energy)\n",
    "    log_term = 1/num_samples * jnp.sum(logs,0)\n",
    "\n",
    "    both = 1/num_samples * jnp.sum(vboth(psiHpsi, logs),0)\n",
    "\n",
    "    gradient_calc = (2 * both - 2*energy * log_term)\n",
    "    return gradient_calc, energy, uncert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, iterations, N, thermal, skip, optimizer):\n",
    "    hs = []\n",
    "    us = []\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    def step(params, opt_state, N, thermal, skip):\n",
    "        gr = gradient(params, omega, N, thermal, skip)\n",
    "        hs.append(gr[1])\n",
    "        us.append(gr[2])\n",
    "        updates, opt_state = optimizer.update(gr[0], opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, gr[1]\n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "    for step_num in pbar:   \n",
    "        params, opt_state, energy = step(params, opt_state, N, thermal, skip)\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists, uncerts, params = train(nn.conv1d(),1000,10**4, 200, 50, optax.adam(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_energy = 1\n",
    "plt.plot(np.arange(0, len(hists)), hists, label=\"Adam, \" + str(round(hists[-1],3)) + \" +/- \" + str(round(uncerts[-1], 2)))\n",
    "# plot the uncertainties\n",
    "a_hists = np.array(hists)\n",
    "a_uncerts = np.array(uncerts)\n",
    "plt.fill_between(np.arange(0,len(hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "\n",
    "plt.plot(np.arange(0, len(hists)), [true_energy for x in np.arange(0, len(hists))], label=r\"True Energy, \" + str(round(true_energy,3)))\n",
    "pdiff = (hists[-1] - true_energy)/true_energy*100\n",
    "plt.show()\n",
    "print(\"Percent Difference: \" + str(round(pdiff, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1, 50, 1], 10k iteratons\n",
    "21m 48.5 seconds (1308.5s)\n",
    "\n",
    "stabilized at around 4000 iterations\n",
    "\n",
    "total iterations per second: 7.642\n",
    "\n",
    "total number of nodes: 52\n",
    "\n",
    "\n",
    "# [1,50,50, 1], 10k iterations\n",
    "25m 18s (1518s)\n",
    "\n",
    "stabilized at around 2000 iterations\n",
    "\n",
    "total iterations per second: 6.58 \n",
    "\n",
    "total number of nodes: 102\n",
    "\n",
    "\n",
    "\n",
    "# [1, 50,50,50,50,50, 1], 10k iterations\n",
    "\n",
    "60m 16s (3616s)\n",
    "\n",
    "stabilized at around 750-1000 iterations\n",
    "\n",
    "total iterations per second: 2.7655\n",
    "\n",
    "total number of nodes: 252"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
