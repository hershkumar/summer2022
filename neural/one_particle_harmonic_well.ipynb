{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from pylab import figure, cm\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import time\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "\n",
    "hbar = 1\n",
    "m = 1\n",
    "omega = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    #TODO: Add in biases, see if that helps\n",
    "    #[1, 5, 10, 10, 10, 10, 5, 1]\n",
    "    def __init__(self, node_counts=[1, 50, 1]):\n",
    "        # defining the structure of the neural network\n",
    "        self.num_layers = len(node_counts)\n",
    "        # the number of nodes for each layer\n",
    "        self.node_counts = node_counts\n",
    "        # the total number of weights\n",
    "        self.params_length = 0\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.params_length += node_counts[i] * node_counts[i + 1]\n",
    "            i+=1\n",
    "        # the list that stores the weight matrices (index 0 is the connections from the input to the first hidden layer)\n",
    "        self.weights = []\n",
    "        # storage for all the biases\n",
    "        self.biases = []\n",
    "        \n",
    "        # generate weight matrices with the correct sizes, and random elements\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights.append(np.random.randn(node_counts[i + 1], node_counts[i]) * np.sqrt(1. / (node_counts[i + 1])))\n",
    "        self.weights = np.array(self.weights, dtype=object)\n",
    "        \n",
    "        # generate the bias arrays\n",
    "        for i in range(self.num_layers - 1):\n",
    "            temp = np.random.randn(node_counts[i + 1]) * np.sqrt(1. / node_counts[i + 1])\n",
    "            self.biases.append(temp[:, None])\n",
    "        self.biases = np.array(self.biases, dtype=object)\n",
    "        # get the shape for reshaping a 1d array to this later\n",
    "        self.dimensions = []\n",
    "        for m in self.weights:\n",
    "            self.dimensions.append(m.shape)\n",
    "\n",
    "    # define the activation function that we use for the layers\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def l_act(self, x):\n",
    "        return celu(x)\n",
    "        #return 1.0 / (1.0 + jnp.exp(-x))    \n",
    "    # define the activation function for the output\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def o_act(self, x):\n",
    "        return celu(x)\n",
    "        #return x\n",
    "        #return 1.0 / (1.0 + jnp.exp(-x))\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def conv1d(self):\n",
    "        flattened = []\n",
    "        # loop through each matrix\n",
    "        for m in range(self.num_layers - 1):\n",
    "            mat = self.weights[m]\n",
    "            for i in range(mat.shape[0]):\n",
    "                for j in range(mat.shape[1]):\n",
    "                    flattened.append(mat[i][j])\n",
    "        for m in range(self.num_layers - 1):\n",
    "            # add the biases onto the end\n",
    "            bias = self.biases[m]\n",
    "            for element in bias:\n",
    "                flattened.append(element)\n",
    "        return jnp.array(flattened)\n",
    "        \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def convnd(self, params):\n",
    "        ret = []\n",
    "        ret_bias = []\n",
    "        # the last index of the matrix\n",
    "        max_ind = 0\n",
    "        prev_ind = 0\n",
    "        for dim in self.dimensions:\n",
    "            max_ind += dim[0]*dim[1]\n",
    "            temp = params[prev_ind : max_ind]\n",
    "            prev_ind = max_ind\n",
    "            ret.append(jnp.array(temp).reshape(dim))\n",
    "        biases_flattened = params[max_ind :]\n",
    "        # now reconstruct the bias matrices\n",
    "        # print(biases_flattened)\n",
    "        ind = 0\n",
    "        for i in range(self.num_layers - 1):\n",
    "            ret_bias.append((biases_flattened[ind: ind + self.node_counts[i + 1]])[:, None])\n",
    "            ind += self.node_counts[i + 1]\n",
    "        return ret, ret_bias\n",
    "\n",
    "\n",
    "    # passing inputs into the neural network and getting an output\n",
    "    @partial(jit, static_argnums=[0])\n",
    "    def output(self, coords, params):\n",
    "        # format the parameters as weights\n",
    "        c = self.convnd(params)\n",
    "        self.weights = c[0]\n",
    "        self.biases = c[1]\n",
    "        # compute the output of the neural network\n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = jnp.array(self.weights[i])\n",
    "            b = jnp.array(self.biases[i])\n",
    "            # print(w)\n",
    "            # print(b)\n",
    "            # if its the first layer, dot it against the input and use the activation function\n",
    "            if i == 0:\n",
    "                temp = self.l_act(jnp.dot(w, coords))\n",
    "            elif (i < self.num_layers):\n",
    "                \n",
    "                temp = self.l_act(jnp.dot(w, temp) + b)\n",
    "            else:\n",
    "                # on the output layer we use the output activation function\n",
    "                temp = self.o_act(jnp.dot(w, temp) + b)\n",
    "        # print(\"output:\" + str(time.time() -  start))\n",
    "        return temp[0]\n",
    "\n",
    "# create the network object\n",
    "nn = Network()\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-nn.output(coords, params)[0]) * jnp.exp(-coords**2)\n",
    "\n",
    "# @jit\n",
    "# def psi(coords, params):\n",
    "#     return jnp.exp(- (params[0]* coords + params[1])**2 + params[2])\n",
    "\n",
    "# TODO: deal with thermalization and autocorrelation\n",
    "# tailor -1,1 to be roughly 50/50 acception rejection\n",
    "def sample(params, num_samples):\n",
    "    # random.seed(seed)\n",
    "    outputs = []\n",
    "    coords_t = 0\n",
    "    for _ in range(num_samples):\n",
    "        coords_prime = coords_t + np.random.uniform(-1,1)\n",
    "        if (np.random.uniform(0,1) < psi(coords_prime, params)**2/psi(coords_t, params)**2):\n",
    "            coords_t = coords_prime\n",
    "        outputs.append(coords_t)\n",
    "    return jnp.array(outputs)\n",
    "\n",
    "# second derivative of the wavefunction with respect to the coordinate\n",
    "ddpsi = jit(grad(jit(grad(psi, 0, allow_int = True)), 0, allow_int = True))\n",
    "\n",
    "@jit\n",
    "def Hpsi(coords, params, omega):\n",
    "    return (m*.5*omega**2*coords**2) - hbar**2 / (2*m) * jnp.sum(ddpsi(coords, params)) * 1/psi(coords, params)\n",
    "\n",
    "venergy = vmap(Hpsi, in_axes=(0, None, None), out_axes=0)\n",
    "\n",
    "\n",
    "@jit\n",
    "def logpsi(coords, params):\n",
    "    return jnp.log(psi(coords, params))\n",
    "\n",
    "# define the derivative with respect to every parameter of the log of psi:\n",
    "dlogpsi_dtheta_stored = jit(grad(logpsi, 1))\n",
    "\n",
    "vlog_term = jit(vmap(dlogpsi_dtheta_stored, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "vboth = vmap(jnp.multiply, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "def gradient(params, omega, num_samples=10**3):\n",
    "    # get the samples\n",
    "    samples = sample(params, num_samples)\n",
    "    psiHpsi = venergy(samples, params, omega)\n",
    "    logs = vlog_term(samples, params)\n",
    "\n",
    "    energy = 1/num_samples * jnp.sum(psiHpsi)\n",
    "    print(energy)\n",
    "    log_term = 1/num_samples * jnp.sum(logs,0)\n",
    "\n",
    "    both = 1/num_samples * jnp.sum(vboth(psiHpsi, logs),0)\n",
    "\n",
    "    gradient_calc = (2 * both - 2*energy * log_term)\n",
    "    return gradient_calc, energy\n",
    "\n",
    "def avg_energy(params, omega, num_samples = 10**3):\n",
    "    samples = sample(params, num_samples)\n",
    "    psiHpsi = venergy(samples, params, omega)\n",
    "    return 1/num_samples * jnp.sum(psiHpsi)\n",
    "\n",
    "\n",
    "weight1 = []\n",
    "last_bias = []\n",
    "energies = []\n",
    "def vgrad_opt(start_params, omega, num_samples=10**3, learning_rate=.1, max_iterations=10000, tolerance=.001):\n",
    "    params = start_params\n",
    "    hist = [start_params]\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        clear_output(wait=True)\n",
    "        if it > 800:\n",
    "            num_samples *= 10\n",
    "        gr = gradient(params, omega, num_samples)\n",
    "        energies.append(gr[1])\n",
    "        diff = jnp.asarray((learning_rate * gr[0]))\n",
    "        #print(diff)\n",
    "        \n",
    "        if all((abs(val) < tolerance) for val in diff):\n",
    "            print(\"All under tolerance\")\n",
    "            return hist\n",
    "        # make a step in the direction opposite the gradient\n",
    "        params = params - diff\n",
    "        weight1.append(params[0])\n",
    "        last_bias.append(params[-1])\n",
    "        \n",
    "        # print(params)\n",
    "        hist.append(params)\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.conv1d())\n",
    "# print(\"======\")\n",
    "\n",
    "# print(nn.weights)\n",
    "# print(\"\")\n",
    "# print(nn.biases)\n",
    "# print(nn.convnd(nn.conv1d())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6374667\n"
     ]
    }
   ],
   "source": [
    "xs = np.linspace(-5,5,1000)\n",
    "start_params = nn.conv1d()\n",
    "#print(start_params)\n",
    "\n",
    "ysi = [psi(x, start_params) for x in xs]\n",
    "optd = vgrad_opt(start_params, omega)\n",
    "# plt.plot(weight1)\n",
    "# plt.plot(last_bias)\n",
    "# plt.show()\n",
    "# xs = np.linspace(-5,5,1000)\n",
    "# inp = [1.0,1.0,1.0]\n",
    "# ysi = [psi(x, inp) for x in xs]\n",
    "# optd = vgrad_opt(jnp.array(inp, float), omega)\n",
    "\n",
    "#print(optd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sampling method\n",
    "def sample(params, num_samples=10**3):\n",
    "    outputs = []\n",
    "    e_plots = []\n",
    "    coords_t = np.random.uniform(-1,1)\n",
    "    for _ in range(num_samples):\n",
    "        coords_prime = coords_t + np.random.uniform(-1,1)\n",
    "        e_plots.append(Hpsi(coords_prime, params, omega))\n",
    "        if (np.random.uniform(0,1) < psi(coords_prime, params)**2/psi(coords_t, params)**2):\n",
    "            coords_t = coords_prime\n",
    "        outputs.append(coords_t)\n",
    "    return jnp.array(outputs), e_plots\n",
    "\n",
    "samples = sample(nn.conv1d(), 10**3)\n",
    "\n",
    "plt.hist(samples[0])\n",
    "plt.show()\n",
    "plt.plot(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the energy over the course of the optimization\n",
    "# ys = []\n",
    "# for param in optd:\n",
    "#     ys.append(avg_energy(param, omega,10**3))\n",
    "\n",
    "plt.title(\"Average Energy\")\n",
    "plt.xlabel(\"Iteration of Gradient Descent\")\n",
    "plt.ylabel(\"Average Energy\")\n",
    "plt.plot(np.arange(0,len(optd)), energies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [psi(x, optd[-1]) for x in xs]\n",
    "yse = [((m*omega)/(np.pi*hbar))**(1/4)*np.exp(-m*omega*x**2/(2*hbar)) for x in xs]\n",
    "yse = [np.exp(-x**2) for x in xs]\n",
    "plt.title(r\"One Particle In Harmonic Oscillator Potential\")\n",
    "plt.plot(xs,ysi, \"--\",label=\"Initial\",)\n",
    "plt.plot(xs,ys, label=\"Final\")\n",
    "plt.plot(xs,yse, label=\"True\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True GS: \", hbar*omega/2)\n",
    "print(\"NN GS: \", avg_energy(optd[-1], omega))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
