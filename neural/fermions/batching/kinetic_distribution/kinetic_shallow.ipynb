{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = N_up + N_down fermions in a harmonic trap, with delta function interaction\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n",
    "os.environ[\"JAX_ENABLE_X64\"]=\"false\"\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(\n",
    "    multiprocessing.cpu_count()\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import gvar as gv\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import jax.example_libraries.optimizers as jax_opt\n",
    "from tqdm import tqdm, trange\n",
    "from math import factorial\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# set the default device to the cpu\n",
    "# jax.default_device(jax.devices(\"cpu\")[0])\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "#use pickle to save the parameters to a file \n",
    "def save_params(params, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "# use pickle to dump the energies and uncertainties to a file\n",
    "def save_energies(hs, us, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((hs, us), f)\n",
    "\n",
    "def save_energies(h, u, filename):\n",
    "    with open(filename,'a') as file:\n",
    "        file.write(str(h)+\",\"+str(u))\n",
    "        file.write('\\n')\n",
    "\n",
    "# use pickle to load the parameters from a file\n",
    "def load_params(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)    \n",
    "# use pickle to load the energies and uncertainties from a file\n",
    "def load_energies(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# using this data:\n",
    "# 1, 0.0854344122657581\n",
    "# 2, 0.12291311754684836\n",
    "# 3, 0.15085178875638838\n",
    "# 4, 0.1753833049403748\n",
    "# 5, 0.1965076660988075\n",
    "# 6, 0.21626916524701872\n",
    "# 7, 0.23330494037478702\n",
    "# 8, 0.2503407155025553\n",
    "# 9, 0.2656729131175468\n",
    "\n",
    "def compute_true_energy():\n",
    "    ret = (N_up**2 + N_down**2)/2 \n",
    "    if N_up == 1: \n",
    "        ret += 0.0854344122657581\n",
    "    elif N_up == 2:\n",
    "        ret += 0.12291311754684836\n",
    "    elif N_up == 3:\n",
    "        ret += 0.15085178875638838\n",
    "    elif N_up == 4:\n",
    "        ret += 0.1753833049403748\n",
    "    elif N_up == 5:\n",
    "        ret += 0.1965076660988075\n",
    "    elif N_up == 6:\n",
    "        ret += 0.21626916524701872\n",
    "    elif N_up == 7:\n",
    "        ret += 0.23330494037478702\n",
    "    elif N_up == 8:\n",
    "        ret += 0.2503407155025553\n",
    "    elif N_up == 9:\n",
    "        ret += 0.2656729131175468\n",
    "    return ret\n",
    "\n",
    "\n",
    "##### Constants\n",
    "N_up = 1\n",
    "N_down = 1\n",
    "depth = 1\n",
    "width = 5150\n",
    "N = N_up + N_down\n",
    "\n",
    "FACT_UP = 2 #increase this when N goes up\n",
    "FACT_DOWN = 2 # increase this when N goes up\n",
    "SYM_DEN = 3\n",
    "GPU_INDEX = 1\n",
    "# division factor in the ansatz\n",
    "DIV = 2\n",
    "\n",
    "INITIAL_SAMPLE = jnp.array(np.random.uniform(-2, 2, N))\n",
    "phi_structure = [width for i in range(depth)]\n",
    "\n",
    "m = 1\n",
    "hbar = 1\n",
    "omega = 1\n",
    "harmonic_omega = 1\n",
    "g = 0\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "# this just gets the shapes of the weights and biases for a neural network with the given structure\n",
    "def gen_weight_shapes(input_size, hidden_sizes, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    if hidden_sizes != [0]:\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    else:\n",
    "        sizes = [input_size, output_size]\n",
    "    for i in range(len(sizes) - 1):\n",
    "        w = np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2/sizes[i])\n",
    "        b = np.random.randn(1, sizes[i+1]) \n",
    "        weights.append(w)\n",
    "        biases.append(b) \n",
    "    return weights, biases\n",
    "\n",
    "# get the shapes\n",
    "weight_shapes, bias_shapes = gen_weight_shapes(N, phi_structure, 1)\n",
    "\n",
    "# generates a set of weights and biases for a neural network with the given structure\n",
    "# returns a flattened array of the parameters\n",
    "\n",
    "def gen_params(input_size, hidden_sizes, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    if hidden_sizes != [0]:\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    else:\n",
    "        sizes = [input_size, output_size]\n",
    "    for i in range(len(sizes) - 1):\n",
    "            w = np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2/sizes[i])\n",
    "            b = np.random.randn(1, sizes[i+1]) \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "    return flatten_params(weights, biases) \n",
    "\n",
    "# calls the neural network with the given parameters and input\n",
    "@jit\n",
    "def nn(x, params):\n",
    "    weights, biases = unflatten_params(params) \n",
    "    a = x\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = jnp.dot(a, weights[i]) + biases[i]\n",
    "        a = celu(z)\n",
    "    a = jnp.dot(a, weights[-1]) + biases[-1]\n",
    "    return a[0][0] \n",
    "\n",
    "# takes the weights and biases of a network and returns a flattened array of the parameters\n",
    "@jit\n",
    "def flatten_params(weights, biases):\n",
    "    params = jnp.array([])\n",
    "    for i in range(len(weights)):\n",
    "        params = jnp.concatenate((params, weights[i].flatten()))\n",
    "        params = jnp.concatenate((params, biases[i].flatten()))\n",
    "    return jnp.array(params)\n",
    "\n",
    "# takes a flattened array of parameters and returns the weights and biases of the network\n",
    "@jit\n",
    "def unflatten_params(params):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    start = 0\n",
    "    for i in range(len(weight_shapes)):\n",
    "        end = start + weight_shapes[i].size \n",
    "        weights.append(jnp.reshape(jnp.array(params[start:end]), weight_shapes[i].shape))\n",
    "        start = end\n",
    "        end = start + bias_shapes[i].size\n",
    "        biases.append(jnp.reshape(jnp.array(params[start:end]), bias_shapes[i].shape))\n",
    "        start = end\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "\n",
    "network = gen_params(N, phi_structure, 1)\n",
    "# the length of the flattened parameters of a single particle neural network\n",
    "phi_params_length = len(network)\n",
    "\n",
    "# function that takes the coords, and moves coords[index] to the front of the list\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def shift_coords(coords, index):\n",
    "    return jnp.concatenate([jnp.array([coords[index]]), jnp.array(coords[:index]), jnp.array(coords[index + 1:])])\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def inputs_up(coords, j):\n",
    "    reordered = shift_coords(coords, j)\n",
    "    sym_piece1 = reordered[1:N_up] / SYM_DEN\n",
    "    sym_piece2 = reordered[N_up:] / SYM_DEN\n",
    "\n",
    "    new1 = [jnp.sum(sym_piece1 ** i) for i in range(1, N_up)]\n",
    "    new2 = [jnp.sum(sym_piece2 ** i) for i in range(1, N_down + 1)]\n",
    "    \n",
    "    return jnp.array([reordered[0]] + new1 + new2)\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def inputs_down(coords, j):\n",
    "    reordered = shift_coords(coords, j + N_up)\n",
    "    \n",
    "    sym_piece1 = reordered[1:N_up+1] / SYM_DEN\n",
    "    sym_piece2 = reordered[N_up + 1:] / SYM_DEN\n",
    "    \n",
    "    new1 = [jnp.sum(sym_piece1 ** i) for i in range(1, N_up + 1)]\n",
    "    new2 = [jnp.sum(sym_piece2 ** i) for i in range(1, N_down)]\n",
    "    \n",
    "    return jnp.array([reordered[0]] + new1 + new2)\n",
    "\n",
    "@jit\n",
    "def Phi_up(coords, params):\n",
    "    # construct the matrix of outputs of the neural networks\n",
    "    # take only the up spin coordinates\n",
    "    mat = jnp.zeros((N_up, N_up))\n",
    "    for i in range(N_up):\n",
    "        ith_params = params[i * phi_params_length : (i + 1) * phi_params_length]\n",
    "        for j in range(N_up): \n",
    "            mat = mat.at[i,j].set(nn(inputs_up(coords, j), ith_params))\n",
    "    return jnp.linalg.det(mat) * FACT_UP \n",
    "\n",
    "@jit\n",
    "def Phi_down(coords, params):\n",
    "    # construct the matrix of outputs of the neural networks\n",
    "    # take only the up spin coordinates\n",
    "    mat = jnp.zeros((N_down, N_down))\n",
    "    for i in range(N_down):\n",
    "        temp = i + N_up\n",
    "        ith_params = params[temp * phi_params_length : (temp + 1) * phi_params_length]\n",
    "        for j in range(N_down): \n",
    "            mat = mat.at[i,j].set(nn(inputs_down(coords, j), ith_params))\n",
    "    return jnp.linalg.det(mat)* FACT_DOWN\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return Phi_up(coords, params) * Phi_down(coords, params) * jnp.exp(-omega * jnp.sum((coords/DIV)**2))\n",
    "\n",
    "\n",
    "@jit\n",
    "def mcstep_E(xis, limit, positions, params):\n",
    "    \n",
    "#     params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "    \n",
    "    newpositions = jnp.array(positions) + xis\n",
    "    \n",
    "    # prob = psi(newpositions, params)**2./psi(positions, params)**2.\n",
    "    prob = (psi(newpositions, params)/psi(positions, params))**2.\n",
    "    \n",
    "    def truefunc(p):\n",
    "        return [newpositions, True]\n",
    "\n",
    "    def falsefunc(p):\n",
    "        return [positions, False]\n",
    "    \n",
    "    return jax.lax.cond(prob >= limit, truefunc, falsefunc, prob)\n",
    "\n",
    "def sample(params, Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "\n",
    "    sq = []\n",
    "    sq_prime = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    rng = np.random.default_rng(int(time.time()))\n",
    "    randoms = rng.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = rng.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved:\n",
    "                counter += 1\n",
    "        \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "    # generate the primed samples by going through every sample and making sample[N_up] = sample[0]\n",
    "    sq_prime = jnp.array(sq.copy())\n",
    "    for i in range(len(sq)):\n",
    "        a = jnp.array(sq[i])\n",
    "        a = a.at[N_up].set(a[0])\n",
    "        sq_prime = sq_prime.at[i].set(jnp.array(a))\n",
    "\n",
    "    return jnp.array(sq), jnp.array(sq_prime), counter/num_total\n",
    "\n",
    "\n",
    "psi_hessian = jax.jacfwd(jit(grad(psi, 0)), 0) # type: ignore\n",
    "\n",
    "@jit\n",
    "def ddpsi(coords, params):\n",
    "    #return jnp.diagonal(A_hessian(transform(coords), params))\n",
    "    return jnp.diag(psi_hessian(coords, params))\n",
    "\n",
    "\n",
    "# derivative of the wavefunction with respect to the parameters\n",
    "dnn_dtheta = jit(grad(psi, 1)) \n",
    "vdnn_dtheta = jit(vmap(dnn_dtheta, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "\n",
    "@jit\n",
    "def Es_nodelta(coords, params):\n",
    "    return - (1/2) * (1/ psi(coords, params)) * jnp.sum(ddpsi(coords, params)) + (1/2) * jnp.sum(coords**2) \n",
    "\n",
    "vEs_nodelta = jit(vmap(Es_nodelta, in_axes=(0,None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def Es_delta(coords, coords_prime, params, alpha, g):\n",
    "    return N_up * N_down * g * (psi(coords_prime, params)**2)/(psi(coords, params)**2) * (1/(np.sqrt(np.pi)*alpha))*np.e**(-(coords[N_up]/alpha)**2)\n",
    "\n",
    "vEs_delta = jit(vmap(Es_delta, in_axes=(0,0, None, None, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def gradient_comp(coords, coords_prime, params, es_nodelta, energy_calc, es_delta):\n",
    "    return 2/(psi(coords,params)) * dnn_dtheta(coords, params) * (es_nodelta - energy_calc) + 2/(psi(coords_prime, params)) * dnn_dtheta(coords_prime, params) * es_delta\n",
    "\n",
    "vgradient_comp = jit(vmap(gradient_comp, in_axes=(0,0,None,0, None, 0), out_axes=0))\n",
    "\n",
    "def accumulator_sample(params, Nsweeps, Ntherm, keep, stepsize, g, positions_initial=INITIAL_SAMPLE, progress=True):\n",
    "    num_total = Nsweeps * keep + Ntherm + 1\n",
    "#     params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "    randoms = np.random.uniform(-stepsize, stepsize, size=(num_total, N))\n",
    "    limits = np.random.uniform(0, 1, size=num_total)\n",
    "\n",
    "    accept_counter = 0\n",
    "    es = 0\n",
    "    grads = 0\n",
    "    mean = 0\n",
    "    m2 = 0\n",
    "    alpha = 1\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "\n",
    "    for i in range(num_total):\n",
    "        new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "        if i >= Ntherm and i % keep == 0:\n",
    "            accept_counter += 1\n",
    "            new_prime = np.copy(new)\n",
    "            new_prime[N_up] = new_prime[0]\n",
    "\n",
    "            temp_nodeltas = Es_nodelta(new, params)\n",
    "            temp_deltas = Es_delta(new, new_prime, params, alpha, g)\n",
    "            temp_sum = temp_nodeltas + temp_deltas\n",
    "\n",
    "            es += temp_sum\n",
    "            curr_e_avg = es / accept_counter\n",
    "            grads += gradient_comp(new, new_prime, params, temp_nodeltas, curr_e_avg, temp_deltas)\n",
    "\n",
    "            temp = temp_sum - mean\n",
    "            mean += temp / accept_counter\n",
    "            m2 += temp * (temp_sum - mean)\n",
    "\n",
    "        positions_prev = new\n",
    "\n",
    "    stddev = np.sqrt(m2 / (accept_counter - 1)) / jnp.sqrt(accept_counter)\n",
    "    return es, grads, stddev\n",
    "\n",
    "def accumulator_gradient(params, g, num_samples=10**3, thermal=200, skip=50, variation_size=1.0):\n",
    "    # sample\n",
    "    es, grads, uncert = accumulator_sample(params, num_samples, thermal, skip, variation_size, g)\n",
    "    energy_calc = es/num_samples\n",
    "    gradient_calc = grads/num_samples\n",
    "    return gradient_calc, energy_calc, uncert\n",
    "\n",
    "def gradient(params, g, num_samples=10**3, thermal=200, skip=50, variation_size=1.0):\n",
    "    # first sample\n",
    "#     params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "    samples, samples_prime, _ = sample(params, num_samples, thermal, skip, variation_size)\n",
    "\n",
    "    ys = jnp.array(samples_prime[:, N_up]) \n",
    "    alpha = jnp.sqrt(jnp.max(abs(jnp.array(ys)))**2/(-jnp.log(jnp.sqrt(jnp.pi)*(10**-10))))\n",
    "\n",
    "    e_nodeltas = vEs_nodelta(samples, params)\n",
    "    e_deltas = vEs_delta(samples, samples_prime, params, alpha, g)\n",
    "\n",
    "    e_term = e_nodeltas + e_deltas\n",
    "    energy_calc = jnp.mean(e_term)\n",
    "    \n",
    "    # compute the uncertainty in the energy\n",
    "    uncert = jnp.std(e_term)/jnp.sqrt(num_samples) \n",
    "    # gradient computation\n",
    "    grads = vgradient_comp(samples, samples_prime, params, e_nodeltas, energy_calc, e_deltas)\n",
    "    gradient_calc = jnp.mean(grads, axis=0)\n",
    "\n",
    "    return gradient_calc, energy_calc, uncert\n",
    "\n",
    "\n",
    "\n",
    "def step(params_arg, step_num, N, thermal, skip, variation_size, g):\n",
    "        gr = gradient(params_arg, g, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        print(gr[0])\n",
    "        # hs.append(gr[1])\n",
    "        # us.append(gr[2])\n",
    "        opt_state = opt_init(params_arg)\n",
    "        new = opt_update(step_num, gr[0], opt_state)\n",
    "        return get_params(new), gr[1], gr[2]\n",
    "def acc_step(params_arg, step_num, N, thermal, skip, variation_size, g):\n",
    "        gr = accumulator_gradient(params_arg, g, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        # hs.append(gr[1])\n",
    "        # us.append(gr[2])\n",
    "        opt_state = opt_init(params_arg)\n",
    "        new = opt_update(step_num, gr[0], opt_state)\n",
    "        return get_params(new), gr[1], gr[2]\n",
    "\n",
    "def train(params, iterations, N, thermal, skip, variation_size, g):\n",
    "    hs = []\n",
    "    us = [] \n",
    "    ns = np.arange(iterations) \n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:   \n",
    "        new_params, energy, uncert = step(old_params, step_num, N, thermal, skip, variation_size, g)\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        old_params = new_params.copy()\n",
    "        # save the energies and uncertainties to a file\n",
    "#         save_energies(hs, us, \"energies.pkl\")\n",
    "        save_energies(energy, uncert, \"energies.csv\")\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "def acc_train(params, iterations, N, thermal, skip, variation_size, g):\n",
    "    hs = []\n",
    "    us = [] \n",
    "    ns = np.arange(iterations) \n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:   \n",
    "        new_params, energy, uncert = acc_step(old_params, step_num, N, thermal, skip, variation_size, g)\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        old_params = new_params.copy()\n",
    "        # save the energies and uncertainties to a file\n",
    "#         save_energies(hs, us, \"energies.pkl\")\n",
    "        save_energies(energy, uncert, \"energies.csv\")\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "\n",
    "#TODO: stop precomputing all the random numbers and storing them, that takes too much memory\n",
    "@partial(jit, static_argnums=(1,2,3,4))\n",
    "def sample_pmap(params, Nsweeps, Ntherm, keep, stepsize, key, positions_initial=INITIAL_SAMPLE):\n",
    "\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    sq = jnp.empty((Nsweeps+1, N))\n",
    "    sq_prime = jnp.empty((Nsweeps+1, N))\n",
    "    \n",
    "    # How many keys do we need?\n",
    "#     subkeys = jax.random.split(key, N)\n",
    "    # key, shape, datatype, then bounds\n",
    "#     randoms = vmap(jax.random.uniform, in_axes=(0,None,None, None,None))(subkeys,(num_total,), jnp.float32,-stepsize, stepsize)\n",
    "#     randoms = jnp.transpose(randoms)\n",
    "#     subkeys = jax.random.split(subkeys[-1], num_total)\n",
    "#     limits = vmap(jax.random.uniform, in_axes=(0,None, None, None, None))(subkeys,(), jnp.float32,0.0,1.0)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    \n",
    "    def true_fun(sq, new, i):\n",
    "        return sq.at[(i - Ntherm)//keep].set(new)\n",
    "    def false_fun(sq, new, i):\n",
    "        return sq\n",
    "    \n",
    "    def body_fun(i, val):\n",
    "        # unpack val\n",
    "        sq, positions_prev, stepkey = val\n",
    "        # get a random number between -stepsize and stepsize\n",
    "        rng_keys = jax.random.split(stepkey, 3)\n",
    "        random_1 = jax.random.uniform(rng_keys[0],(N,), jnp.float32, -stepsize, stepsize)\n",
    "        random_2 = jax.random.uniform(rng_keys[1], (), jnp.float32, 0.0, 1.0)\n",
    "#         new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        new, moved = mcstep_E(random_1, random_2, positions_prev, params)\n",
    "        sq = jax.lax.cond(jnp.logical_and(jnp.mod(i, keep) == 0,i >= Ntherm), true_fun, false_fun, sq, new, i)\n",
    "        positions_prev = new\n",
    "        return sq, positions_prev, rng_keys[2]\n",
    "    \n",
    "    sq, positions_prev, key = jax.lax.fori_loop(0, num_total, body_fun, (sq, positions_prev, key))\n",
    "    del positions_prev\n",
    "    del key\n",
    "    \n",
    "\n",
    "    def set_prime(a):\n",
    "        # Set sample[N_up] = sample[0] for each sample\n",
    "        return a.at[N_up].set(a[0])\n",
    "\n",
    "    # Apply the `set_prime` function to every sample in `sq` using vmap\n",
    "    sq_prime = jax.vmap(set_prime)(sq)\n",
    "        \n",
    "    return jnp.array(sq), jnp.array(sq_prime)\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def batch_gradient(samples, samples_prime, params):\n",
    "    num_samples = len(samples)\n",
    "    ys = jnp.array(samples_prime[:, N_up])\n",
    "    alpha = jnp.sqrt(jnp.max(abs(jnp.array(ys)))**2/(-jnp.log(jnp.sqrt(jnp.pi)*(10**-10))))\n",
    "\n",
    "    e_nodeltas = vEs_nodelta(samples, params)\n",
    "    e_deltas = vEs_delta(samples, samples_prime, params, alpha, g)\n",
    "\n",
    "    e_term = e_nodeltas + e_deltas\n",
    "    energy_calc = jnp.mean(e_term)\n",
    "    \n",
    "    # compute the uncertainty in the energy\n",
    "    uncert = jnp.std(e_term)/jnp.sqrt(num_samples) \n",
    "    # gradient computation\n",
    "    grads = vgradient_comp(samples, samples_prime, params, e_nodeltas, energy_calc, e_deltas)\n",
    "    gradient_calc = jnp.mean(grads, axis=0)\n",
    "    \n",
    "    return gradient_calc, energy_calc, uncert\n",
    "\n",
    "def batch_step(params_arg, step_num, N, N_batches, thermal, skip, variation_size, g, start_key):\n",
    "    # compute the gradient for each batch\n",
    "    samples_per_batch = N//N_batches\n",
    "    grads = []\n",
    "    energies = []\n",
    "    uncerts = []\n",
    "    \n",
    "    def grad_wrapper(key):\n",
    "        samples, samples_prime = sample_pmap(params_arg, samples_per_batch, thermal, skip, variation_size, key)\n",
    "        return batch_gradient(samples, samples_prime, params_arg)\n",
    "\n",
    "    grad_pmap = jax.pmap(grad_wrapper, backend=\"cpu\")\n",
    "\n",
    "    inputs = jax.random.split(start_key, N_batches)\n",
    "    out = grad_pmap(inputs)\n",
    "    \n",
    "    # average the gradients\n",
    "    gradient_avg = jnp.mean(out[0], axis=0)\n",
    "\n",
    "    # average the averages\n",
    "    energy_calc = jnp.mean(out[1])\n",
    "    uncert_calc = jnp.sqrt(jnp.sum(jnp.square(out[2])))/N_batches\n",
    "    \n",
    "    \n",
    "    opt_state = opt_init(params_arg)\n",
    "    new = opt_update(step_num, gradient_avg, opt_state)\n",
    "    return get_params(new), energy_calc, uncert_calc\n",
    "\n",
    "\n",
    "def batch_train(params, iterations, N, N_batches, thermal, skip, variation_size, g):\n",
    "    hs = []\n",
    "    us = [] \n",
    "    ns = np.arange(iterations) \n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:\n",
    "        new_params, energy, uncert = batch_step(old_params, step_num, N, N_batches, thermal, skip, variation_size, g, jax.random.key(int(time.time())))\n",
    "\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        old_params = new_params.copy()\n",
    "        # save the energies and uncertainties to a file\n",
    "#         save_energies(hs, us, \"energies.pkl\")\n",
    "        save_energies(energy, uncert, \"energies.csv\")\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_ar(params, Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "\n",
    "    sq = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    rng = np.random.default_rng(int(time.time()))\n",
    "    randoms = rng.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = rng.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved:\n",
    "                counter += 1\n",
    "        \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "\n",
    "\n",
    "    return jnp.array(sq), counter/num_total\n",
    "\n",
    "\n",
    "def find_step_size(params, start):\n",
    "    lr = .1\n",
    "    target = 0.5\n",
    "    tolerance = .1\n",
    "    max_it = 1000\n",
    "    step = start\n",
    "    best_step = start\n",
    "    best_acc = 0\n",
    "    it_num = 0\n",
    "    last = start\n",
    "    # get the samples \n",
    "    _, _, acc = sample(params, 1000, 100, 5, step)\n",
    "    # while the acceptance rate is not within +/- .5 of the target\n",
    "    while (acc < target - tolerance or acc > target + tolerance) and it_num < max_it:\n",
    "        it_num += 1\n",
    "        last = step\n",
    "        # if the acceptance rate is too low, increase the step size\n",
    "        if acc < target - tolerance:\n",
    "            step -= lr\n",
    "        # if the acceptance rate is too high, decrease the step size\n",
    "        elif acc > target + tolerance:\n",
    "            step += lr\n",
    "        # if we cross the target, decrease the learning rate and go back\n",
    "        if (acc < target and best_acc > target) or (acc > target and best_acc < target):\n",
    "            lr /= 2\n",
    "            step = best_step\n",
    "        # keep track of the best step size\n",
    "        if abs(acc - target) < abs(best_acc - target):\n",
    "            best_acc = acc\n",
    "            best_step = step\n",
    "\n",
    "        if last == step:\n",
    "            break \n",
    "        # get the samples for the next step size\n",
    "        _, _, acc = sample(params, 1000, 100, 5, step)\n",
    "    return best_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(jax.devices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0854344122657582\n"
     ]
    }
   ],
   "source": [
    "print(compute_true_energy())\n",
    "# clear the energies.pkl file\n",
    "# save_energies([], [], \"energies.pkl\")\n",
    "open(\"energies.csv\", 'w').close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41202\n"
     ]
    }
   ],
   "source": [
    "# make N sets of parameters\n",
    "params = gen_params(N, phi_structure, 1)\n",
    "for i in range(N - 1):\n",
    "    params = jnp.concatenate((params, gen_params(N, phi_structure, 1)))\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC: 100%|██████████| 6001/6001 [00:00<00:00, 6239.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5390768205299117\n"
     ]
    }
   ],
   "source": [
    "step_size = 1.25\n",
    "samples = sample_ar(params, 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])\n",
    "# print(samples[0].shape)\n",
    "# print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def kinetic(coords, params):\n",
    "    return - (1/2) * (1/ psi(coords, params)) * jnp.sum(ddpsi(coords, params)) \n",
    "\n",
    "vkinetic = jit(vmap(kinetic, in_axes=(0,None), out_axes=0))\n",
    "\n",
    "\n",
    "def measure_kinetic(params, num_samples, thermal, skip, step_size):\n",
    "    samples,_ = sample_pmap(params, num_samples, thermal, skip, step_size, jax.random.key(int(time.time())))\n",
    "    \n",
    "    vals = vkinetic(samples, params)\n",
    "    \n",
    "    return jnp.mean(vals)\n",
    "\n",
    "# print(measure_kinetic(params, 10000, 1000, 5, step_size))\n",
    "\n",
    "def probe(num_params_set, num_samples):\n",
    "    energies = []\n",
    "    ps = []\n",
    "    steps = []\n",
    "    for i in tqdm(range(num_params_set)):\n",
    "        # get a set of params\n",
    "        params = gen_params(N, phi_structure, 1)\n",
    "        for i in range(N - 1):\n",
    "            params = jnp.concatenate((params, gen_params(N, phi_structure, 1)))\n",
    "        ps.append(params)\n",
    "        # find the step size\n",
    "        steps.append(find_step_size(params, 1.25))\n",
    "    ps = jnp.array(ps)\n",
    "    steps = jnp.array(steps)\n",
    "    # convert this loop to a vectorized operation\n",
    "    # for i in tqdm(range(num_params_set)): \n",
    "    #     # now measure the kinetic energy\n",
    "    #     energies.append(measure_kinetic(params, num_samples, 1000, 5, step_size))\n",
    "    energies = vmap(measure_kinetic, in_axes=(0, None, None, None, 0))(ps, num_samples, 1000, 5, steps)\n",
    "    return energies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "energies = probe(10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(energies)\n",
    "plt.title(\"Shallow Network Kinetic Energy Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10 ** (-4))\n",
    "\n",
    "g=0\n",
    "\n",
    "resultsa = batch_train(resultsa[3], 25, 300000, 64, 5000, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]\n",
    "# + resultsb[0] \n",
    "# + resultsc[0]  + resultsd[0]  + resultse[0]\n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1] \n",
    "\n",
    "# + resultsb[1]  \n",
    "# + resultsc[1]  + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsa[3],\"1+1_data/g0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsa[0][-1], resultsa[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1.35\n",
    "samples = sample_ar(resultsa[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = -.5\n",
    "\n",
    "resultsb = batch_train(resultsa[3], 10, 400000, 64, 5000, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] \n",
    "# + resultsc[0]  + resultsd[0]  + resultse[0]\n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  \n",
    "# + resultsc[1]  + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsb[3],\"1+1_data/g0.5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsb[0][-1], resultsb[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_size = 1.25\n",
    "samples = sample_ar(resultsb[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=-1\n",
    "\n",
    "resultsc = batch_train(resultsb[3], 100, 400000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] \n",
    "# + resultsd[0]  + resultse[0]\n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1] \n",
    "# + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsc[3],\"1+1_data/g1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsc[0][-1], resultsc[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1.15\n",
    "samples = sample_ar(resultsc[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=-1.5\n",
    "\n",
    "resultsd = batch_train(resultsd[3], 50, 500000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0] \n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1]\n",
    "# + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsd[3],\"1+1_data/g1.5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsd[0][-1], resultsd[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultsd[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 2.0\n",
    "\n",
    "resultse = batch_train(resultsd[3], 4, 200000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultse[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 2.5\n",
    "\n",
    "resultsf = batch_train(resultse[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultsf[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 3\n",
    "\n",
    "resultsg = batch_train(resultsf[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultsg[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 3.5\n",
    "\n",
    "resultsh = batch_train(resultsg[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .3\n",
    "samples = sample_ar(resultsh[3], 2500, 1000, 3, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 3.5\n",
    "\n",
    "resultsi = batch_train(resultsh[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0] + resultsi[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1] + resultsi[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .3\n",
    "samples = sample_ar(resultsi[3], 2500, 1000, 3, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 4\n",
    "\n",
    "resultsj = batch_train(resultsi[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0] + resultsi[0] + resultsj[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1] + resultsi[1] + resultsj[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .3\n",
    "samples = sample_ar(resultsj[3], 2500, 1000, 3, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 4.5\n",
    "\n",
    "resultsk = train(resultsj[3], 2, 200000, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0] + resultsi[0] + resultsj[0] + resultsk[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1] + resultsi[1] + resultsj[1] + resultsk[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
