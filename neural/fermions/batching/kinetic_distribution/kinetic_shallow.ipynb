{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = N_up + N_down fermions in a harmonic trap, with delta function interaction\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n",
    "os.environ[\"JAX_ENABLE_X64\"]=\"false\"\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(\n",
    "    multiprocessing.cpu_count()\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import jax\n",
    "from jax import grad, hessian, jit, vmap\n",
    "from jax.nn import celu\n",
    "import gvar as gv\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import jax.example_libraries.optimizers as jax_opt\n",
    "from tqdm import tqdm, trange\n",
    "from math import factorial\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# set the default device to the cpu\n",
    "# jax.default_device(jax.devices(\"cpu\")[0])\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "#use pickle to save the parameters to a file \n",
    "def save_params(params, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "# use pickle to dump the energies and uncertainties to a file\n",
    "def save_energies(hs, us, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((hs, us), f)\n",
    "\n",
    "def save_energies(h, u, filename):\n",
    "    with open(filename,'a') as file:\n",
    "        file.write(str(h)+\",\"+str(u))\n",
    "        file.write('\\n')\n",
    "\n",
    "# use pickle to load the parameters from a file\n",
    "def load_params(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)    \n",
    "# use pickle to load the energies and uncertainties from a file\n",
    "def load_energies(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# using this data:\n",
    "# 1, 0.0854344122657581\n",
    "# 2, 0.12291311754684836\n",
    "# 3, 0.15085178875638838\n",
    "# 4, 0.1753833049403748\n",
    "# 5, 0.1965076660988075\n",
    "# 6, 0.21626916524701872\n",
    "# 7, 0.23330494037478702\n",
    "# 8, 0.2503407155025553\n",
    "# 9, 0.2656729131175468\n",
    "\n",
    "def compute_true_energy():\n",
    "    ret = (N_up**2 + N_down**2)/2 \n",
    "    if N_up == 1: \n",
    "        ret += 0.0854344122657581\n",
    "    elif N_up == 2:\n",
    "        ret += 0.12291311754684836\n",
    "    elif N_up == 3:\n",
    "        ret += 0.15085178875638838\n",
    "    elif N_up == 4:\n",
    "        ret += 0.1753833049403748\n",
    "    elif N_up == 5:\n",
    "        ret += 0.1965076660988075\n",
    "    elif N_up == 6:\n",
    "        ret += 0.21626916524701872\n",
    "    elif N_up == 7:\n",
    "        ret += 0.23330494037478702\n",
    "    elif N_up == 8:\n",
    "        ret += 0.2503407155025553\n",
    "    elif N_up == 9:\n",
    "        ret += 0.2656729131175468\n",
    "    return ret\n",
    "\n",
    "\n",
    "##### Constants\n",
    "N_up = 1\n",
    "N_down = 1\n",
    "depth = 1\n",
    "width = 5150\n",
    "N = N_up + N_down\n",
    "\n",
    "FACT_UP = 2 #increase this when N goes up\n",
    "FACT_DOWN = 2 # increase this when N goes up\n",
    "SYM_DEN = 3\n",
    "GPU_INDEX = 1\n",
    "# division factor in the ansatz\n",
    "DIV = 2\n",
    "\n",
    "INITIAL_SAMPLE = jnp.array(np.random.uniform(-2, 2, N))\n",
    "phi_structure = [width for i in range(depth)]\n",
    "\n",
    "m = 1\n",
    "hbar = 1\n",
    "omega = 1\n",
    "harmonic_omega = 1\n",
    "g = 0\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "# this just gets the shapes of the weights and biases for a neural network with the given structure\n",
    "def gen_weight_shapes(input_size, hidden_sizes, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    if hidden_sizes != [0]:\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    else:\n",
    "        sizes = [input_size, output_size]\n",
    "    for i in range(len(sizes) - 1):\n",
    "        w = np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2/sizes[i])\n",
    "        b = np.random.randn(1, sizes[i+1]) \n",
    "        weights.append(w)\n",
    "        biases.append(b) \n",
    "    return weights, biases\n",
    "\n",
    "# get the shapes\n",
    "weight_shapes, bias_shapes = gen_weight_shapes(N, phi_structure, 1)\n",
    "\n",
    "# generates a set of weights and biases for a neural network with the given structure\n",
    "# returns a flattened array of the parameters\n",
    "\n",
    "def gen_params(input_size, hidden_sizes, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    if hidden_sizes != [0]:\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    else:\n",
    "        sizes = [input_size, output_size]\n",
    "    for i in range(len(sizes) - 1):\n",
    "            w = np.random.randn(sizes[i], sizes[i+1]) * np.sqrt(2/sizes[i])\n",
    "            b = np.random.randn(1, sizes[i+1]) \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "    return flatten_params(weights, biases) \n",
    "\n",
    "# calls the neural network with the given parameters and input\n",
    "@jit\n",
    "def nn(x, params):\n",
    "    weights, biases = unflatten_params(params) \n",
    "    a = x\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = jnp.dot(a, weights[i]) + biases[i]\n",
    "        a = celu(z)\n",
    "    a = jnp.dot(a, weights[-1]) + biases[-1]\n",
    "    return a[0][0] \n",
    "\n",
    "# takes the weights and biases of a network and returns a flattened array of the parameters\n",
    "@jit\n",
    "def flatten_params(weights, biases):\n",
    "    params = jnp.array([])\n",
    "    for i in range(len(weights)):\n",
    "        params = jnp.concatenate((params, weights[i].flatten()))\n",
    "        params = jnp.concatenate((params, biases[i].flatten()))\n",
    "    return jnp.array(params)\n",
    "\n",
    "# takes a flattened array of parameters and returns the weights and biases of the network\n",
    "@jit\n",
    "def unflatten_params(params):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    start = 0\n",
    "    for i in range(len(weight_shapes)):\n",
    "        end = start + weight_shapes[i].size \n",
    "        weights.append(jnp.reshape(jnp.array(params[start:end]), weight_shapes[i].shape))\n",
    "        start = end\n",
    "        end = start + bias_shapes[i].size\n",
    "        biases.append(jnp.reshape(jnp.array(params[start:end]), bias_shapes[i].shape))\n",
    "        start = end\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "\n",
    "network = gen_params(N, phi_structure, 1)\n",
    "# the length of the flattened parameters of a single particle neural network\n",
    "phi_params_length = len(network)\n",
    "\n",
    "# function that takes the coords, and moves coords[index] to the front of the list\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def shift_coords(coords, index):\n",
    "    return jnp.concatenate([jnp.array([coords[index]]), jnp.array(coords[:index]), jnp.array(coords[index + 1:])])\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def inputs_up(coords, j):\n",
    "    reordered = shift_coords(coords, j)\n",
    "    sym_piece1 = reordered[1:N_up] / SYM_DEN\n",
    "    sym_piece2 = reordered[N_up:] / SYM_DEN\n",
    "\n",
    "    new1 = [jnp.sum(sym_piece1 ** i) for i in range(1, N_up)]\n",
    "    new2 = [jnp.sum(sym_piece2 ** i) for i in range(1, N_down + 1)]\n",
    "    \n",
    "    return jnp.array([reordered[0]] + new1 + new2)\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def inputs_down(coords, j):\n",
    "    reordered = shift_coords(coords, j + N_up)\n",
    "    \n",
    "    sym_piece1 = reordered[1:N_up+1] / SYM_DEN\n",
    "    sym_piece2 = reordered[N_up + 1:] / SYM_DEN\n",
    "    \n",
    "    new1 = [jnp.sum(sym_piece1 ** i) for i in range(1, N_up + 1)]\n",
    "    new2 = [jnp.sum(sym_piece2 ** i) for i in range(1, N_down)]\n",
    "    \n",
    "    return jnp.array([reordered[0]] + new1 + new2)\n",
    "\n",
    "@jit\n",
    "def Phi_up(coords, params):\n",
    "    # construct the matrix of outputs of the neural networks\n",
    "    # take only the up spin coordinates\n",
    "    mat = jnp.zeros((N_up, N_up))\n",
    "    for i in range(N_up):\n",
    "        ith_params = params[i * phi_params_length : (i + 1) * phi_params_length]\n",
    "        for j in range(N_up): \n",
    "            mat = mat.at[i,j].set(nn(inputs_up(coords, j), ith_params))\n",
    "    return jnp.linalg.det(mat) * FACT_UP \n",
    "\n",
    "@jit\n",
    "def Phi_down(coords, params):\n",
    "    # construct the matrix of outputs of the neural networks\n",
    "    # take only the up spin coordinates\n",
    "    mat = jnp.zeros((N_down, N_down))\n",
    "    for i in range(N_down):\n",
    "        temp = i + N_up\n",
    "        ith_params = params[temp * phi_params_length : (temp + 1) * phi_params_length]\n",
    "        for j in range(N_down): \n",
    "            mat = mat.at[i,j].set(nn(inputs_down(coords, j), ith_params))\n",
    "    return jnp.linalg.det(mat)* FACT_DOWN\n",
    "\n",
    "@jit\n",
    "def psi(coords, params):\n",
    "    return Phi_up(coords, params) * Phi_down(coords, params) * jnp.exp(-omega * jnp.sum((coords/DIV)**2))\n",
    "\n",
    "\n",
    "@jit\n",
    "def mcstep_E(xis, limit, positions, params):\n",
    "    \n",
    "#     params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "    \n",
    "    newpositions = jnp.array(positions) + xis\n",
    "    \n",
    "    # prob = psi(newpositions, params)**2./psi(positions, params)**2.\n",
    "    prob = (psi(newpositions, params)/psi(positions, params))**2.\n",
    "    \n",
    "    def truefunc(p):\n",
    "        return [newpositions, True]\n",
    "\n",
    "    def falsefunc(p):\n",
    "        return [positions, False]\n",
    "    \n",
    "    return jax.lax.cond(prob >= limit, truefunc, falsefunc, prob)\n",
    "\n",
    "def sample(params, Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "\n",
    "    sq = []\n",
    "    sq_prime = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    rng = np.random.default_rng(int(time.time()))\n",
    "    randoms = rng.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = rng.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved:\n",
    "                counter += 1\n",
    "        \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "    # generate the primed samples by going through every sample and making sample[N_up] = sample[0]\n",
    "    sq_prime = jnp.array(sq.copy())\n",
    "    for i in range(len(sq)):\n",
    "        a = jnp.array(sq[i])\n",
    "        a = a.at[N_up].set(a[0])\n",
    "        sq_prime = sq_prime.at[i].set(jnp.array(a))\n",
    "\n",
    "    return jnp.array(sq), jnp.array(sq_prime), counter/num_total\n",
    "\n",
    "\n",
    "psi_hessian = jax.jacfwd(jit(grad(psi, 0)), 0) # type: ignore\n",
    "\n",
    "@jit\n",
    "def ddpsi(coords, params):\n",
    "    #return jnp.diagonal(A_hessian(transform(coords), params))\n",
    "    return jnp.diag(psi_hessian(coords, params))\n",
    "\n",
    "\n",
    "# derivative of the wavefunction with respect to the parameters\n",
    "dnn_dtheta = jit(grad(psi, 1)) \n",
    "vdnn_dtheta = jit(vmap(dnn_dtheta, in_axes=(0, None), out_axes=0))\n",
    "\n",
    "\n",
    "@jit\n",
    "def Es_nodelta(coords, params):\n",
    "    return - (1/2) * (1/ psi(coords, params)) * jnp.sum(ddpsi(coords, params)) + (1/2) * jnp.sum(coords**2) \n",
    "\n",
    "vEs_nodelta = jit(vmap(Es_nodelta, in_axes=(0,None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def Es_delta(coords, coords_prime, params, alpha, g):\n",
    "    return N_up * N_down * g * (psi(coords_prime, params)**2)/(psi(coords, params)**2) * (1/(np.sqrt(np.pi)*alpha))*np.e**(-(coords[N_up]/alpha)**2)\n",
    "\n",
    "vEs_delta = jit(vmap(Es_delta, in_axes=(0,0, None, None, None), out_axes=0))\n",
    "\n",
    "@jit\n",
    "def gradient_comp(coords, coords_prime, params, es_nodelta, energy_calc, es_delta):\n",
    "    return 2/(psi(coords,params)) * dnn_dtheta(coords, params) * (es_nodelta - energy_calc) + 2/(psi(coords_prime, params)) * dnn_dtheta(coords_prime, params) * es_delta\n",
    "\n",
    "vgradient_comp = jit(vmap(gradient_comp, in_axes=(0,0,None,0, None, 0), out_axes=0))\n",
    "\n",
    "def accumulator_sample(params, Nsweeps, Ntherm, keep, stepsize, g, positions_initial=INITIAL_SAMPLE, progress=True):\n",
    "    num_total = Nsweeps * keep + Ntherm + 1\n",
    "#     params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "    randoms = np.random.uniform(-stepsize, stepsize, size=(num_total, N))\n",
    "    limits = np.random.uniform(0, 1, size=num_total)\n",
    "\n",
    "    accept_counter = 0\n",
    "    es = 0\n",
    "    grads = 0\n",
    "    mean = 0\n",
    "    m2 = 0\n",
    "    alpha = 1\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "\n",
    "    for i in range(num_total):\n",
    "        new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "        if i >= Ntherm and i % keep == 0:\n",
    "            accept_counter += 1\n",
    "            new_prime = np.copy(new)\n",
    "            new_prime[N_up] = new_prime[0]\n",
    "\n",
    "            temp_nodeltas = Es_nodelta(new, params)\n",
    "            temp_deltas = Es_delta(new, new_prime, params, alpha, g)\n",
    "            temp_sum = temp_nodeltas + temp_deltas\n",
    "\n",
    "            es += temp_sum\n",
    "            curr_e_avg = es / accept_counter\n",
    "            grads += gradient_comp(new, new_prime, params, temp_nodeltas, curr_e_avg, temp_deltas)\n",
    "\n",
    "            temp = temp_sum - mean\n",
    "            mean += temp / accept_counter\n",
    "            m2 += temp * (temp_sum - mean)\n",
    "\n",
    "        positions_prev = new\n",
    "\n",
    "    stddev = np.sqrt(m2 / (accept_counter - 1)) / jnp.sqrt(accept_counter)\n",
    "    return es, grads, stddev\n",
    "\n",
    "def accumulator_gradient(params, g, num_samples=10**3, thermal=200, skip=50, variation_size=1.0):\n",
    "    # sample\n",
    "    es, grads, uncert = accumulator_sample(params, num_samples, thermal, skip, variation_size, g)\n",
    "    energy_calc = es/num_samples\n",
    "    gradient_calc = grads/num_samples\n",
    "    return gradient_calc, energy_calc, uncert\n",
    "\n",
    "def gradient(params, g, num_samples=10**3, thermal=200, skip=50, variation_size=1.0):\n",
    "    # first sample\n",
    "#     params = jax.device_put(params, device=jax.devices(\"cpu\")[0])\n",
    "\n",
    "    samples, samples_prime, _ = sample(params, num_samples, thermal, skip, variation_size)\n",
    "\n",
    "    ys = jnp.array(samples_prime[:, N_up]) \n",
    "    alpha = jnp.sqrt(jnp.max(abs(jnp.array(ys)))**2/(-jnp.log(jnp.sqrt(jnp.pi)*(10**-10))))\n",
    "\n",
    "    e_nodeltas = vEs_nodelta(samples, params)\n",
    "    e_deltas = vEs_delta(samples, samples_prime, params, alpha, g)\n",
    "\n",
    "    e_term = e_nodeltas + e_deltas\n",
    "    energy_calc = jnp.mean(e_term)\n",
    "    \n",
    "    # compute the uncertainty in the energy\n",
    "    uncert = jnp.std(e_term)/jnp.sqrt(num_samples) \n",
    "    # gradient computation\n",
    "    grads = vgradient_comp(samples, samples_prime, params, e_nodeltas, energy_calc, e_deltas)\n",
    "    gradient_calc = jnp.mean(grads, axis=0)\n",
    "\n",
    "    return gradient_calc, energy_calc, uncert\n",
    "\n",
    "\n",
    "\n",
    "def step(params_arg, step_num, N, thermal, skip, variation_size, g):\n",
    "        gr = gradient(params_arg, g, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        print(gr[0])\n",
    "        # hs.append(gr[1])\n",
    "        # us.append(gr[2])\n",
    "        opt_state = opt_init(params_arg)\n",
    "        new = opt_update(step_num, gr[0], opt_state)\n",
    "        return get_params(new), gr[1], gr[2]\n",
    "def acc_step(params_arg, step_num, N, thermal, skip, variation_size, g):\n",
    "        gr = accumulator_gradient(params_arg, g, N, thermal, skip, variation_size)\n",
    "        # print(gr)\n",
    "        # hs.append(gr[1])\n",
    "        # us.append(gr[2])\n",
    "        opt_state = opt_init(params_arg)\n",
    "        new = opt_update(step_num, gr[0], opt_state)\n",
    "        return get_params(new), gr[1], gr[2]\n",
    "\n",
    "def train(params, iterations, N, thermal, skip, variation_size, g):\n",
    "    hs = []\n",
    "    us = [] \n",
    "    ns = np.arange(iterations) \n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:   \n",
    "        new_params, energy, uncert = step(old_params, step_num, N, thermal, skip, variation_size, g)\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        old_params = new_params.copy()\n",
    "        # save the energies and uncertainties to a file\n",
    "#         save_energies(hs, us, \"energies.pkl\")\n",
    "        save_energies(energy, uncert, \"energies.csv\")\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "def acc_train(params, iterations, N, thermal, skip, variation_size, g):\n",
    "    hs = []\n",
    "    us = [] \n",
    "    ns = np.arange(iterations) \n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:   \n",
    "        new_params, energy, uncert = acc_step(old_params, step_num, N, thermal, skip, variation_size, g)\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        old_params = new_params.copy()\n",
    "        # save the energies and uncertainties to a file\n",
    "#         save_energies(hs, us, \"energies.pkl\")\n",
    "        save_energies(energy, uncert, \"energies.csv\")\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "\n",
    "#TODO: stop precomputing all the random numbers and storing them, that takes too much memory\n",
    "@partial(jit, static_argnums=(1,2,3,4))\n",
    "def sample_pmap(params, Nsweeps, Ntherm, keep, stepsize, key, positions_initial=INITIAL_SAMPLE):\n",
    "\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    sq = jnp.empty((Nsweeps+1, N))\n",
    "    sq_prime = jnp.empty((Nsweeps+1, N))\n",
    "    \n",
    "    # How many keys do we need?\n",
    "#     subkeys = jax.random.split(key, N)\n",
    "    # key, shape, datatype, then bounds\n",
    "#     randoms = vmap(jax.random.uniform, in_axes=(0,None,None, None,None))(subkeys,(num_total,), jnp.float32,-stepsize, stepsize)\n",
    "#     randoms = jnp.transpose(randoms)\n",
    "#     subkeys = jax.random.split(subkeys[-1], num_total)\n",
    "#     limits = vmap(jax.random.uniform, in_axes=(0,None, None, None, None))(subkeys,(), jnp.float32,0.0,1.0)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    \n",
    "    def true_fun(sq, new, i):\n",
    "        return sq.at[(i - Ntherm)//keep].set(new)\n",
    "    def false_fun(sq, new, i):\n",
    "        return sq\n",
    "    \n",
    "    def body_fun(i, val):\n",
    "        # unpack val\n",
    "        sq, positions_prev, stepkey = val\n",
    "        # get a random number between -stepsize and stepsize\n",
    "        rng_keys = jax.random.split(stepkey, 3)\n",
    "        random_1 = jax.random.uniform(rng_keys[0],(N,), jnp.float32, -stepsize, stepsize)\n",
    "        random_2 = jax.random.uniform(rng_keys[1], (), jnp.float32, 0.0, 1.0)\n",
    "#         new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        new, moved = mcstep_E(random_1, random_2, positions_prev, params)\n",
    "        sq = jax.lax.cond(jnp.logical_and(jnp.mod(i, keep) == 0,i >= Ntherm), true_fun, false_fun, sq, new, i)\n",
    "        positions_prev = new\n",
    "        return sq, positions_prev, rng_keys[2]\n",
    "    \n",
    "    sq, positions_prev, key = jax.lax.fori_loop(0, num_total, body_fun, (sq, positions_prev, key))\n",
    "    del positions_prev\n",
    "    del key\n",
    "    \n",
    "\n",
    "    def set_prime(a):\n",
    "        # Set sample[N_up] = sample[0] for each sample\n",
    "        return a.at[N_up].set(a[0])\n",
    "\n",
    "    # Apply the `set_prime` function to every sample in `sq` using vmap\n",
    "    sq_prime = jax.vmap(set_prime)(sq)\n",
    "        \n",
    "    return jnp.array(sq), jnp.array(sq_prime)\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def batch_gradient(samples, samples_prime, params):\n",
    "    num_samples = len(samples)\n",
    "    ys = jnp.array(samples_prime[:, N_up])\n",
    "    alpha = jnp.sqrt(jnp.max(abs(jnp.array(ys)))**2/(-jnp.log(jnp.sqrt(jnp.pi)*(10**-10))))\n",
    "\n",
    "    e_nodeltas = vEs_nodelta(samples, params)\n",
    "    e_deltas = vEs_delta(samples, samples_prime, params, alpha, g)\n",
    "\n",
    "    e_term = e_nodeltas + e_deltas\n",
    "    energy_calc = jnp.mean(e_term)\n",
    "    \n",
    "    # compute the uncertainty in the energy\n",
    "    uncert = jnp.std(e_term)/jnp.sqrt(num_samples) \n",
    "    # gradient computation\n",
    "    grads = vgradient_comp(samples, samples_prime, params, e_nodeltas, energy_calc, e_deltas)\n",
    "    gradient_calc = jnp.mean(grads, axis=0)\n",
    "    \n",
    "    return gradient_calc, energy_calc, uncert\n",
    "\n",
    "def batch_step(params_arg, step_num, N, N_batches, thermal, skip, variation_size, g, start_key):\n",
    "    # compute the gradient for each batch\n",
    "    samples_per_batch = N//N_batches\n",
    "    grads = []\n",
    "    energies = []\n",
    "    uncerts = []\n",
    "    \n",
    "    def grad_wrapper(key):\n",
    "        samples, samples_prime = sample_pmap(params_arg, samples_per_batch, thermal, skip, variation_size, key)\n",
    "        return batch_gradient(samples, samples_prime, params_arg)\n",
    "\n",
    "    grad_pmap = jax.pmap(grad_wrapper, backend=\"cpu\")\n",
    "\n",
    "    inputs = jax.random.split(start_key, N_batches)\n",
    "    out = grad_pmap(inputs)\n",
    "    \n",
    "    # average the gradients\n",
    "    gradient_avg = jnp.mean(out[0], axis=0)\n",
    "\n",
    "    # average the averages\n",
    "    energy_calc = jnp.mean(out[1])\n",
    "    uncert_calc = jnp.sqrt(jnp.sum(jnp.square(out[2])))/N_batches\n",
    "    \n",
    "    \n",
    "    opt_state = opt_init(params_arg)\n",
    "    new = opt_update(step_num, gradient_avg, opt_state)\n",
    "    return get_params(new), energy_calc, uncert_calc\n",
    "\n",
    "\n",
    "def batch_train(params, iterations, N, N_batches, thermal, skip, variation_size, g):\n",
    "    hs = []\n",
    "    us = [] \n",
    "    ns = np.arange(iterations) \n",
    "\n",
    "    pbar = trange(iterations, desc=\"\", leave=True)\n",
    "\n",
    "    old_params = params.copy()\n",
    "    for step_num in pbar:\n",
    "        new_params, energy, uncert = batch_step(old_params, step_num, N, N_batches, thermal, skip, variation_size, g, jax.random.key(int(time.time())))\n",
    "\n",
    "        hs.append(energy)\n",
    "        us.append(uncert)\n",
    "        old_params = new_params.copy()\n",
    "        # save the energies and uncertainties to a file\n",
    "#         save_energies(hs, us, \"energies.pkl\")\n",
    "        save_energies(energy, uncert, \"energies.csv\")\n",
    "        pbar.set_description(\"Energy = \" + str(energy), refresh=True)\n",
    "        if np.isnan(energy):\n",
    "            print(\"NaN encountered, stopping...\")\n",
    "            break\n",
    "    clear_output(wait=True)\n",
    "    return hs, us, ns, old_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_ar(params, Nsweeps, Ntherm, keep, stepsize, positions_initial=INITIAL_SAMPLE, progress=False):\n",
    "\n",
    "    sq = []\n",
    "    counter = 0\n",
    "    num_total = Nsweeps * keep + Ntherm + 1 \n",
    "    rng = np.random.default_rng(int(time.time()))\n",
    "    randoms = rng.uniform(-stepsize, stepsize, size = (num_total, N))\n",
    "    limits = rng.uniform(0, 1, size = num_total)\n",
    "\n",
    "    positions_prev = positions_initial\n",
    "    \n",
    "    if progress:\n",
    "        for i in tqdm(range(0, num_total), position = 0, leave = True, desc = \"MC\"):\n",
    "            \n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved:\n",
    "                counter += 1\n",
    "        \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "                \n",
    "    else: \n",
    "        for i in range(num_total):\n",
    "            new, moved = mcstep_E(randoms[i], limits[i], positions_prev, params)\n",
    "        \n",
    "            if moved == True:\n",
    "                counter += 1\n",
    "                \n",
    "            if i%keep == 0 and i >= Ntherm:\n",
    "                #sq = np.vstack((sq, np.array(new)))\n",
    "                sq.append(new)\n",
    "                \n",
    "            positions_prev = new\n",
    "\n",
    "\n",
    "    return jnp.array(sq), counter/num_total\n",
    "\n",
    "\n",
    "def find_step_size(params, start):\n",
    "    lr = .1\n",
    "    target = 0.5\n",
    "    tolerance = .1\n",
    "    max_it = 1000\n",
    "    step = start\n",
    "    best_step = start\n",
    "    best_acc = 0\n",
    "    it_num = 0\n",
    "    last = start\n",
    "    # get the samples \n",
    "    _, _, acc = sample(params, 1000, 100, 5, step)\n",
    "    # while the acceptance rate is not within +/- .5 of the target\n",
    "    while (acc < target - tolerance or acc > target + tolerance) and it_num < max_it:\n",
    "        it_num += 1\n",
    "        last = step\n",
    "        # if the acceptance rate is too low, increase the step size\n",
    "        if acc < target - tolerance:\n",
    "            step -= lr\n",
    "        # if the acceptance rate is too high, decrease the step size\n",
    "        elif acc > target + tolerance:\n",
    "            step += lr\n",
    "        # if we cross the target, decrease the learning rate and go back\n",
    "        if (acc < target and best_acc > target) or (acc > target and best_acc < target):\n",
    "            lr /= 2\n",
    "            step = best_step\n",
    "        # keep track of the best step size\n",
    "        if abs(acc - target) < abs(best_acc - target):\n",
    "            best_acc = acc\n",
    "            best_step = step\n",
    "\n",
    "        if last == step:\n",
    "            break \n",
    "        # get the samples for the next step size\n",
    "        _, _, acc = sample(params, 1000, 100, 5, step)\n",
    "    return best_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(jax.devices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0854344122657582\n"
     ]
    }
   ],
   "source": [
    "print(compute_true_energy())\n",
    "# clear the energies.pkl file\n",
    "# save_energies([], [], \"energies.pkl\")\n",
    "open(\"energies.csv\", 'w').close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41202\n"
     ]
    }
   ],
   "source": [
    "# make N sets of parameters\n",
    "params = gen_params(N, phi_structure, 1)\n",
    "for i in range(N - 1):\n",
    "    params = jnp.concatenate((params, gen_params(N, phi_structure, 1)))\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC: 100%|██████████| 6001/6001 [00:00<00:00, 7283.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5045825695717381\n"
     ]
    }
   ],
   "source": [
    "step_size = 1.25\n",
    "samples = sample_ar(params, 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])\n",
    "# print(samples[0].shape)\n",
    "# print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def kinetic(coords, params):\n",
    "    return - (1/2) * (1/ psi(coords, params)) * jnp.sum(ddpsi(coords, params)) \n",
    "\n",
    "vkinetic = jit(vmap(kinetic, in_axes=(0,None), out_axes=0))\n",
    "\n",
    "\n",
    "def measure_kinetic(params, num_samples, thermal, skip, step_size, key):\n",
    "    samples,_ = sample_pmap(params, num_samples, thermal, skip, step_size, key)\n",
    "    \n",
    "    vals = vkinetic(samples, params)\n",
    "    \n",
    "    return jnp.mean(vals)\n",
    "\n",
    "# using multithreading, write a function that generates many sets of parameters\n",
    "def gen_many_params(num_params):\n",
    "    # first define a function that generates a single set of parameters\n",
    "    def gen_single_params(index):\n",
    "        params = gen_params(N, phi_structure, 1)\n",
    "        for i in range(N - 1):\n",
    "            params = jnp.concatenate((params, gen_params(N, phi_structure, 1)))\n",
    "        return params \n",
    "    # now use vmap to generate many sets of parameters\n",
    "    return vmap(gen_single_params, in_axes=(0))(jnp.arange(num_params))\n",
    "\n",
    "def probe(num_params_set, num_samples):\n",
    "    energies = []\n",
    "    ps = []\n",
    "    steps = jnp.array([.8 for i in range(num_params_set)]) \n",
    "    # for i in tqdm(range(num_params_set)):\n",
    "    #     # get a set of params\n",
    "    #     params = gen_params(N, phi_structure, 1)\n",
    "    #     for i in range(N - 1):\n",
    "    #         params = jnp.concatenate((params, gen_params(N, phi_structure, 1)))\n",
    "    #     ps.append(params)\n",
    "    ps = gen_many_params(num_params_set)\n",
    "    # convert this loop to a vectorized operation\n",
    "    # for i in tqdm(range(num_params_set)): \n",
    "    #     # now measure the kinetic energy\n",
    "    #     energies.append(measure_kinetic(params, num_samples, 1000, 5, step_size))\n",
    "    energies = vmap(measure_kinetic, in_axes=(0, None, None, None, 0, 0))(ps, num_samples, 1000, 5, steps, jax.random.split(jax.random.key(int(time.time())), num_params_set))\n",
    "    return energies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = probe(100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(energies)\n",
    "plt.title(\"Shallow Network Kinetic Energy Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = jax_opt.adam(10 ** (-4))\n",
    "\n",
    "g=0\n",
    "\n",
    "resultsa = batch_train(resultsa[3], 25, 300000, 64, 5000, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]\n",
    "# + resultsb[0] \n",
    "# + resultsc[0]  + resultsd[0]  + resultse[0]\n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1] \n",
    "\n",
    "# + resultsb[1]  \n",
    "# + resultsc[1]  + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsa[3],\"1+1_data/g0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsa[0][-1], resultsa[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1.35\n",
    "samples = sample_ar(resultsa[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = -.5\n",
    "\n",
    "resultsb = batch_train(resultsa[3], 10, 400000, 64, 5000, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] \n",
    "# + resultsc[0]  + resultsd[0]  + resultse[0]\n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  \n",
    "# + resultsc[1]  + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsb[3],\"1+1_data/g0.5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsb[0][-1], resultsb[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_size = 1.25\n",
    "samples = sample_ar(resultsb[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=-1\n",
    "\n",
    "resultsc = batch_train(resultsb[3], 100, 400000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] \n",
    "# + resultsd[0]  + resultse[0]\n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1] \n",
    "# + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsc[3],\"1+1_data/g1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsc[0][-1], resultsc[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1.15\n",
    "samples = sample_ar(resultsc[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=-1.5\n",
    "\n",
    "resultsd = batch_train(resultsd[3], 50, 500000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0] \n",
    "# + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1]\n",
    "# + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(resultsd[3],\"1+1_data/g1.5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultsd[0][-1], resultsd[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultsd[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 2.0\n",
    "\n",
    "resultse = batch_train(resultsd[3], 4, 200000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultse[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 2.5\n",
    "\n",
    "resultsf = batch_train(resultse[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultsf[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 3\n",
    "\n",
    "resultsg = batch_train(resultsf[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .27\n",
    "samples = sample_ar(resultsg[3], 1000, 1000, 5, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 3.5\n",
    "\n",
    "resultsh = batch_train(resultsg[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .3\n",
    "samples = sample_ar(resultsh[3], 2500, 1000, 3, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 3.5\n",
    "\n",
    "resultsi = batch_train(resultsh[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0] + resultsi[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1] + resultsi[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .3\n",
    "samples = sample_ar(resultsi[3], 2500, 1000, 3, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 4\n",
    "\n",
    "resultsj = batch_train(resultsi[3], 4, 100000, 64, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0] + resultsi[0] + resultsj[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1] + resultsi[1] + resultsj[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = .3\n",
    "samples = sample_ar(resultsj[3], 2500, 1000, 3, step_size, progress=True)\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 4.5\n",
    "\n",
    "resultsk = train(resultsj[3], 2, 200000, 2500, 10, step_size, g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hists =  resultsa[0]  + resultsb[0] + resultsc[0] + resultsd[0]  + resultse[0] + resultsf[0] + resultsg[0] + resultsh[0] + resultsi[0] + resultsj[0] + resultsk[0]\n",
    "\n",
    "# + resultsd[0]\n",
    "total_uncerts = resultsa[1]  + resultsb[1]  + resultsc[1]  + resultsd[1] + resultse[1] + resultsf[1] + resultsg[1] + resultsh[1] + resultsi[1] + resultsj[1] + resultsk[1]\n",
    "# + resultse[1]\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0, len(total_hists)), total_hists)# plot the uncertainties\n",
    "a_hists = np.array(total_hists)\n",
    "a_uncerts = np.array(total_uncerts)\n",
    "plt.fill_between(np.arange(0,len(total_hists)), a_hists - a_uncerts, a_hists + a_uncerts, alpha=.4)\n",
    "# plt.ylim(0, 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
