{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from pylab import figure, cm\n",
    "import random\n",
    "from jax import grad, hessian, jit, vmap\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "N = 2\n",
    "omega = 1\n",
    "g = 1\n",
    "true_E = N*omega/2 -  g**2/24 * N * (N**2 - 1)\n",
    "hbar = 1\n",
    "m = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, node_counts=[N, 10, 5, 1]):\n",
    "        # defining the structure of the neural network\n",
    "        self.num_layers = len(node_counts)\n",
    "        # the number of nodes for each layer\n",
    "        self.node_counts = node_counts\n",
    "        # the total number of weights\n",
    "        self.params_length = 0\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.params_length += node_counts[i] * node_counts[i + 1]\n",
    "            i+=1\n",
    "        \n",
    "        # the list that stores the weight matrices (index 0 is the connections from the input to the first hidden layer)\n",
    "        self.weights = []\n",
    "        # generate weight matrices with the correct sizes, and random elements\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights.append(np.random.randn(node_counts[i + 1], node_counts[i]) * np.sqrt(1. / node_counts[i + 1]))\n",
    "        self.weights = np.asarray(self.weights, dtype=object)\n",
    "        \n",
    "        # get the shape for reshaping a 1d array to this later\n",
    "        self.weights_shape = self.weights.shape\n",
    "        \n",
    "\n",
    "    # define the activation function that we use for the layers\n",
    "    def l_act(self, x, derivative = False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    # define the activation function for the output\n",
    "    def o_act(self, x):\n",
    "        return 1/(jnp.exp(-x) + 1)\n",
    "    \n",
    "    def conv1d(self):\n",
    "        return self.weights.ravel()\n",
    "    \n",
    "    def convnd(self, params):\n",
    "        return params.reshape(self.weights_shape)\n",
    "    \n",
    "    # passing inputs into the neural network and getting an output\n",
    "    def output(self, coords, params):\n",
    "        # sort the input coordinates in order to enforce particle swap invariance\n",
    "        coords.sort()\n",
    "        coords = jnp.asarray(coords)\n",
    "        # format the parameters as weights\n",
    "        self.weights = self.convnd(params)\n",
    "        # compute the output of the neural network\n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = self.weights[i]\n",
    "            # if its the first layer, dot it against the input and use the activation function\n",
    "            if i == 0:\n",
    "                temp = self.l_act(jnp.dot(w, coords))\n",
    "            elif (i < self.num_layers):\n",
    "                temp = self.l_act(jnp.dot(w, temp))\n",
    "            else:\n",
    "                # on the output layer we se the output activation function\n",
    "                temp = self.o_act(jnp.dot(w, temp))\n",
    "        return temp[0]\n",
    "\n",
    "# create the network object\n",
    "nn = Network([N, 10, 10, 1])\n",
    "\n",
    "# defines the wavefunction based on calls to the neural network\n",
    "def psi(coords, params):\n",
    "    return jnp.exp(-nn.output(coords, params))\n",
    "\n",
    "#TODO: do the computations and figure out what's needed to compute the average energy\n",
    "# as well as the gradient.\n",
    "# then use the same gradient descent algorithm\n",
    "# Perhaps we'll need to use a different method for computing the gradient, due to the increased\n",
    "# number of parameters\n",
    "\n",
    "#TODO: eventually add bias nodes to the neural network\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
